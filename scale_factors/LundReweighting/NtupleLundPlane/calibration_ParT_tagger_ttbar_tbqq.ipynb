{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.28/00\n"
     ]
    }
   ],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import vector\n",
    "vector.register_awkward()\n",
    "from matplotlib import pyplot as plt\n",
    "import mplhep as hep\n",
    "hep.style.use(\"CMS\")\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "from numpy.typing import ArrayLike\n",
    "import numpy as np\n",
    "import correctionlib\n",
    "import awkward as ak\n",
    "import fastjet\n",
    "from coffea.nanoevents.methods import vector\n",
    "from coffea import nanoevents\n",
    "from coffea import processor\n",
    "from coffea.nanoevents.methods import candidate\n",
    "from coffea.analysis_tools import Weights, PackedSelection\n",
    "from hist import Hist\n",
    "ak.behavior.update(vector.behavior)\n",
    "import sys\n",
    "sys.path.append('/home/pku/zhaoyz/Higgs/LundReweighting/utils')\n",
    "from LundReweighter import *\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Ntuple files, and actually some selection is done in the Ntuple step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for Electron_genPartIdx => GenPart\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for Electron_jetIdx => Jet\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for Electron_photonIdx => Photon\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:208: RuntimeWarning: Missing cross-reference target for FatJet_genJetAK8Idx => GenJetAK8\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for FsrPhoton_muonIdx => Muon\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for GenVisTau_genPartIdxMother => GenPart\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:208: RuntimeWarning: Missing cross-reference target for Jet_electronIdx1 => Electron\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:208: RuntimeWarning: Missing cross-reference target for Jet_electronIdx2 => Electron\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:208: RuntimeWarning: Missing cross-reference target for Jet_genJetIdx => GenJet\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:208: RuntimeWarning: Missing cross-reference target for Jet_muonIdx1 => Muon\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:208: RuntimeWarning: Missing cross-reference target for Jet_muonIdx2 => Muon\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for Muon_fsrPhotonIdx => FsrPhoton\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for Muon_genPartIdx => GenPart\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for Muon_jetIdx => Jet\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for Photon_electronIdx => Electron\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for Photon_genPartIdx => GenPart\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for Photon_jetIdx => Jet\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for Tau_genPartIdx => GenPart\n",
      "  warnings.warn(\n",
      "/home/pku/zhaoyz/anaconda3/envs/lpr/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:201: RuntimeWarning: Missing cross-reference index for Tau_jetIdx => Jet\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "events = nanoevents.NanoEventsFactory.from_root(\n",
    "        \"/data/bond/zhaoyz/Ntuple/V3/2018/Merged/ttbar_validation_final/TTToSemiLeptonic.root\",\n",
    "        schemaclass=nanoevents.NanoAODSchema,\n",
    "    ).events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [211, -211, -211, -211, ... 130, 22, 22] type='276 * int32[parameters={\"_...'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[0].PFCands.pdgId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 1\n",
    "n_use = int(frac*len(events))\n",
    "events_1 = events[:n_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['etagenq5f',\n",
       " 'phigenzf',\n",
       " 'phigenq5f',\n",
       " 'phigenq3l',\n",
       " 'ST',\n",
       " 'Flag',\n",
       " 'puWeightUp',\n",
       " 'OtherPV',\n",
       " 'massgenq3f',\n",
       " 'etagenq5l',\n",
       " 'PFCands',\n",
       " 'LHEWeight',\n",
       " 'massgenq2f',\n",
       " 'genWeight',\n",
       " 'phigenq3f',\n",
       " 'massgenwl',\n",
       " 'massgenwf',\n",
       " 'puWeight',\n",
       " 'GenFatJetSVs',\n",
       " 'DeepMETResolutionTune',\n",
       " 'AK15PuppiSubJet',\n",
       " 'Nj8',\n",
       " 'puWeightDown',\n",
       " 'SubJet',\n",
       " 'mothergenq2f',\n",
       " 'run',\n",
       " 'RawPuppiMET',\n",
       " 'GenVtx',\n",
       " 'DeepMETResponseTune',\n",
       " 'GenPart',\n",
       " 'massgenq1l',\n",
       " 'genH',\n",
       " 'phigenq1l',\n",
       " 'etagenq1f',\n",
       " 'etagenq4l',\n",
       " 'mothergenq4f',\n",
       " 'etagenq1l',\n",
       " 'CorrT1METJet',\n",
       " 'MET',\n",
       " 'etagenzl',\n",
       " 'Jet',\n",
       " 'taggenwl',\n",
       " 'taggenzl',\n",
       " 'massgengl',\n",
       " 'massgengf',\n",
       " 'fixedGridRhoFastjetAll',\n",
       " 'massgenzl',\n",
       " 'massgenq5f',\n",
       " 'SV',\n",
       " 'ak4jet',\n",
       " 'HLT',\n",
       " 'phigenq5l',\n",
       " 'PuppiMET',\n",
       " 'ptgenq2l',\n",
       " 'etagenzf',\n",
       " 'FatJet',\n",
       " 'phigenq2l',\n",
       " 'massgenq3l',\n",
       " 'massgenq5l',\n",
       " 'etagengl',\n",
       " 'fixedGridRhoFastjetCentralNeutral',\n",
       " 'phigengl',\n",
       " 'ptgenq4l',\n",
       " 'LHE',\n",
       " 'gent',\n",
       " 'mothergengf',\n",
       " 'FatJetPFCands',\n",
       " 'GenCands',\n",
       " 'genantit',\n",
       " 'CaloMET',\n",
       " 'm',\n",
       " 'mothergenq5f',\n",
       " 'Pileup',\n",
       " 'usenumber2',\n",
       " 'nLooseEle',\n",
       " 'PSWeight',\n",
       " 'phigengf',\n",
       " 'ptgenzl',\n",
       " 'AK15Puppi',\n",
       " 'LHEPdfWeight',\n",
       " 'phigenwf',\n",
       " 'etagenq3f',\n",
       " 'nLooseMu',\n",
       " 'massgenq4f',\n",
       " 'ptgenq4f',\n",
       " 'phigenwl',\n",
       " 'massgenq2l',\n",
       " 'ptgenwf',\n",
       " 'luminosityBlock',\n",
       " 'jetAK8puppi',\n",
       " 'mothergenq3f',\n",
       " 'LHEScaleWeight',\n",
       " 'phigenq2f',\n",
       " 'genantitop',\n",
       " 'fixedGridRhoFastjetCentral',\n",
       " 'ptgenzf',\n",
       " 'PV',\n",
       " 'FatJetSVs',\n",
       " 'etagenq4f',\n",
       " 'usenumber3',\n",
       " 'GenFatJetCands',\n",
       " 'ptgenq5f',\n",
       " 'TkMET',\n",
       " 'IDLoose',\n",
       " 'JetPFCands',\n",
       " 'Generator',\n",
       " 'GenMET',\n",
       " 'etagenwf',\n",
       " 'ptgenq3l',\n",
       " 'btagWeight',\n",
       " 'ptgenq1l',\n",
       " 'massgenq4l',\n",
       " 'mothergenq1f',\n",
       " 'fixedGridRhoFastjetCentralChargedPileUp',\n",
       " 'ptgenq2f',\n",
       " 'ptgenq5l',\n",
       " 'ptgenq3f',\n",
       " 'etagengf',\n",
       " 'etagenq3l',\n",
       " 'massgenq1f',\n",
       " 'PrefireWeight',\n",
       " 'genTtbarId',\n",
       " 'ptgengl',\n",
       " 'phigenq1f',\n",
       " 'etagenq2l',\n",
       " 'etagenwl',\n",
       " 'ptgenq1f',\n",
       " 'usenumber1',\n",
       " 'LHEReweightingWeight',\n",
       " 'RawMET',\n",
       " 'phigenzl',\n",
       " 'ptgengf',\n",
       " 'fixedGridRhoFastjetCentralCalo',\n",
       " 'phigenq4f',\n",
       " 'gentop',\n",
       " 'ptgenwl',\n",
       " 'massgenzf',\n",
       " 'JetSVs',\n",
       " 'phigenq4l',\n",
       " 'etagenq2f',\n",
       " 'ChsMET',\n",
       " 'genw',\n",
       " 'event']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick look at the fields of loaded files\n",
    "events_1.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NanoEventsArray [<event 1:142148:142147035>, ... ] type='2758516 * event'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define necessary functions to run the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad array with given value\n",
    "def pad_val(\n",
    "    arr: ak.Array,\n",
    "    target: int,\n",
    "    value: float, #value can also be Bool variable \n",
    "    axis: int = 0,\n",
    "    to_numpy: bool = True,\n",
    "    clip: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    pads awkward array up to ``target`` index along axis ``axis`` with value ``value``,\n",
    "    optionally converts to numpy array\n",
    "    \"\"\"\n",
    "    padded_arr = ak.fill_none(ak.pad_none(arr, target, axis=axis, clip=clip), value, axis=axis)\n",
    "    # pad_none will fill the array to target length with \"None\" for dedicated axis\n",
    "    # \"clip\" means cut the array to the target length or not\n",
    "    # fill_none will replace \"None\" value to some value\n",
    "    return padded_arr.to_numpy() if to_numpy else padded_arr\n",
    "\n",
    "def add_selection(\n",
    "    name: str,\n",
    "    sel: np.ndarray,\n",
    "    selection: PackedSelection,\n",
    "    cutflow: dict = None,\n",
    "    isData: bool = False,\n",
    "    signGenWeights: ak.Array = None,\n",
    "):\n",
    "    \"\"\"adds selection to PackedSelection object and the cutflow dictionary\"\"\"\n",
    "    selection.add(name, sel)\n",
    "    if cutflow is not None: #only add to cutflow dictionary if cutflow is not None\n",
    "        cutflow[name] = (\n",
    "            np.sum(selection.all(*selection.names))\n",
    "            if isData\n",
    "            # add up sign of genWeights for MC\n",
    "            else np.sum(signGenWeights[selection.all(*selection.names)])\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-selection:\n",
    "#1.Leading jet pT > 400GeV, maximum jet mass > 50GeV\n",
    "#2.Require 2 or 3 AK8 jet with pT > 200GeV\n",
    "#3.Veto (mini-)Isolated leptons\n",
    "isData = False\n",
    "signGenWeights = None if isData else np.sign(events_1[\"genWeight\"]) #get genWeight sign, because only the sign matters\n",
    "n_events = len(events_1) if isData else int(np.sum(signGenWeights)) #events number for MC events should be the sum of \"sign\"\n",
    "selection = PackedSelection() #initialize a new object\n",
    "\n",
    "cutflow = {}\n",
    "# cutflow[\"all\"] = len(events) #shouldn't be n_events?\n",
    "cutflow[\"all\"] = n_events\n",
    "preselection_cut_vals = {\"pt\": 200, \"msd\": 20, \"leading_pt\":400,\"maximum_mass\":50}\n",
    "num_jets = 2\n",
    "\n",
    "# fatjets = corrections.get_jec_jets(events, \"2018\")\n",
    "fatjets = events_1.FatJet\n",
    "\n",
    "preselection_cut_1 = pad_val(\n",
    "        ( ak.max(events_1.FatJet.pt, axis = 1) > preselection_cut_vals[\"leading_pt\"])\n",
    "        * (ak.max(events_1.FatJet.msoftdrop, axis = 1) > preselection_cut_vals[\"maximum_mass\"]), #mass and pT cut of each jet in event\n",
    "        len(events_1), #pad to num_jets length\n",
    "        False,  #pad with value False\n",
    "        )\n",
    "# finally with the length of events number, \"1\" for all jets are pT > pT_cut and mass > mass_cut\n",
    " # N.B. here clip always = True\n",
    "\n",
    "add_selection(\n",
    "    \"leading pT and maximum mass\", #string name\n",
    "    preselection_cut_1.astype(bool), #selection content\n",
    "    selection, #PackedSelection object\n",
    "    cutflow, #cut-flow dict, storing events number after each cut\n",
    "    isData,\n",
    "    signGenWeights,#sum the signGenWeights for events which pass the selection\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "preselection_cut_2 = np.prod(\n",
    "    pad_val(\n",
    "        (events_1.FatJet.pt > preselection_cut_vals[\"pt\"]),\n",
    "        # * (events.FatJet.msoftdrop > preselection_cut_vals[\"msd\"]), #mass and pT cut of each jet in event\n",
    "        num_jets, #pad to num_jets length\n",
    "        False,  #pad with value False\n",
    "        axis=1, #pad to axis=1\n",
    "    ),\n",
    "    axis=1,\n",
    ")# finally with the length of events number, \"1\" for all jets are pT > pT_cut and mass > mass_cut\n",
    " # N.B. here clip always = True\n",
    "\n",
    "add_selection(\n",
    "    \"at least 2 AK8 jet with pT >200GeV\", #string name\n",
    "    preselection_cut_2.astype(bool), #selection content\n",
    "    selection, #PackedSelection object\n",
    "    cutflow, #cut-flow dict, storing events number after each cut\n",
    "    isData,\n",
    "    signGenWeights,#sum the signGenWeights for events which pass the selection\n",
    ")\n",
    "\n",
    "preselection_cut_3 = pad_val(\n",
    "        (ak.num(events_1.FatJet.pt) == 2) | (ak.num(events_1.FatJet.pt) == 3) , #mass and pT cut of each jet in event\n",
    "        len(events_1), #pad to num_jets length\n",
    "        False,  #pad with value False\n",
    "        )\n",
    "\n",
    "add_selection(\n",
    "    \"2 or 3 AK8 jet\", #string name\n",
    "    preselection_cut_3.astype(bool), #selection content\n",
    "    selection, #PackedSelection object\n",
    "    cutflow, #cut-flow dict, storing events number after each cut\n",
    "    isData,\n",
    "    signGenWeights,#sum the signGenWeights for events which pass the selection\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': 2688878,\n",
       " 'leading pT and maximum mass': 2637411.0,\n",
       " 'at least 2 AK8 jet with pT >200GeV': 2637249.0,\n",
       " '2 or 3 AK8 jet': 2513907.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_2 = events_1[selection.all(*selection.names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NanoEventsArray [<event 1:142148:142147035>, ... ] type='2581743 * event'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del events_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_3 = events_2\n",
    "del events_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preselection done here, but we still have to do Top enrich selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find leading tagger score jet, denote it as leading_fatjet\n",
    "eventsScoreFatjet = events_3\n",
    "tagger_scores = eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWev0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWev1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWmv0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWmv1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWq0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWq1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWq2c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWqq0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWqq1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWqq2c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtauev0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtauev1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtauhv0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtauhv1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtaumv0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtaumv1c\n",
    "#get maximum tagger score jet index\n",
    "max_tagger_indices = (tagger_scores == np.max(tagger_scores, axis=1))\n",
    "fatjets = events_3.FatJet\n",
    "leading_fatjet = fatjets[max_tagger_indices][:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect inclusive tight b jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "btagDeepFlavB_WP = {\n",
    "        \"bWPloose\"  : 0.0490,\n",
    "        \"bWPmedium\" : 0.2783,\n",
    "        \"bWPtight\"  : 0.7100,\n",
    "    } #2018 btagDeepFlavB working point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top enriched-selection:\n",
    "#1. 125GeV<Mja<225GeV\n",
    "#2. MET_pt/PTja>0.4\n",
    "#3. PTja>500GeV\n",
    "#4. Tight WP inclusive b jets >= 1\n",
    "\n",
    "isData = False\n",
    "signGenWeights = None if isData else np.sign(events_3[\"genWeight\"]) #get genWeight sign, because only the sign matters\n",
    "n_events = len(events_3) if isData else int(np.sum(signGenWeights)) #events number for MC events should be the sum of \"sign\"\n",
    "selection = PackedSelection() #initialize a new object\n",
    "\n",
    "cutflow = {}\n",
    "# cutflow[\"all\"] = len(events) #shouldn't be n_events?\n",
    "cutflow[\"all\"] = n_events\n",
    "muon_enriched_cut_vals = {\"pt_min\": 400, \"MET_pt_min\":0.4, \"msd_min\": 125, \"msd_max\":225}\n",
    "\n",
    "muon_enriched_cut_1 = pad_val(\n",
    "        ( leading_fatjet.msoftdrop >= muon_enriched_cut_vals[\"msd_min\"])\n",
    "        * ( leading_fatjet.msoftdrop <= muon_enriched_cut_vals[\"msd_max\"])\n",
    "        * ( leading_fatjet.pt >= muon_enriched_cut_vals[\"pt_min\"]) \n",
    "        * ( events_3.MET.pt/leading_fatjet.pt >= 0.4), #mass and pT cut of each jet in event\n",
    "        len(events_3), #pad to num_jets length\n",
    "        False,  #pad with value False\n",
    "        )\n",
    "\n",
    "add_selection(\n",
    "    \"pt mass MET selection\", #string name\n",
    "    muon_enriched_cut_1.astype(bool), #selection content\n",
    "    selection, #PackedSelection object\n",
    "    cutflow, #cut-flow dict, storing events number after each cut\n",
    "    isData,\n",
    "    signGenWeights,#sum the signGenWeights for events which pass the selection\n",
    ")\n",
    "\n",
    "nb_t_inclusive_collections = (events_3.Jet.btagDeepFlavB >= btagDeepFlavB_WP[\"bWPtight\"])\n",
    "nb_t_inclusive = np.sum(nb_t_inclusive_collections, axis = 1)\n",
    "\n",
    "muon_enriched_cut_2 = pad_val(\n",
    "    (nb_t_inclusive >= 1),\n",
    "    len(events_3),\n",
    "    False,\n",
    ")# finally with the length of events number, \"1\" for all jets are pT > pT_cut and mass > mass_cut\n",
    " # N.B. here clip always = True\n",
    "\n",
    "add_selection(\n",
    "    \"at least 1 inclusive tight b jets\", #string name\n",
    "    muon_enriched_cut_2.astype(bool), #selection content\n",
    "    selection, #PackedSelection object\n",
    "    cutflow, #cut-flow dict, storing events number after each cut\n",
    "    isData,\n",
    "    signGenWeights,#sum the signGenWeights for events which pass the selection\n",
    ")\n",
    "\n",
    "delta_phi = np.subtract(events_3.MET.phi, leading_fatjet.phi)\n",
    "delta_phi = np.where(delta_phi > np.pi, delta_phi - 2*np.pi, delta_phi)\n",
    "delta_phi = np.where(delta_phi < -np.pi, delta_phi + 2*np.pi, delta_phi)\n",
    "delta_phi = np.abs(delta_phi)\n",
    "\n",
    "\n",
    "dphi_cut = pad_val(\n",
    "    (delta_phi >= 2.0),\n",
    "    len(events_3),\n",
    "    False,\n",
    ")# finally with the length of events number, \"1\" for all jets are pT > pT_cut and mass > mass_cut\n",
    " # N.B. here clip always = True\n",
    " \n",
    "add_selection(\n",
    "    \"dphi cut\", #string name\n",
    "    dphi_cut.astype(bool), #selection content\n",
    "    selection, #PackedSelection object\n",
    "    cutflow, #cut-flow dict, storing events number after each cut\n",
    "    isData,\n",
    "    signGenWeights,#sum the signGenWeights for events which pass the selection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': 2513907,\n",
       " 'pt mass MET selection': 139264.0,\n",
       " 'at least 1 inclusive tight b jets': 117859.0,\n",
       " 'dphi cut': 100239.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cutflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_4 = events_3[selection.all(*selection.names)]\n",
    "del events_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start to run LP stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = PackedSelection() #initialize a new object\n",
    "isData = False\n",
    "signGenWeights = None if isData else np.sign(events_4[\"genWeight\"]) #get genWeight sign, because only the sign matters\n",
    "n_events = len(events_4) if isData else int(np.sum(signGenWeights)) #events number for MC events should be the sum of \"sign\"\n",
    "selection = PackedSelection() #initialize a new object\n",
    "\n",
    "cutflow = {}\n",
    "# cutflow[\"all\"] = len(events) #shouldn't be n_events?\n",
    "cutflow[\"all\"] = n_events\n",
    "\n",
    "#require WW decaying to 4q, because we want to calibrate H3q4q jet at first stage\n",
    "d_PDGID = 1\n",
    "u_PDGID = 2\n",
    "s_PDGID = 3\n",
    "c_PDGID = 4\n",
    "b_PDGID = 5\n",
    "g_PDGID = 21\n",
    "TOP_PDGID = 6\n",
    "\n",
    "ELE_PDGID = 11\n",
    "vELE_PDGID = 12\n",
    "MU_PDGID = 13\n",
    "vMU_PDGID = 14\n",
    "TAU_PDGID = 15\n",
    "vTAU_PDGID = 16\n",
    "\n",
    "Z_PDGID = 23\n",
    "W_PDGID = 24\n",
    "HIGGS_PDGID = 25\n",
    "Y_PDGID = 35\n",
    "\n",
    "b_PDGIDS = [511, 521, 523]\n",
    "\n",
    "GRAV_PDGID = 39\n",
    "\n",
    "GEN_FLAGS = [\"fromHardProcess\", \"isLastCopy\"]\n",
    "\n",
    "FILL_NONE_VALUE = -99999\n",
    "PAD_VAL = -99999\n",
    "\n",
    "skim_vars = {\n",
    "    \"eta\": \"Eta\",\n",
    "    \"phi\": \"Phi\",\n",
    "    \"mass\": \"Mass\",\n",
    "    \"pt\": \"Pt\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start to match top, b and W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107157\n"
     ]
    }
   ],
   "source": [
    "print(len(events_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find gen-level top quark\n",
    "tops = events_4.GenPart[\n",
    "        (abs(events_4.GenPart.pdgId) == TOP_PDGID) * events_4.GenPart.hasFlags(GEN_FLAGS)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new leading tagger score jet, denote it as leading_fatjet\n",
    "eventsScoreFatjet = events_4\n",
    "tagger_scores = eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWev0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWev1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWmv0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWmv1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWq0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWq1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWq2c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWqq0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWqq1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWqq2c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtauev0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtauev1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtauhv0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtauhv1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtaumv0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtaumv1c\n",
    "#get maximum tagger score jet index\n",
    "max_tagger_indices = (tagger_scores == np.max(tagger_scores, axis=1))\n",
    "fatjets = events_4.FatJet\n",
    "leading_fatjet = fatjets[max_tagger_indices][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find top children, refer to:https://github.com/rkansal47/HHbbVV/blob/2fc4110669081d565e115d32486c7d555db5bac7/src/HHbbVV/processors/GenSelection.py#L888\n",
    "tops_children = tops.distinctChildren\n",
    "tops_children = tops_children[tops_children.hasFlags(GEN_FLAGS)]\n",
    "#may take some time since it's recursive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get gen-level W informatino\n",
    "ws = ak.flatten(tops_children[np.abs(tops_children.pdgId) == W_PDGID], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107157"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hadronic W and top\n",
    "had_top_sel = np.all(np.abs(ws.children.pdgId) <= 5, axis=2)\n",
    "had_ws = ak.flatten(ws[had_top_sel])\n",
    "had_ws_children = had_ws.children\n",
    "had_tops = ak.flatten(tops[had_top_sel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get leptonic W and top(since we are using ttbar_semilep sample)\n",
    "lep_top_sel = np.all(np.abs(ws.children.pdgId) >= 11, axis=2)\n",
    "lep_ws = ak.flatten(ws[lep_top_sel])\n",
    "lep_ws_children = lep_ws.children\n",
    "lep_tops = ak.flatten(tops[lep_top_sel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lep_leps = lep_ws_children[(np.abs(lep_ws_children.pdgId) == 11) | (np.abs(lep_ws_children.pdgId) == 13) | (np.abs(lep_ws_children.pdgId) == 15)] #select lepton but not nu, it's always the first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for b's from top\n",
    "had_top_children = ak.flatten(tops_children[had_top_sel], axis=1)\n",
    "had_bs = had_top_children[np.abs(had_top_children.pdgId) == 5]\n",
    "# add_selection(\"top_has_bs\", np.any(had_bs.pdgId, axis=1), *selection_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b quark and hadronic decay W boson from the top\n",
    "# gen_quarks = ak.concatenate([had_bs[:, :1], had_ws_children[:, :2]], axis=1)\n",
    "had_bs.eta[:, :1]\n",
    "gen_quarks_eta = ak.concatenate([had_bs.eta[:, :1], had_ws_children.eta[:, :2]], axis=1)\n",
    "gen_quarks_phi = ak.concatenate([had_bs.phi[:, :1], had_ws_children.phi[:, :2]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly, collect lepton and hadronic decay W boson from the top\n",
    "gen_quarks_lepton_eta = ak.concatenate([lep_leps.eta[:,:1], had_ws_children.eta[:, :2]], axis=1)\n",
    "gen_quarks_lepton_phi = ak.concatenate([lep_leps.phi[:,:1], had_ws_children.phi[:, :2]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaR = 0.8\n",
    "had_w_jet_match = ak.fill_none(\n",
    "    ak.all(had_ws_children.delta_r(leading_fatjet) < deltaR, axis=1), False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaR = 0.8\n",
    "lep_jet_match = ak.flatten(\n",
    "    pad_val(\n",
    "        ak.fill_none(lep_leps.delta_r(leading_fatjet) < deltaR, [], axis=0),\n",
    "        1,\n",
    "        False,\n",
    "        axis=1,\n",
    "        to_numpy=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaR = 0.8\n",
    "had_b_jet_match = ak.flatten(\n",
    "    pad_val(\n",
    "        ak.fill_none(had_bs.delta_r(leading_fatjet) < deltaR, [], axis=0),\n",
    "        1,\n",
    "        False,\n",
    "        axis=1,\n",
    "        to_numpy=False,\n",
    "    )\n",
    ")\n",
    "top_match_dict = {\n",
    "    \"top_matched\": had_w_jet_match * had_b_jet_match * ~lep_jet_match,\n",
    "    \"w_matched\": had_w_jet_match * ~had_b_jet_match * ~lep_jet_match,\n",
    "    \"tlqq_matched\" : had_w_jet_match * lep_jet_match,\n",
    "    \"unmatched\": ~had_w_jet_match,\n",
    "}\n",
    "top_match_dict = {key: val.to_numpy().astype(int) for key, val in top_match_dict.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store the PFCands, GenEtaPhi, Higgs candidate AK8 jet 4-vector information for Lund Plane use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NanoEventsArray [<event 1:143567:143566774>, ... ] type='62517 * event'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only select events whose jet_a is top jets\n",
    "events_after_cut = events_4[top_match_dict[\"top_matched\"].astype(bool)]\n",
    "events_after_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### four vector for HWW jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "HWWJets = leading_fatjet[top_match_dict[\"top_matched\"].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([615.       ,  -0.9633789,   0.7246094, 187.       ], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# four vector for HWW jet\n",
    "higgs_jet_4vec = np.array(np.stack((np.array(HWWJets.pt), np.array(HWWJets.eta),np.array(HWWJets.phi),np.array(HWWJets.mass)), axis=1))\n",
    "higgs_jet_4vec[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62517"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(higgs_jet_4vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next eta-phi for 4 quarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here only the quarks from tbqq matched are used\n",
    "eta = gen_quarks_eta[top_match_dict[\"top_matched\"].astype(bool)].to_numpy()\n",
    "phi = gen_quarks_phi[top_match_dict[\"top_matched\"].astype(bool)].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_parts_eta_phi_bqq = np.array(np.dstack((eta,phi)))\n",
    "# can do test like : gen_parts_eta_phi[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get FatJetPFCands 4-vector, up to 150 length to suit the input of Oz's function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NanoEventsArray [<event 1:143567:143566774>, ... ] type='62517 * event'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_final = events_after_cut\n",
    "events_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HWW_jet_idx = ak.argmax(tagger_scores[], axis=1)\n",
    "eventsScoreFatjet = events_final\n",
    "tagger_scores = eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWev0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWev1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWmv0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWmv1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWq0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWq1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWq2c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWqq0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWqq1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWqq2c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtauev0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtauev1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtauhv0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtauhv1c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtaumv0c + eventsScoreFatjet.FatJet.inclParTMDV1_probHWqqWtaumv1c\n",
    "#get maximum tagger score jet index\n",
    "HWW_jet_idx = ak.argmax(tagger_scores, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [0, 0, 0, 0, 0, 0, ... 0, 0, 0, 0, 0, 0] type='62517 * ?int64'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HWW_jet_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get the jet_idx HWW jet, each event has one jet_idx\n",
    "# HWW_match = WWdr <= match_dR #FatJetIdx in each event, which is real HWW jet\n",
    "# HWW_match_padded = pad_val(HWW_match,3,False,1,True) #pad the array with False value\n",
    "# HWW_jet_idx = np.argmax(HWW_match_padded,axis = 1) #the jet index in each jet which is true HWW jet\n",
    "# then get all the FatJetPFCands according to the jet_idx, and get PF_idx\n",
    "HWW_FatJetPFCands = (events_final.FatJetPFCands.jetIdx == HWW_jet_idx)\n",
    "HWW_FatJetPFCands_pFCandsIdx = events_final.FatJetPFCands.pFCandsIdx[HWW_FatJetPFCands]\n",
    "# at last, get PFCands 4-vector according to the PF_idx in last step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [True, True, True, ... False, False] type='101 * bool'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HWW_FatJetPFCands[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [14, 17, 18, 25, ... 190, 192, 193, 199] type='68 * int32[parameters={\"__...'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HWW_FatJetPFCands_pFCandsIdx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_array =   ak.Array(events_final.PFCands.pt)\n",
    "eta_array =  ak.Array(events_final.PFCands.eta)\n",
    "phi_array =  ak.Array(events_final.PFCands.phi)\n",
    "mass_array = ak.Array(events_final.PFCands.mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pt =  pt_array[HWW_FatJetPFCands_pFCandsIdx]\n",
    "selected_eta = eta_array[HWW_FatJetPFCands_pFCandsIdx]\n",
    "selected_phi = phi_array[HWW_FatJetPFCands_pFCandsIdx]\n",
    "selected_mass = mass_array[HWW_FatJetPFCands_pFCandsIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pt_padded = pad_val(selected_pt,150,0,1,True)\n",
    "selected_eta_padded = pad_val(selected_eta,150,0,1,True)\n",
    "selected_phi_padded = pad_val(selected_phi,150,0,1,True)\n",
    "selected_mass_padded = pad_val(selected_mass,150,0,1,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct (px,py,pz,E) using (pt,eta,phi,mass) information as the input\n",
    "pf_cands_px = selected_pt_padded * np.cos(selected_phi_padded)\n",
    "pf_cands_py = selected_pt_padded * np.sin(selected_phi_padded)\n",
    "pf_cands_pz = selected_pt_padded * np.sinh(selected_eta_padded)\n",
    "pf_cands_E = np.sqrt(pf_cands_px**2 + pf_cands_py**2 + pf_cands_pz**2 + selected_mass_padded**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_cands_pxpypzE = np.dstack((pf_cands_px,pf_cands_py,pf_cands_pz,pf_cands_E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_cands_4vec = np.dstack((selected_pt_padded,selected_eta_padded,selected_phi_padded,selected_mass_padded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next compute tagger score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_scores = (leading_fatjet.inclParTMDV1_probHWqqWev0c + leading_fatjet.inclParTMDV1_probHWqqWev1c + leading_fatjet.inclParTMDV1_probHWqqWmv0c + leading_fatjet.inclParTMDV1_probHWqqWmv1c + leading_fatjet.inclParTMDV1_probHWqqWq0c + leading_fatjet.inclParTMDV1_probHWqqWq1c + leading_fatjet.inclParTMDV1_probHWqqWq2c + leading_fatjet.inclParTMDV1_probHWqqWqq0c + leading_fatjet.inclParTMDV1_probHWqqWqq1c + leading_fatjet.inclParTMDV1_probHWqqWqq2c + leading_fatjet.inclParTMDV1_probHWqqWtauev0c + leading_fatjet.inclParTMDV1_probHWqqWtauev1c + leading_fatjet.inclParTMDV1_probHWqqWtauhv0c + leading_fatjet.inclParTMDV1_probHWqqWtauhv1c + leading_fatjet.inclParTMDV1_probHWqqWtaumv0c + leading_fatjet.inclParTMDV1_probHWqqWtaumv1c)/(ak.ones_like(leading_fatjet.inclParTMDV1_probHWqqWev0c) - leading_fatjet.inclParTMDV1_probTopbWtauev - leading_fatjet.inclParTMDV1_probTopbWq0c - leading_fatjet.inclParTMDV1_probTopbWmv - leading_fatjet.inclParTMDV1_probTopbWev - leading_fatjet.inclParTMDV1_probTopbWqq0c - leading_fatjet.inclParTMDV1_probTopbWqq1c - leading_fatjet.inclParTMDV1_probTopbWtauhv - leading_fatjet.inclParTMDV1_probTopbWq1c -leading_fatjet.inclParTMDV1_probTopbWtaumv)\n",
    "HWWJets_tagger_score = tagger_scores[top_match_dict[\"top_matched\"].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [0.055, 0.263, 0.0344, ... 0.162, 0.147] type='62517 * ?float32'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HWWJets_tagger_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate SFs and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nominal efficiency 0.258, Corrected efficiency 0.268, SF (corrected / nom) 1.039\n",
      "Stat variation toys eff. avg 0.266, std dev 0.005\n",
      "Pt variation toys eff. avg 0.268, std dev 0.001\n",
      "\n",
      "\n",
      "Calibrated efficiency  is 0.268 +/- 0.007  (stat) +/- 0.001 (pt) +/- -0.008/0.009 (sys)+/- 0.047 (matching)  \n",
      "\n",
      "\n",
      "Now perform SFs information\n",
      "SF (corrected / nom) 1.039\n",
      "tagger cut =  [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
      "\n",
      "\n",
      "SF is 1.039 +/-0.028(stat) +/-0.003(pt) +0.008/-0.009(sys) +/-0.182(match) \n",
      "\n",
      "\n",
      "total unc. =  0.1845683994064236\n",
      "*******\n",
      "\n",
      "Nominal efficiency 0.224, Corrected efficiency 0.234, SF (corrected / nom) 1.041\n",
      "Stat variation toys eff. avg 0.233, std dev 0.005\n",
      "Pt variation toys eff. avg 0.234, std dev 0.001\n",
      "\n",
      "\n",
      "Calibrated efficiency  is 0.234 +/- 0.006  (stat) +/- 0.001 (pt) +/- -0.008/0.008 (sys)+/- 0.041 (matching)  \n",
      "\n",
      "\n",
      "Now perform SFs information\n",
      "SF (corrected / nom) 1.041\n",
      "tagger cut =  [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
      "\n",
      "\n",
      "SF is 1.041 +/-0.025(stat) +/-0.003(pt) +0.008/-0.008(sys) +/-0.183(match) \n",
      "\n",
      "\n",
      "total unc. =  0.18452214662870234\n",
      "*******\n",
      "\n",
      "Nominal efficiency 0.190, Corrected efficiency 0.199, SF (corrected / nom) 1.047\n",
      "Stat variation toys eff. avg 0.196, std dev 0.005\n",
      "Pt variation toys eff. avg 0.198, std dev 0.001\n",
      "\n",
      "\n",
      "Calibrated efficiency  is 0.199 +/- 0.007  (stat) +/- 0.001 (pt) +/- -0.007/0.006 (sys)+/- 0.035 (matching)  \n",
      "\n",
      "\n",
      "Now perform SFs information\n",
      "SF (corrected / nom) 1.047\n",
      "tagger cut =  [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
      "\n",
      "\n",
      "SF is 1.047 +/-0.037(stat) +/-0.003(pt) +0.007/-0.006(sys) +/-0.184(match) \n",
      "\n",
      "\n",
      "total unc. =  0.18741174553967097\n",
      "*******\n",
      "\n",
      "Nominal efficiency 0.157, Corrected efficiency 0.167, SF (corrected / nom) 1.060\n",
      "Stat variation toys eff. avg 0.165, std dev 0.004\n",
      "Pt variation toys eff. avg 0.167, std dev 0.000\n",
      "\n",
      "\n",
      "Calibrated efficiency  is 0.167 +/- 0.005  (stat) +/- 0.001 (pt) +/- -0.006/0.006 (sys)+/- 0.029 (matching)  \n",
      "\n",
      "\n",
      "Now perform SFs information\n",
      "SF (corrected / nom) 1.060\n",
      "tagger cut =  [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
      "\n",
      "\n",
      "SF is 1.060 +/-0.033(stat) +/-0.003(pt) +0.006/-0.006(sys) +/-0.186(match) \n",
      "\n",
      "\n",
      "total unc. =  0.18906199981639282\n",
      "*******\n",
      "\n",
      "Nominal efficiency 0.123, Corrected efficiency 0.132, SF (corrected / nom) 1.074\n",
      "Stat variation toys eff. avg 0.130, std dev 0.003\n",
      "Pt variation toys eff. avg 0.132, std dev 0.000\n",
      "\n",
      "\n",
      "Calibrated efficiency  is 0.132 +/- 0.005  (stat) +/- 0.000 (pt) +/- -0.006/0.005 (sys)+/- 0.023 (matching)  \n",
      "\n",
      "\n",
      "Now perform SFs information\n",
      "SF (corrected / nom) 1.074\n",
      "tagger cut =  [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
      "\n",
      "\n",
      "SF is 1.074 +/-0.041(stat) +/-0.004(pt) +0.006/-0.005(sys) +/-0.188(match) \n",
      "\n",
      "\n",
      "total unc. =  0.19282479916506565\n",
      "*******\n",
      "\n",
      "Nominal efficiency 0.090, Corrected efficiency 0.097, SF (corrected / nom) 1.077\n",
      "Stat variation toys eff. avg 0.096, std dev 0.003\n",
      "Pt variation toys eff. avg 0.097, std dev 0.000\n",
      "\n",
      "\n",
      "Calibrated efficiency  is 0.097 +/- 0.004  (stat) +/- 0.000 (pt) +/- -0.004/0.004 (sys)+/- 0.017 (matching)  \n",
      "\n",
      "\n",
      "Now perform SFs information\n",
      "SF (corrected / nom) 1.077\n",
      "tagger cut =  [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
      "\n",
      "\n",
      "SF is 1.077 +/-0.041(stat) +/-0.004(pt) +0.004/-0.004(sys) +/-0.189(match) \n",
      "\n",
      "\n",
      "total unc. =  0.19351897042537736\n",
      "*******\n",
      "\n",
      "Nominal efficiency 0.056, Corrected efficiency 0.060, SF (corrected / nom) 1.074\n",
      "Stat variation toys eff. avg 0.059, std dev 0.002\n",
      "Pt variation toys eff. avg 0.060, std dev 0.000\n",
      "\n",
      "\n",
      "Calibrated efficiency  is 0.060 +/- 0.003  (stat) +/- 0.000 (pt) +/- -0.002/0.002 (sys)+/- 0.011 (matching)  \n",
      "\n",
      "\n",
      "Now perform SFs information\n",
      "SF (corrected / nom) 1.074\n",
      "tagger cut =  [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
      "\n",
      "\n",
      "SF is 1.074 +/-0.050(stat) +/-0.004(pt) +0.002/-0.002(sys) +/-0.188(match) \n",
      "\n",
      "\n",
      "total unc. =  0.1949087840370128\n",
      "*******\n",
      "\n",
      "Nominal efficiency 0.024, Corrected efficiency 0.025, SF (corrected / nom) 1.067\n",
      "Stat variation toys eff. avg 0.025, std dev 0.001\n",
      "Pt variation toys eff. avg 0.025, std dev 0.000\n",
      "\n",
      "\n",
      "Calibrated efficiency  is 0.025 +/- 0.001  (stat) +/- 0.000 (pt) +/- -0.001/0.002 (sys)+/- 0.004 (matching)  \n",
      "\n",
      "\n",
      "Now perform SFs information\n",
      "SF (corrected / nom) 1.067\n",
      "tagger cut =  [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
      "\n",
      "\n",
      "SF is 1.067 +/-0.045(stat) +/-0.005(pt) +0.001/-0.002(sys) +/-0.187(match) \n",
      "\n",
      "\n",
      "total unc. =  0.19262257722329712\n",
      "*******\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, '')\n",
    "sys.path.append(\"/home/pku/zhaoyz/Higgs/LundReweighting\")\n",
    "from utils.LundReweighter import *\n",
    "from utils.Utils import *\n",
    "\"\"\" An example how to use the Lund Plane reweighting  code \"\"\"\n",
    "\n",
    "######################## Setup \n",
    "\n",
    "#Input file \n",
    "fname = \"/home/pku/zhaoyz/Higgs/LundReweighting/data/example_signal.h5\"\n",
    "#File containing data/MC Lund Plane ratio\n",
    "f_ratio_name = '/home/pku/zhaoyz/Higgs/LundReweighting/data/ratio_2018.root'\n",
    "\n",
    "f_sig = h5py.File(fname, \"r\")\n",
    "f_ratio = ROOT.TFile.Open(f_ratio_name)\n",
    "\n",
    "#Class to help read input dataset, \"Dataset\" class defined in Utils.py\n",
    "# d = Dataset(f_sig, dtype = 1)\n",
    "# d.compute_obs() #there may be some dedicated variables inside the .h5 file\n",
    "\n",
    "#The cut we will compute a SF for 'tau21 < 0.34'\n",
    "tag_obs = 'tau21'\n",
    "score_thresh = 0.975\n",
    "\n",
    "\n",
    "#nominal data/MC Lund plane ratio (3d histogram)\n",
    "h_ratio = f_ratio.Get(\"ratio_nom\")\n",
    "#systematic variations\n",
    "h_ratio_sys_up = f_ratio.Get(\"ratio_sys_tot_up\")\n",
    "h_ratio_sys_down = f_ratio.Get(\"ratio_sys_tot_down\")\n",
    "#MC ratio of b to light quarks\n",
    "b_light_ratio = f_ratio.Get(\"h_bl_ratio\")\n",
    "\n",
    "\n",
    "#directory of pt extrapolation fits\n",
    "f_ratio.cd('pt_extrap')\n",
    "rdir = ROOT.gDirectory #get the present working directory and give it to rdir\n",
    "\n",
    "#Main class for reweighting utilities\n",
    "LP_rw = LundReweighter(pt_extrap_dir = rdir)\n",
    "\n",
    "max_evts = 50000\n",
    "tagger_cut_value = [0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95]\n",
    "for value_i in tagger_cut_value:\n",
    "    # score = getattr(d, tag_obs)[:max_evts]\n",
    "    score_cut = ((HWWJets_tagger_score >= value_i ) & (HWWJets_tagger_score <= (value_i + 0.5)))\n",
    "    # score_cut = ((HWWJets_tagger_score >= 0.9) & (HWWJets_tagger_score <= 0.95))\n",
    "    # score_cut = (HWWJets_tagger_score > 0.95)\n",
    "    score_cut = score_cut[:max_evts]\n",
    "\n",
    "\n",
    "    #Number of toys for statistical and pt extrapolation uncertainties\n",
    "    nToys = 100\n",
    "    #Noise vectors used to to generate the toys\n",
    "    #NOTE the same vector has to be used for the whole sample/signal file for the toys to be consistent \n",
    "    rand_noise = np.random.normal(size = (nToys, h_ratio.GetNbinsX(), h_ratio.GetNbinsY(), h_ratio.GetNbinsZ()))\n",
    "    pt_rand_noise = np.random.normal(size = (nToys, h_ratio.GetNbinsY(), h_ratio.GetNbinsZ(), 3))\n",
    "\n",
    "\n",
    "    ################### Compute reweighting factors\n",
    "\n",
    "    #PF candidates in the AK8 jet\n",
    "    # pf_cands = d.get_masked(\"jet1_PFCands\").astype(np.float64)[:max_evts]\n",
    "    pf_cands = pf_cands_pxpypzE[:max_evts]\n",
    "    #Generator level quarks from hard process\n",
    "\n",
    "    # gen_parts = d.get_masked('gen_info')[:max_evts]\n",
    "    gen_parts_eta_phi = gen_parts_eta_phi_bqq[:max_evts]\n",
    "    # gen_parts_pdg_ids = gen_parts[:,:,3]\n",
    "\n",
    "    B_PDG_ID = 5\n",
    "\n",
    "    # ak8_jets = d.get_masked('jet_kinematics')[:max_evts][:,2:6].astype(np.float64)\n",
    "    ak8_jets = higgs_jet_4vec[:max_evts]\n",
    "\n",
    "    #Nominal event weights of the MC, assume every event is weight '1' for this example\n",
    "    weights_nom = np.ones(max_evts)\n",
    "\n",
    "    LP_weights = []\n",
    "    LP_weights_sys_up = []\n",
    "    LP_weights_sys_down = []\n",
    "    stat_smeared_weights = []\n",
    "    pt_smeared_weights = []\n",
    "    b_weights_up = []\n",
    "    b_weights_down = []\n",
    "    bad_matches = []\n",
    "\n",
    "\n",
    "    for i,cands in enumerate(pf_cands):\n",
    "        # if i == 4: break\n",
    "        # print(\"now processing:\",i)\n",
    "        #Get the subjets, splittings and checking matching based on PF candidates in the jet and gen-level quarks\n",
    "        subjets, splittings, bad_match, deltaRs = LP_rw.get_splittings_and_matching(cands, gen_parts_eta_phi[i], ak8_jets[i])\n",
    "        # print(bad_match)\n",
    "        # print(deltaRs)\n",
    "        #Gets the nominal LP reweighting factor for this event and statistical + pt extrapolation toys\n",
    "        LP_weight, stat_smeared_weight, pt_smeared_weight = LP_rw.reweight_lund_plane(h_rw = h_ratio, subjets = subjets, splittings = splittings,\n",
    "                rand_noise = rand_noise, pt_rand_noise = pt_rand_noise, )\n",
    "        #Now get systematic variations\n",
    "        LP_weight_sys_up,_,_ = LP_rw.reweight_lund_plane(h_rw = h_ratio_sys_up, subjets = subjets, splittings = splittings)\n",
    "        LP_weight_sys_down,_,_ = LP_rw.reweight_lund_plane(h_rw = h_ratio_sys_down, subjets = subjets, splittings = splittings)\n",
    "\n",
    "        LP_weights.append(LP_weight)\n",
    "        stat_smeared_weights.append(stat_smeared_weight)\n",
    "        pt_smeared_weights.append(pt_smeared_weight)\n",
    "\n",
    "        LP_weights_sys_up.append(LP_weight_sys_up)\n",
    "        LP_weights_sys_down.append(LP_weight_sys_down)\n",
    "        bad_matches.append(bad_match)\n",
    "\n",
    "\n",
    "\n",
    "    ############### Normalize weights to preserve normalization of the MC sample\n",
    "\n",
    "    #The nominal Lund Plane correction event weights\n",
    "    LP_weights = LP_rw.normalize_weights(LP_weights) * weights_nom \n",
    "\n",
    "    #Toy variations for stat and pt uncertainties\n",
    "    stat_smeared_weights = LP_rw.normalize_weights(stat_smeared_weights) * weights_nom.reshape(max_evts, 1)\n",
    "    pt_smeared_weights = LP_rw.normalize_weights(pt_smeared_weights) * weights_nom.reshape(max_evts,1)\n",
    "\n",
    "    #Systematic up/down variations\n",
    "    LP_weights_sys_up = LP_rw.normalize_weights(LP_weights_sys_up) * weights_nom\n",
    "    LP_weights_sys_down = LP_rw.normalize_weights(LP_weights_sys_down) * weights_nom\n",
    "\n",
    "    ############### Compute efficiences and uncertainties\n",
    "\n",
    "\n",
    "    #Efficiency of the cut in nominal MC\n",
    "    eff_nom = np.average(score_cut, weights = weights_nom) #TODO\n",
    "\n",
    "    #Efficiency of the cut after the Lund Plane reweighting\n",
    "    eff_rw = np.average(score_cut, weights = LP_weights)\n",
    "\n",
    "    #Nominal 'scale factor'\n",
    "    SF = eff_rw / eff_nom\n",
    "\n",
    "    print(\"Nominal efficiency %.3f, Corrected efficiency %.3f, SF (corrected / nom) %.3f\" % (eff_nom, eff_rw, SF))\n",
    "\n",
    "    #NOTE, better to use corrected efficiency computed separately for each sample rather than a single 'SF'\n",
    "\n",
    "\n",
    "    #Compute efficiency for each of the stat/pt toys\n",
    "    eff_toys = []\n",
    "    pt_eff_toys = []\n",
    "    for i in range(nToys):\n",
    "        eff = np.average(score_cut, weights = stat_smeared_weights[:,i])\n",
    "        eff_toys.append(eff)\n",
    "\n",
    "        eff1 = np.average(score_cut, weights = pt_smeared_weights[:,i])\n",
    "        pt_eff_toys.append(eff1)\n",
    "\n",
    "    #Compute stat and pt uncertainty based on variation in the toys\n",
    "    toys_mean = np.mean(eff_toys)\n",
    "    toys_std = np.std(eff_toys)\n",
    "    pt_toys_mean = np.mean(pt_eff_toys)\n",
    "    pt_toys_std = np.std(pt_eff_toys)\n",
    "\n",
    "    eff_stat_unc = (abs(toys_mean - eff_rw)  + toys_std) \n",
    "    eff_pt_unc = (abs(pt_toys_mean - eff_rw) + pt_toys_std)\n",
    "\n",
    "    print(\"Stat variation toys eff. avg %.3f, std dev %.3f\" % (toys_mean, toys_std))\n",
    "    print(\"Pt variation toys eff. avg %.3f, std dev %.3f\" % (pt_toys_mean, pt_toys_std))\n",
    "\n",
    "\n",
    "    #Compute difference in efficiency due to weight variations as uncertainty\n",
    "    def get_uncs(score_cut, weights_up, weights_down, eff_baseline):\n",
    "        eff_up =  np.average(score_cut, weights = weights_up)\n",
    "        eff_down =  np.average(score_cut, weights = weights_down)\n",
    "\n",
    "        unc_up = eff_up - eff_baseline\n",
    "        unc_down = eff_down - eff_baseline \n",
    "        return unc_up, unc_down\n",
    "\n",
    "\n",
    "    #Compute efficiency of systematic variations\n",
    "    sys_unc_up, sys_unc_down = get_uncs(score_cut, LP_weights_sys_up, LP_weights_sys_down, eff_rw)\n",
    "    # b_unc_up, b_unc_down = get_uncs(score_cut, b_weights_up, b_weights_down, eff_rw)\n",
    "\n",
    "\n",
    "    #matching uncertainty, taken as a fractional uncertainty on efficiency\n",
    "    bad_match_frac = np.mean(bad_matches)\n",
    "    bad_match_unc = bad_match_frac * eff_rw\n",
    "\n",
    "\n",
    "    ############ Results\n",
    "    print(\"\\n\\nCalibrated efficiency  is %.3f +/- %.3f  (stat) +/- %.3f (pt) +/- %.3f/%.3f (sys)+/- %.3f (matching)  \\n\\n\"  % \n",
    "            (eff_rw, eff_stat_unc, eff_pt_unc, sys_unc_up, sys_unc_down, bad_match_unc))\n",
    "\n",
    "    #next compute the uncertainty about SFs\n",
    "\n",
    "    #Efficiency of the cut in nominal MC\n",
    "    eff_nom = np.average(score_cut, weights = weights_nom) #TODO\n",
    "\n",
    "    #Efficiency of the cut after the Lund Plane reweighting\n",
    "    eff_rw = np.average(score_cut, weights = LP_weights)\n",
    "\n",
    "    #Nominal 'scale factor'\n",
    "    print(\"Now perform SFs information\")\n",
    "    SF = eff_rw / eff_nom\n",
    "\n",
    "    print(\"SF (corrected / nom) %.3f\" % (SF))\n",
    "\n",
    "    #propagate statistical and pt extrapolation uncertainties to SF\n",
    "    SF_stat_unc = (abs(toys_mean - eff_rw)  + toys_std) /eff_nom\n",
    "    SF_pt_unc = (abs(pt_toys_mean - eff_rw) + pt_toys_std) /eff_nom\n",
    "\n",
    "    #propagate systemetic uncertainty to SF\n",
    "    eff_sys_up =  np.average(score_cut, weights = LP_weights_sys_up)\n",
    "    eff_sys_down =  np.average(score_cut, weights = LP_weights_sys_down)\n",
    "\n",
    "    sys_unc_up = abs(eff_rw - eff_sys_up)\n",
    "    sys_unc_down = abs(eff_rw - eff_sys_down)\n",
    "\n",
    "    SF_sys_unc_up = sys_unc_up/eff_nom\n",
    "    SF_sys_unc_down = sys_unc_down/eff_nom\n",
    "\n",
    "    #calculate bad matching uncertainty directly\n",
    "    SF_match_unc = bad_match_frac * SF\n",
    "    print(\"tagger cut = \",tagger_cut_value)\n",
    "    print(\"\\n\\nSF is %.3f +/-%.3f(stat) +/-%.3f(pt) +%.3f/-%.3f(sys) +/-%.3f(match) \\n\\n\"  % (SF, SF_stat_unc, SF_pt_unc, sys_unc_up, sys_unc_down, SF_match_unc))\n",
    "    print(\"total unc. = \",np.sqrt(SF_stat_unc **2 + SF_pt_unc**2 + sys_unc_up**2 + SF_match_unc**2 ))\n",
    "    print(\"*******\\n\")\n",
    "\n",
    "f_ratio.Close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some new plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "spans must have compatible lengths",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/pku/zhaoyz/Higgs/boostedHWW/scale_factors/LundReweighting/NtupleLundPlane/calibration_ParT_tagger_ttbar_tbqq.ipynb 单元格 72\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfarm/home/pku/zhaoyz/Higgs/boostedHWW/scale_factors/LundReweighting/NtupleLundPlane/calibration_ParT_tagger_ttbar_tbqq.ipynb#Y131sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m hist_before_err \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(hist_before\u001b[39m.\u001b[39mview()\u001b[39m.\u001b[39mvariance)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfarm/home/pku/zhaoyz/Higgs/boostedHWW/scale_factors/LundReweighting/NtupleLundPlane/calibration_ParT_tagger_ttbar_tbqq.ipynb#Y131sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m hist_after \u001b[39m=\u001b[39m bh\u001b[39m.\u001b[39mHistogram(bh\u001b[39m.\u001b[39maxis\u001b[39m.\u001b[39mRegular(nbins, x_min, x_max), storage\u001b[39m=\u001b[39mbh\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mWeight())\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfarm/home/pku/zhaoyz/Higgs/boostedHWW/scale_factors/LundReweighting/NtupleLundPlane/calibration_ParT_tagger_ttbar_tbqq.ipynb#Y131sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m hist_after\u001b[39m.\u001b[39;49mfill(HWWJets_tagger_score[:max_evts],weight\u001b[39m=\u001b[39;49mLP_weights[:max_evts])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfarm/home/pku/zhaoyz/Higgs/boostedHWW/scale_factors/LundReweighting/NtupleLundPlane/calibration_ParT_tagger_ttbar_tbqq.ipynb#Y131sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m hist_after_value \u001b[39m=\u001b[39m hist_after\u001b[39m.\u001b[39mview()\u001b[39m.\u001b[39mvalue\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfarm/home/pku/zhaoyz/Higgs/boostedHWW/scale_factors/LundReweighting/NtupleLundPlane/calibration_ParT_tagger_ttbar_tbqq.ipynb#Y131sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m hist_after_err \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(hist_after\u001b[39m.\u001b[39mview()\u001b[39m.\u001b[39mvariance)\n",
      "File \u001b[0;32m~/anaconda3/envs/lpr/lib/python3.9/site-packages/boost_histogram/_internal/hist.py:506\u001b[0m, in \u001b[0;36mHistogram.fill\u001b[0;34m(self, weight, sample, threads, *args)\u001b[0m\n\u001b[1;32m    503\u001b[0m     threads \u001b[39m=\u001b[39m cpu_count()\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m threads \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m threads \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_hist\u001b[39m.\u001b[39;49mfill(\u001b[39m*\u001b[39;49margs_ars, weight\u001b[39m=\u001b[39;49mweight_ars, sample\u001b[39m=\u001b[39;49msample_ars)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hist\u001b[39m.\u001b[39m_storage_type \u001b[39min\u001b[39;00m {\n\u001b[1;32m    510\u001b[0m     _core\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mmean,\n\u001b[1;32m    511\u001b[0m     _core\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mweighted_mean,\n\u001b[1;32m    512\u001b[0m }:\n",
      "\u001b[0;31mValueError\u001b[0m: spans must have compatible lengths"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import boost_histogram as bh\n",
    "from cycler import cycler\n",
    "max_evts = 60000\n",
    "#implement CMS plot style functions\n",
    "use_helvet = False ## true: use helvetica for plots, make sure the system have the font installed\n",
    "if use_helvet:\n",
    "    CMShelvet = hep.style.CMS\n",
    "    CMShelvet['font.sans-serif'] = ['Helvetica', 'Arial']\n",
    "    plt.style.use(CMShelvet)\n",
    "else:\n",
    "    plt.style.use(hep.style.CMS)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "ax=plt.gca()\n",
    "plt.grid()\n",
    "hep.cms.label(data=False, year=\"2018\", ax=ax, fontname='sans-serif')\n",
    "%matplotlib inline\n",
    "#step1: plot \n",
    "\n",
    "# plt.hist(eventsEventsID3Prongs4Prongs['HqqqqVsQcdTop'], bins=20, range=(0,1), histtype='step', label='before reweighting',density=True);\n",
    "# plt.hist(eventsEventsID3Prongs4Prongs['HqqqqVsQcdTop'], bins=20, range=(0,1), histtype='step', label='after reweighting', weights=eventsEventsID3Prongs4Prongs[\"LP_weight\"],density=True);\n",
    "nbins, x_min, x_max = 20, 0, 1.0\n",
    "hist_before = bh.Histogram(bh.axis.Regular(nbins, x_min, x_max), storage=bh.storage.Weight())\n",
    "hist_before.fill(HWWJets_tagger_score[:max_evts])\n",
    "hist_before_value = hist_before.view().value\n",
    "hist_before_err = np.sqrt(hist_before.view().variance)\n",
    "hist_after = bh.Histogram(bh.axis.Regular(nbins, x_min, x_max), storage=bh.storage.Weight())\n",
    "hist_after.fill(HWWJets_tagger_score[:max_evts],weight=LP_weights[:max_evts])\n",
    "hist_after_value = hist_after.view().value\n",
    "hist_after_err = np.sqrt(hist_after.view().variance)\n",
    "bins = hist_before.axes[0].edges\n",
    "\n",
    "\n",
    "hep.histplot(hist_before_value,    bins=bins, yerr=hist_before_err, label= 'before Lund Plane reweighting', lw = 2,edges = False, histtype=\"step\")\n",
    "hep.histplot(hist_after_value,     bins=bins, yerr=hist_after_err,  label= 'after Lund Plane reweighting', lw = 2,edges = False, histtype=\"step\")\n",
    "\n",
    "\n",
    "plt.legend(loc='upper left',frameon=False,fontsize=20)\n",
    "y_min,y_max = plt.gca().get_ylim()\n",
    "plt.text(0.08, 0.83*y_max, \"from tt-semileptonic sample,tbqq matched jets\", fontsize=20)\n",
    "# plt.xlabel(r'$H_{qqqq} / (H_{qqqq} + QCD + Top)$')\n",
    "plt.xlabel(r'$jet_{a}:HWW\\ score(without\\ P_{top}\\ in\\ denominator)$', fontsize=20, ha='right', x=1)\n",
    "plt.ylabel('Events(No xs-weighted)',fontsize=20, ha='right', y=1)\n",
    "plt.savefig(f\"TaggerDistribution_2018_ttbar_tbqq_60000evts.pdf\", bbox_inches='tight')\n",
    "plt.xticks(size=14)\n",
    "plt.yticks(size=14)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HWWCali",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
