{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: This is the Jupyter notebook to do the flowing things:\n",
    "\n",
    "1. Read slimmed PKU Tree files\n",
    "2. Store the raw MC histograms to pickle files\n",
    "\n",
    "kernel:HWW\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# import pandas as pd\n",
    "import random\n",
    "import awkward as ak\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import boost_histogram as bh\n",
    "# from scipy import interpolate\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "from cycler import cycler\n",
    "import uproot\n",
    "# means uproot4\n",
    "# import ROOT\n",
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "\n",
    "import hist as hist2\n",
    "import pyarrow\n",
    "# import utils #local file: utils.py\n",
    "import yaml\n",
    "from typing import Dict, List, Union\n",
    "from dataclasses import dataclass, field\n",
    "from copy import deepcopy\n",
    "\n",
    "# from coffea import hist\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea.nanoevents.methods import vector\n",
    "# from coffea.nanoevents.methods.vector import PtEtaPhiMLorentzVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = \"2016APV\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define templates dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \".\"\n",
    "\n",
    "plot_dir = f\"{MAIN_DIR}/templates/23Mar24_unc_v1\"\n",
    "_ = os.system(f\"mkdir -p {plot_dir}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read SlimmedTree files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the slimmedtree files using uproot\n",
    "\n",
    "#different year available here.\n",
    "# year = \"2016APV\"\n",
    "# year = \"2016\"\n",
    "# year = \"2017\"\n",
    "# year = \"2018\"\n",
    "# year = \"Full-Run2\"\n",
    "year = YEAR\n",
    "\n",
    "#if run on PKU cluster, use this:\n",
    "# CustNanoData = {\n",
    "#     'data'        : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/Data/SlimmedTree_Data.root\"%(year),\n",
    "#     'QCD'         : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/MC/SlimmedTree_QCD.root\"%(year),\n",
    "#     'Top'         : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/MC/SlimmedTree_Top.root\"%(year),\n",
    "#     'WJets'       : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/MC/SlimmedTree_WJets.root\"%(year),\n",
    "#     'Rest'        : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/MC/SlimmedTree_Rest.root\"%(year),\n",
    "#     'TotalSignal' : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/Signal/SlimmedTree_Total.root\"%(year),\n",
    "#     'ggF'         : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/Signal/SlimmedTree_GluGlu.root\"%(year),\n",
    "#     'VH'          : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/Signal/SlimmedTree_VH.root\"%(year),\n",
    "#     'ttH'         : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/Signal/SlimmedTree_ttH.root\"%(year),\n",
    "#     'VBF'         : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/Signal/SlimmedTree_VBF.root\"%(year),\n",
    "# } \n",
    "\n",
    "#if run on lxplus, use this:\n",
    "# CustNanoData = {\n",
    "#     'data'        : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/Data/SlimmedTree_Data.root\"%(year),\n",
    "#     'QCD'         : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/MC/SlimmedTree_QCD.root\"%(year),\n",
    "#     'Top'         : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/MC/SlimmedTree_Top.root\"%(year),\n",
    "#     'WJets'       : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/MC/SlimmedTree_WJets.root\"%(year),\n",
    "#     'Rest'        : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/MC/SlimmedTree_Rest.root\"%(year),\n",
    "#     'TotalSignal' : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/Signal/SlimmedTree_Total.root\"%(year),\n",
    "#     'ggF'         : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/Signal/SlimmedTree_GluGlu.root\"%(year),\n",
    "#     'VH'          : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/Signal/SlimmedTree_VH.root\"%(year),\n",
    "#     'ttH'         : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/Signal/SlimmedTree_ttH.root\"%(year),\n",
    "#     'VBF'         : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/Signal/SlimmedTree_VBF.root\"%(year),\n",
    "# }  \n",
    "\n",
    "#if run on CMSconnect, use this:\n",
    "CustNanoData = {\n",
    "    'data'        : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Data/SlimmedTree_Data.root\"%(year),\n",
    "    'QCD'         : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/MC/SlimmedTree_QCD.root\"%(year),\n",
    "    'TT'          : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/MC/SlimmedTree_TT.root\"%(year),\n",
    "    'ST'          : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/MC/SlimmedTree_ST.root\"%(year),\n",
    "    'WJets'       : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/MC/SlimmedTree_WJets.root\"%(year),\n",
    "    'Rest'        : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/MC/SlimmedTree_Rest.root\"%(year),\n",
    "    'TotalSignal' : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Signal/SlimmedTree_Total.root\"%(year),\n",
    "    'ggF'         : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Signal/SlimmedTree_GluGlu.root\"%(year),\n",
    "    'ZH'          : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Signal/SlimmedTree_WH.root\"%(year),\n",
    "    'WH'          : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Signal/SlimmedTree_HZJ.root\"%(year),\n",
    "    'ttH'         : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Signal/SlimmedTree_ttH.root\"%(year),\n",
    "    'VBF'         : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Signal/SlimmedTree_VBF.root\"%(year),\n",
    "}        \n",
    "\n",
    "files = {typefile : {} for typefile in CustNanoData}\n",
    "for typefile in CustNanoData:\n",
    "    files[typefile] = uproot.lazy({CustNanoData[typefile]: \"PKUTree\" })\n",
    "    \n",
    "#for signal decomposition plots, we store these information in different objects\n",
    "files[r\"$H^{4q}$\"] = files[\"TotalSignal\"][files[\"TotalSignal\"][\"R4q_a\"] == 1]\n",
    "files[r\"$H^{3q}$\"] = files[\"TotalSignal\"][(files[\"TotalSignal\"][\"R3q_a\"] == 1) & (files[\"TotalSignal\"][\"R3q_taudecay_a\"] != 1)]\n",
    "files[\"W\"] = files[\"TotalSignal\"][files[\"TotalSignal\"][\"w_a\"] == 1]\n",
    "files[\"top\"] = files[\"TotalSignal\"][files[\"TotalSignal\"][\"t_a\"] == 1]\n",
    "files[\"Z\"] = files[\"TotalSignal\"][files[\"TotalSignal\"][\"z_a\"] == 1]\n",
    "files[r\"$H^{lqq}$\"] = files[\"TotalSignal\"][(files[\"TotalSignal\"][\"Rlqq_a\"] == 1) | (files[\"TotalSignal\"][\"R3q_taudecay_a\"] == 1)]\n",
    "files[\"g/q\"] = files[\"TotalSignal\"][files[\"TotalSignal\"][\"gKK_g_a\"] == 1]\n",
    "files[\"rest\"] = files[\"TotalSignal\"][(files[\"TotalSignal\"][\"u_a\"] == 1) | (files[\"TotalSignal\"][\"Rlq_a\"] == 1) | (files[\"TotalSignal\"][\"R2q_a\"] == 1)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get DPhi in the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dphi(events):\n",
    "    pT_higgs   = events[\"PTj_V2_a\"]\n",
    "    eta_higgs  = events[\"Etaj_V2_a\"]\n",
    "    phi_higgs  = events[\"Phij_V2_a\"]\n",
    "    mass_higgs = events[\"Mj_V2_a\"]\n",
    "    pT_MET = events[\"MET_et\"]\n",
    "    eta_MET = events[\"Etaj_V2_a\"]\n",
    "    phi_MET = events[\"MET_phi\"]\n",
    "    mass_MET = ak.zeros_like(events[\"MET_phi\"])\n",
    "    vec_higgs = ak.zip({\n",
    "        \"pt\"   : pT_higgs   ,\n",
    "        \"eta\"  : eta_higgs  ,\n",
    "        \"phi\"  : phi_higgs  ,\n",
    "        \"mass\" : mass_higgs ,\n",
    "    },\n",
    "    with_name=\"PtEtaPhiMLorentzVector\",\n",
    "    behavior=vector.behavior,\n",
    "    )\n",
    "    vec_MET = ak.zip({\n",
    "        \"pt\"   :   pT_MET   ,\n",
    "        \"eta\"  :  eta_MET  ,\n",
    "        \"phi\"  :  phi_MET  ,\n",
    "        \"mass\" : mass_MET ,\n",
    "    },\n",
    "    with_name=\"PtEtaPhiMLorentzVector\",\n",
    "    behavior=vector.behavior,\n",
    "    )\n",
    "    delta_phi = np.subtract(vec_MET.phi, vec_higgs.phi)\n",
    "    delta_phi = np.where(delta_phi > np.pi, delta_phi - 2*np.pi, delta_phi)\n",
    "    delta_phi = np.where(delta_phi < -np.pi, delta_phi + 2*np.pi, delta_phi)\n",
    "    delta_phi = np.abs(delta_phi)\n",
    "    print(delta_phi)\n",
    "    events[\"DPhi\"] = delta_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add dphi of: data\n",
      "[3.14, 0.298, 2.27, 0.915, 0.409, 0.824, ... 1.54, 0.654, 0.7, 2.53, 1.87, 0.291]\n",
      "Add dphi of: QCD\n",
      "[2.31, 3.08, 2.34, 3.11, 2.24, 2.81, ... 0.702, 0.273, 0.0464, 2.74, 3.07, 0.518]\n",
      "Add dphi of: TT\n",
      "[3.04, 0.473, 3.05, 1.31, 0.713, 0.0965, ... 0.407, 0.131, 3.02, 2.47, 2.96, 1.77]\n",
      "Add dphi of: ST\n",
      "[2.41, 0.636, 1.02, 0.0351, 2.12, 0.928, 1.44, ... 1.48, 3.02, 2.9, 2.58, 2.77, 0.61]\n",
      "Add dphi of: WJets\n",
      "[0.108, 0.158, 2, 0.181, 0.337, 2.65, 0.613, ... 2.6, 2.94, 0.249, 0.245, 2.88, 2.77]\n",
      "Add dphi of: Rest\n",
      "[2.99, 0.122, 3.07, 0.258, 0.213, 2.38, ... 1.85, 1.57, 0.00223, 0.102, 0.152, 0.309]\n",
      "Add dphi of: TotalSignal\n",
      "[1.89, 0.0198, 0.206, 3.14, 0.668, 1.61, ... 0.424, 0.474, 3.13, 0.239, 0.263, 0.227]\n",
      "Add dphi of: ggF\n",
      "[1.89, 0.0198, 0.206, 3.14, 0.668, 1.61, ... 0.0329, 0.688, 0.31, 0.00507, 2.89, 2.9]\n",
      "Add dphi of: ZH\n",
      "[0.102, 3.03, 0.153, 2.24, 2.7, 3, ... 0.424, 0.474, 3.13, 0.239, 0.263, 0.227]\n",
      "Add dphi of: WH\n",
      "[0.424, 2.3, 1.97, 1.23, 2.17, 2.47, ... 0.084, 1.92, 0.409, 1.61, 2.1, 0.0788]\n",
      "Add dphi of: ttH\n",
      "[2.67, 0.821, 3.03, 2.52, 0.515, 2.84, ... 3.08, 1.94, 0.795, 0.592, 0.57, 3.01]\n",
      "Add dphi of: VBF\n",
      "[0.763, 0.0619, 0.353, 1.86, 0.0673, 0.653, ... 1.85, 0.588, 2.18, 0.00056, 0.0315]\n",
      "Add dphi of: $H^{4q}$\n",
      "[0.206, 0.668, 1.53, 0.86, 2.52, 0.208, ... 0.196, 2.94, 3.11, 0.385, 2.47, 0.474]\n",
      "Add dphi of: $H^{3q}$\n",
      "[1.89, 3.14, 1.14, 2.38, 1.85, 2.55, 2.45, ... 0.585, 1, 0.137, 2.46, 2.47, 3.13]\n",
      "Add dphi of: W\n",
      "[0.0868, 0.266, 0.569, 1.42, 3.07, 2.61, ... 1.24, 2.87, 0.474, 0.454, 0.263, 0.227]\n",
      "Add dphi of: top\n",
      "[2.67, 3.03, 0.515, 2.84, 0.284, 2.18, ... 2.91, 1.59, 1.21, 0.232, 0.149, 3.08]\n",
      "Add dphi of: Z\n",
      "[0.077, 0.0245, 3.1, 1.64, 2.44, 2.95, ... 1.34, 1.7, 0.494, 0.0033, 0.084, 0.409]\n",
      "Add dphi of: $H^{lqq}$\n",
      "[0.0198, 2.99, 0.0146, 0.378, 0.0706, 1.44, ... 0.02, 2.36, 0.0365, 0.424, 0.239]\n",
      "Add dphi of: g/q\n",
      "[1.61, 1.16, 2.7, 1.92, 2.52, 0.654, 1.73, ... 0.0487, 1.75, 2.08, 0.19, 0.862, 2.58]\n",
      "Add dphi of: rest\n",
      "[2.98, 0.0103, 2.03, 0.343, 0.296, 2.9, ... 0.153, 1.63, 2.98, 0.07, 0.0489, 0.146]\n"
     ]
    }
   ],
   "source": [
    "for k in files:\n",
    "    print(\"Add dphi of:\",k)\n",
    "    get_dphi(events=files[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MET recovery mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_reco(events,mass = \"Mj_corr_V2_a\", MET_UE = None):\n",
    "    pT_higgs   = events[\"PTj_V2_a\"]\n",
    "    eta_higgs  = events[\"Etaj_V2_a\"]\n",
    "    phi_higgs  = events[\"Phij_V2_a\"]\n",
    "    mass_higgs = events[mass]\n",
    "    if not MET_UE:\n",
    "        pT_MET = events[\"MET_et\"]\n",
    "        phi_MET = events[\"MET_phi\"]\n",
    "    elif MET_UE == \"up\":\n",
    "        pT_MET = events[\"MET_et_UEup\"]\n",
    "        phi_MET = events[\"MET_phi_UEup\"]\n",
    "    elif MET_UE == \"down\":\n",
    "        pT_MET = events[\"MET_et_UEdown\"]\n",
    "        phi_MET = events[\"MET_phi_UEdown\"]\n",
    "    eta_MET = events[\"Etaj_V2_a\"]\n",
    "    mass_MET = ak.zeros_like(events[\"MET_phi\"])\n",
    "    vec_higgs = ak.zip({\n",
    "        \"pt\"   : pT_higgs   ,\n",
    "        \"eta\"  : eta_higgs  ,\n",
    "        \"phi\"  : phi_higgs  ,\n",
    "        \"mass\" : mass_higgs ,\n",
    "    },\n",
    "    with_name=\"PtEtaPhiMLorentzVector\",\n",
    "    behavior=vector.behavior,\n",
    "    )\n",
    "\n",
    "    vec_MET = ak.zip({\n",
    "        \"pt\"   :   pT_MET   ,\n",
    "        \"eta\"  :  eta_MET  ,\n",
    "        \"phi\"  :  phi_MET  ,\n",
    "        \"mass\" : mass_MET ,\n",
    "    },\n",
    "    with_name=\"PtEtaPhiMLorentzVector\",\n",
    "    behavior=vector.behavior,\n",
    "    )\n",
    "\n",
    "    vec_sum = vec_MET + vec_higgs\n",
    "    mH_reco = vec_sum.mass\n",
    "    if not MET_UE: mass_str = \"MH_Reco\" if mass == \"Mj_corr_V2_a\" or mass == \"Mj_V2_a\" else mass\n",
    "    else : mass_str = \"MH_Reco\" + \"_UE_\" + MET_UE\n",
    "    \n",
    "    if not MET_UE: \n",
    "        events[mass_str] = ak.where(( (events[\"DPhi\"] < 0.8) & (events[\"MET_et\"]/events[\"PTj_V2_a\"] > 0.1)), mH_reco, events[mass])\n",
    "    elif MET_UE == \"up\" : \n",
    "        events[mass_str] = ak.where(( (events[\"DPhi\"] < 0.8) & (events[\"MET_et_UEup\"]/events[\"PTj_V2_a\"] > 0.1)), mH_reco, events[mass])\n",
    "    elif MET_UE == \"down\" : \n",
    "        events[mass_str] = ak.where(( (events[\"DPhi\"] < 0.8) & (events[\"MET_et_UEdown\"]/events[\"PTj_V2_a\"] > 0.1)), mH_reco, events[mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add reco of: TT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add reco of: ST\n",
      "Add reco of: WJets\n",
      "Add reco of: Rest\n",
      "Add reco of: TotalSignal\n",
      "Add reco of: ggF\n",
      "Add reco of: ZH\n",
      "Add reco of: WH\n",
      "Add reco of: ttH\n",
      "Add reco of: VBF\n",
      "Add reco of: $H^{4q}$\n",
      "Add reco of: $H^{3q}$\n",
      "Add reco of: W\n",
      "Add reco of: top\n",
      "Add reco of: Z\n",
      "Add reco of: $H^{lqq}$\n",
      "Add reco of: g/q\n",
      "Add reco of: rest\n"
     ]
    }
   ],
   "source": [
    "for k in files:\n",
    "    # note that QCD and Data don't have such variation\n",
    "    if k == \"data\" or k == \"QCD\": continue\n",
    "    print(\"Add reco of:\",k)\n",
    "    get_reco(events=files[k])\n",
    "    get_reco(events=files[k],MET_UE = \"up\")\n",
    "    get_reco(events=files[k],MET_UE = \"down\")\n",
    "    \n",
    "    get_reco(events=files[k],mass = \"Mj_jesTotalUp_a\")\n",
    "    get_reco(events=files[k],mass = \"Mj_jesTotalDown_a\")\n",
    "    get_reco(events=files[k],mass = \"Mj_jerUp_a\")\n",
    "    get_reco(events=files[k],mass = \"Mj_jerDown_a\")\n",
    "    \n",
    "    get_reco(events=files[k],mass = \"Mj_jmsUp_a\")\n",
    "    get_reco(events=files[k],mass = \"Mj_jmsDown_a\")\n",
    "    get_reco(events=files[k],mass = \"Mj_jmrUp_a\")\n",
    "    get_reco(events=files[k],mass = \"Mj_jmrDown_a\")\n",
    "for k in [\"QCD\",\"data\"]:\n",
    "    get_reco(events=files[k],mass = \"Mj_V2_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some test about variables / output all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a_TopbWqq1c',\n",
       " 'a_HWqqWq2c',\n",
       " 'nb_m_deep_ex',\n",
       " 'a_TopbWmv',\n",
       " 'nb_l_deep_in',\n",
       " 'puWeightDown',\n",
       " 'Rlqq_a',\n",
       " 'b_HWWvsQCD_V2',\n",
       " 'LHEScaleWeight_1',\n",
       " 'a_HWqqWq1c',\n",
       " 'a_QCDc',\n",
       " 'Mj_V2_c',\n",
       " 'R2q_a',\n",
       " 'R3q_taudecay_c',\n",
       " 'puWeightUp',\n",
       " 'Mj_jmrDown_b',\n",
       " 'R4q_b',\n",
       " 'a_QCDbb',\n",
       " 'gKK_g_b',\n",
       " 'a_TopbWq0c',\n",
       " 'a_Hcc',\n",
       " 'tlqq_a',\n",
       " 'Mj_jerUp_b',\n",
       " 'Mj_corr_3',\n",
       " 'Etaj',\n",
       " 'PTj_2',\n",
       " 'PTj_mid',\n",
       " 'a_HWqqWtauhv0c',\n",
       " 'w_b',\n",
       " 'LHEScaleWeight_6',\n",
       " 'LHEScaleWeight_2',\n",
       " 'Mj',\n",
       " 'Rlq_a',\n",
       " 'tlqq_b',\n",
       " 'a_HWqqWtaumv1c',\n",
       " 'Mj_jmsUp_b',\n",
       " 'MET_et_UEup',\n",
       " 'tlqq_c',\n",
       " 'Mj_3',\n",
       " 'a_Htauhtauh',\n",
       " 'a_HWqqWev0c',\n",
       " 'u_a',\n",
       " 'a_HWqqWqq2c',\n",
       " 'a_HWqqWqq1c',\n",
       " 'Mj_jmsDown_c',\n",
       " 'z_a',\n",
       " 'R3q_taudecay_b',\n",
       " 'puWeight',\n",
       " 'Etaj_V2_c',\n",
       " 'c_HWWvsQCD_V2',\n",
       " 'Mj_jerDown_b',\n",
       " 'R2q_c',\n",
       " 'Mj_jmrUp_b',\n",
       " 'a_QCDb',\n",
       " 'Rlqq_c',\n",
       " 'PSWeight_2',\n",
       " 'MET_T1Smear_phi_XYcorr',\n",
       " 'a_HWqqWqq0c',\n",
       " 'a_QCDothers',\n",
       " 'Mj_jesTotalUp_c',\n",
       " 'MJJ',\n",
       " 'a_HWqqWtauhv1c',\n",
       " 'b_HWW_V2',\n",
       " 'Mj_jerDown_c',\n",
       " 'Mj_jmsDown_b',\n",
       " 'Mj_jesTotalUp_b',\n",
       " 'nb_m_deep_in',\n",
       " 'PSWeight_1',\n",
       " 'Mj_V2_b',\n",
       " 'MET_et',\n",
       " 'Mj_jesTotalDown_c',\n",
       " 'a_HWqqWtaumv0c',\n",
       " 'PTj_3',\n",
       " 't_c',\n",
       " 'SF',\n",
       " 'a_HWqqWq0c',\n",
       " 'MET_et_UEdown',\n",
       " 'nb_t_deep_in',\n",
       " 'LHEScaleWeight_5',\n",
       " 'z_c',\n",
       " 'a_HWqqWtauev0c',\n",
       " 'Etaj_V2_b',\n",
       " 'Mj_min',\n",
       " 'Mj_corr_2',\n",
       " 'MET_phi_UEdown',\n",
       " 'Mj_max',\n",
       " 'a_TopbWqq0c',\n",
       " 'a_HWqqWmv1c',\n",
       " 'w_a',\n",
       " 'a_HWqqWtauev1c',\n",
       " 'a_HWWvsQCD_V2',\n",
       " 'Phij_min',\n",
       " 'nb_t_deep_ex',\n",
       " 'Phij_max',\n",
       " 'Mj_jesTotalDown_b',\n",
       " 'Phij_mid',\n",
       " 'R3q_a',\n",
       " 'MET_et_NoXYCorr',\n",
       " 'z_b',\n",
       " 'Mj_jmrUp_c',\n",
       " 'R4q_a',\n",
       " 'w_c',\n",
       " 'nb_l_deep_ex',\n",
       " 'Rlqq_b',\n",
       " 'Mj_mid',\n",
       " 'a_Hbb',\n",
       " 'a_HWqqWev1c',\n",
       " 'R3q_c',\n",
       " 'Nj4_ex',\n",
       " 'PTj',\n",
       " 'PSWeight_3',\n",
       " 'Nj8',\n",
       " 'Etaj_mid',\n",
       " 'a_QCDcc',\n",
       " 'Etaj_3',\n",
       " 'a_Htauhtaum',\n",
       " 'Nj4_in',\n",
       " 'HT',\n",
       " 'MET_phi_UEup',\n",
       " 'a_TopbWev',\n",
       " 'LHEScaleWeight_7',\n",
       " 'PSWeight_0',\n",
       " 'c_HWW_V2',\n",
       " 't_a',\n",
       " 'PTj_V2_b',\n",
       " 'a_Hqq',\n",
       " 'a_TopbWtauhv',\n",
       " 'PTj_V2_c',\n",
       " 'Phij_V2_c',\n",
       " 'Mj_jmrDown_c',\n",
       " 'Etaj_max',\n",
       " 'Mj_V2_a',\n",
       " 'gKK_g_a',\n",
       " 'Mj_corr_c',\n",
       " 't_b',\n",
       " 'Mj_jmsUp_c',\n",
       " 'Phij_3',\n",
       " 'a_Hss',\n",
       " 'a_TopbWtaumv',\n",
       " 'gKK_g_c',\n",
       " 'ST',\n",
       " 'weight',\n",
       " 'Mj_corr_b',\n",
       " 'PTj_min',\n",
       " 'R3q_b',\n",
       " 'Mj_corr_a',\n",
       " 'u_b',\n",
       " 'MJJJ',\n",
       " 'MET_phi',\n",
       " 'Etaj_min',\n",
       " 'a_HWW_V2',\n",
       " 'Mj_corr_V2_b',\n",
       " 'Phij_V2_a',\n",
       " 'Mj_2',\n",
       " 'LHEScaleWeight_8',\n",
       " 'PTj_V2_a',\n",
       " 'Mj_corr_V2_c',\n",
       " 'Phij_V2_b',\n",
       " 'Etaj_2',\n",
       " 'Rlq_b',\n",
       " 'Rlq_c',\n",
       " 'a_TopbWq1c',\n",
       " 'R3q_taudecay_a',\n",
       " 'R2q_b',\n",
       " 'PTj_max',\n",
       " 'MET_T1Smear_pt_XYcorr',\n",
       " 'LHEScaleWeight_3',\n",
       " 'u_c',\n",
       " 'a_HWqqWmv0c',\n",
       " 'a_TopbWtauev',\n",
       " 'Mj_corr',\n",
       " 'Mj_jerUp_c',\n",
       " 'Phij',\n",
       " 'LHEScaleWeight_4',\n",
       " 'a_Htauhtaue',\n",
       " 'Phij_2',\n",
       " 'Mj_corr_V2_a',\n",
       " 'MET_phi_NoXYCorr',\n",
       " 'R4q_c',\n",
       " 'SF_unc',\n",
       " 'LHEScaleWeight_0',\n",
       " 'Etaj_V2_a',\n",
       " 'DPhi',\n",
       " 'MH_Reco',\n",
       " 'MH_Reco_UE_up',\n",
       " 'MH_Reco_UE_down',\n",
       " 'Mj_jesTotalUp_a',\n",
       " 'Mj_jesTotalDown_a',\n",
       " 'Mj_jerUp_a',\n",
       " 'Mj_jerDown_a',\n",
       " 'Mj_jmsUp_a',\n",
       " 'Mj_jmsDown_a',\n",
       " 'Mj_jmrUp_a',\n",
       " 'Mj_jmrDown_a']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[\"TotalSignal\"].fields"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import boost_histogram as bh\n",
    "from cycler import cycler\n",
    "\n",
    "use_helvet = False ## true: use helvetica for plots, make sure the system have the font installed\n",
    "if use_helvet:\n",
    "    CMShelvet = hep.style.CMS\n",
    "    CMShelvet['font.sans-serif'] = ['Helvetica', 'Arial']\n",
    "    plt.style.use(CMShelvet)\n",
    "else:\n",
    "    plt.style.use(hep.style.CMS)\n",
    "\n",
    "def flow(hist: bh.Histogram, overflow: bool=True, underflow: bool=True):\n",
    "    h, var = hist.view(flow=(overflow | underflow)).value, hist.view(flow=(overflow | underflow)).variance\n",
    "    if overflow: \n",
    "        # h, var also include underflow bins but in plots usually no underflow data\n",
    "        # And we've filled None with -999, so we shouldn't show underflow data (mostly from filled None)\n",
    "        # You have to access the overflow and underflow bins data like below:\n",
    "        h[-2] += h[-1]; var[-2] += var[-1]\n",
    "    if underflow:\n",
    "        h[1] += h[0]; var[1] += var[0]\n",
    "    if overflow or underflow:\n",
    "        h, var = h[1:-1], var[1:-1]\n",
    "    return h, var\n",
    "    # Return the updated histogram and variance\n",
    "\n",
    "def error_bar(h, var, type='data'):\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    if type == 'data':\n",
    "        number = h\n",
    "    elif type == 'mc':  # h = k*N, var = k^2*N, std = k*sqrt(N)\n",
    "        number = h**2 / var\n",
    "    else:\n",
    "        raise ValueError(\"type should be 'data' or 'mc'! \")\n",
    "    center = range(11) # Number: 0-10\n",
    "    up = np.array([1.84, 3.30, 4.64, 5.92, 7.16, 8.38, 9.58, 10.77, 11.95, 13.11, 14.27]) - center\n",
    "    down = center - np.array([0, 0.17, 0.71, 1.37, 2.09, 2.84, 3.62, 4.42, 5.23, 6.06, 6.89])\n",
    "    #cs means to create a CubicSpline object\n",
    "    cs_up = CubicSpline(x=center, y=up)\n",
    "    cs_down = CubicSpline(x=center, y=down)\n",
    "    \n",
    "    Garwood = (number>0)&(number<10)\n",
    "    poison_error_bar = np.sqrt(number)\n",
    "    up_error_bar = np.copy(poison_error_bar)\n",
    "    down_error_bar = np.copy(poison_error_bar)\n",
    "    up_error_bar[Garwood] = cs_up(number[Garwood])\n",
    "    down_error_bar[Garwood] = cs_down(number[Garwood])\n",
    "    if type == 'mc':\n",
    "        up_error_bar *= var/h\n",
    "        down_error_bar *= var/h\n",
    "    up_error_bar [up_error_bar < 0 ] = 0\n",
    "    down_error_bar [down_error_bar < 0 ] = 0\n",
    "    return np.array([down_error_bar, up_error_bar])\n",
    "\n",
    "\n",
    "# function to find the optimal region with S/sqrt(B)\n",
    "# not used so far\n",
    "def optimalcut(shist, bhist):\n",
    "    n_bins = len(shist)\n",
    "    best_lower = None\n",
    "    best_upper = None\n",
    "    best_s_sqrt_b = 0\n",
    "\n",
    "    for lower in range(n_bins):\n",
    "        for upper in range(lower+1, n_bins+1):\n",
    "            s = np.sum(shist[lower:upper])\n",
    "            b = np.sum(bhist[lower:upper])\n",
    "            s_sqrt_b = s / np.sqrt(b + 1)\n",
    "\n",
    "            if s_sqrt_b > best_s_sqrt_b:\n",
    "                best_lower = lower\n",
    "                best_upper = upper\n",
    "                best_s_sqrt_b = s_sqrt_b\n",
    "\n",
    "    return best_lower, best_upper, best_s_sqrt_b\n",
    "\n",
    "def optimalcut_oneside(shist, bhist, epsilon = 0.01):\n",
    "    '''\n",
    "    Given the signal histogram and background histogram, \n",
    "    show the one-side cut for the variable to get best s/sqrt(b).\n",
    "    Args:\n",
    "        shist:signal histogram\n",
    "        bhist:background histogram\n",
    "        epsilon(float): epsilon to avoid numerical errs \n",
    "    '''\n",
    "    n_bins = len(shist)\n",
    "    best_cut = 0\n",
    "    best_s_sqrt_b = 0\n",
    "\n",
    "    for cut in range(n_bins):\n",
    "        s = np.sum(shist[cut:])\n",
    "        b = np.sum(bhist[cut:])\n",
    "        s_sqrt_b = s / np.sqrt(b + epsilon)\n",
    "        if s_sqrt_b > best_s_sqrt_b:\n",
    "            best_cut = cut\n",
    "            best_s_sqrt_b = s_sqrt_b\n",
    "        \n",
    "    return best_cut, best_s_sqrt_b\n",
    "\n",
    "def optimalcut_mid_combine(shist1, shist2, bhist, epsilon = 1):\n",
    "    '''\n",
    "    Given the signal histogram and background histogram, \n",
    "    show the one-side cut for the variable to get best s/sqrt(b).\n",
    "    Args:\n",
    "        shist:signal histogram\n",
    "        bhist:background histogram\n",
    "        epsilon(float): epsilon to avoid numerical errs \n",
    "    '''\n",
    "    n_bins = len(shist1)\n",
    "    best_cut = 0\n",
    "    best_combined_sig_two_side = 0\n",
    "\n",
    "    for cut in range(n_bins):\n",
    "        s_right_side = np.sum(shist2[cut:])\n",
    "        b_right_side = np.sum(bhist[cut:])\n",
    "        s_left_side = np.sum(shist1[:cut])\n",
    "        b_left_side = np.sum(bhist[:cut])\n",
    "        s_sqrt_b_right_side = s_right_side / np.sqrt(b_right_side + epsilon)\n",
    "        s_sqrt_b_left_side = s_left_side / np.sqrt(b_left_side + epsilon)\n",
    "        combined_sig_two_side = np.sqrt((s_sqrt_b_right_side)**2 + (s_sqrt_b_left_side)**2)\n",
    "        if combined_sig_two_side > best_combined_sig_two_side:\n",
    "            best_cut = cut\n",
    "            best_combined_sig_two_side = combined_sig_two_side\n",
    "        \n",
    "    return best_cut, best_combined_sig_two_side\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define observable object variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ShapeVar:\n",
    "    \"\"\"Class to store attributes of a variable to make a histogram of.\n",
    "\n",
    "    Args:\n",
    "        var (str): variable name\n",
    "        label (str): variable label\n",
    "        bins (List[int]): bins\n",
    "        reg (bool, optional): Use a regular axis or variable binning. Defaults to True.\n",
    "        blind_window (List[int], optional): if blinding, set min and max values to set 0. Defaults to None.\n",
    "        significance_dir (str, optional): if plotting significance, which direction to plot it in.\n",
    "          See more in plotting.py:ratioHistPlot(). Options are [\"left\", \"right\", \"bin\"]. Defaults to \"right\".\n",
    "    \"\"\"\n",
    "\n",
    "    var: str = None\n",
    "    label: str = None\n",
    "    bins: List[int] = None\n",
    "    reg: bool = True #regular axis\n",
    "    blind_window: List[int] = None\n",
    "    significance_dir: str = \"right\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # create axis used for histogramming\n",
    "        if self.reg:\n",
    "            self.axis = hist2.axis.Regular(*self.bins, name=self.var, label=self.label)\n",
    "        else:\n",
    "            self.axis = hist2.axis.Variable(self.bins, name=self.var, label=self.label)\n",
    "\n",
    "@dataclass\n",
    "class Syst:\n",
    "    samples: list[str] = None\n",
    "    years: list[str] = field(default_factory=lambda: years)\n",
    "    label: str = None\n",
    "    \n",
    "def blindBins(h: hist2.Hist, blind_region: List, blind_sample: str = None, axis=0):\n",
    "    \"\"\"\n",
    "    Blind (i.e. zero) bins in histogram ``h``.\n",
    "    If ``blind_sample`` specified, only blind that sample, else blinds all.\n",
    "    \"\"\"\n",
    "    if axis > 0:\n",
    "        raise Exception(\"not implemented > 1D blinding yet\")\n",
    "\n",
    "    bins = h.axes[axis + 1].edges\n",
    "    lv = int(np.searchsorted(bins, blind_region[0], \"right\"))\n",
    "    rv = int(np.searchsorted(bins, blind_region[1], \"left\") + 1)\n",
    "\n",
    "    if blind_sample is not None:\n",
    "        data_key_index = np.where(np.array(list(h.axes[0])) == blind_sample)[0][0]\n",
    "        h.view(flow=True)[data_key_index][lv:rv].value = 0\n",
    "        h.view(flow=True)[data_key_index][lv:rv].variance = 0\n",
    "    else:\n",
    "        h.view(flow=True)[:, lv:rv].value = 0\n",
    "        h.view(flow=True)[:, lv:rv].variance = 0       \n",
    "shape_vars = [\n",
    "    ShapeVar(\n",
    "        \"MH_Reco\",\n",
    "        r\"Higgs candidate MET recovery mass [GeV]\",\n",
    "        [20, 50, 250],\n",
    "        reg=True,\n",
    "        blind_window=[90, 150],\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define samples we consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_keys = [\n",
    "    \"ggF\",\n",
    "    \"VBF\",\n",
    "    \"ZH\",\n",
    "    \"WH\",\n",
    "    \"ttH\",\n",
    "]\n",
    "\n",
    "bkg_keys = [\n",
    "    \"TT\",\n",
    "    \"WJets\",\n",
    "    \"ST\",\n",
    "    \"Rest\"\n",
    "]\n",
    "\n",
    "mc_keys = sig_keys + bkg_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define weight shift list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2016APV\", \"2016\", \"2017\", \"2018\"]\n",
    "\n",
    "weight_shifts = {\n",
    "    \"pileup\": Syst(samples=mc_keys, label=\"Pileup\"),\n",
    "    \"ISRPartonShower\": Syst(samples=mc_keys, label=\"ISR Parton Shower\"),\n",
    "    \"FSRPartonShower\": Syst(samples=mc_keys, label=\"FSR Parton Shower\"),\n",
    "    \"QCDscale\": Syst(samples=bkg_keys, label=\"QCDScale\"),\n",
    "    \"trigger\" : Syst(samples=mc_keys, label=\"Trigger SF\"),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Lund Plane SF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_lp = {\n",
    "    \"a\" : 0.898,\n",
    "    \"b\" : 0.957,\n",
    "}\n",
    "\n",
    "#unc is not used here, since we will add the unc in the datacards directly\n",
    "weight_lp_unc = {\n",
    "    \"a\" : 0.334,\n",
    "    \"b\" : 0.349,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-organize weight information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: down\n",
      "processing: ggF\n",
      "processing: pileup\n",
      "processing: VBF\n",
      "processing: pileup\n",
      "processing: ZH\n",
      "processing: pileup\n",
      "processing: WH\n",
      "processing: pileup\n",
      "processing: ttH\n",
      "processing: pileup\n",
      "processing: TT\n",
      "processing: pileup\n",
      "processing: WJets\n",
      "processing: pileup\n",
      "processing: ST\n",
      "processing: pileup\n",
      "processing: Rest\n",
      "processing: pileup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: ggF\n",
      "processing: ISRPartonShower\n",
      "processing: VBF\n",
      "processing: ISRPartonShower\n",
      "processing: ZH\n",
      "processing: ISRPartonShower\n",
      "processing: WH\n",
      "processing: ISRPartonShower\n",
      "processing: ttH\n",
      "processing: ISRPartonShower\n",
      "processing: TT\n",
      "processing: ISRPartonShower\n",
      "processing: WJets\n",
      "processing: ISRPartonShower\n",
      "processing: ST\n",
      "processing: ISRPartonShower\n",
      "processing: Rest\n",
      "processing: ISRPartonShower\n",
      "processing: ggF\n",
      "processing: FSRPartonShower\n",
      "processing: VBF\n",
      "processing: FSRPartonShower\n",
      "processing: ZH\n",
      "processing: FSRPartonShower\n",
      "processing: WH\n",
      "processing: FSRPartonShower\n",
      "processing: ttH\n",
      "processing: FSRPartonShower\n",
      "processing: TT\n",
      "processing: FSRPartonShower\n",
      "processing: WJets\n",
      "processing: FSRPartonShower\n",
      "processing: ST\n",
      "processing: FSRPartonShower\n",
      "processing: Rest\n",
      "processing: FSRPartonShower\n",
      "processing: TT\n",
      "processing: QCDscale\n",
      "processing: WJets\n",
      "processing: QCDscale\n",
      "processing: ST\n",
      "processing: QCDscale\n",
      "processing: Rest\n",
      "processing: QCDscale\n",
      "processing: ggF\n",
      "processing: trigger\n",
      "processing: VBF\n",
      "processing: trigger\n",
      "processing: ZH\n",
      "processing: trigger\n",
      "processing: WH\n",
      "processing: trigger\n",
      "processing: ttH\n",
      "processing: trigger\n",
      "processing: TT\n",
      "processing: trigger\n",
      "processing: WJets\n",
      "processing: trigger\n",
      "processing: ST\n",
      "processing: trigger\n",
      "processing: Rest\n",
      "processing: trigger\n",
      "processing: up\n",
      "processing: ggF\n",
      "processing: pileup\n",
      "processing: VBF\n",
      "processing: pileup\n",
      "processing: ZH\n",
      "processing: pileup\n",
      "processing: WH\n",
      "processing: pileup\n",
      "processing: ttH\n",
      "processing: pileup\n",
      "processing: TT\n",
      "processing: pileup\n",
      "processing: WJets\n",
      "processing: pileup\n",
      "processing: ST\n",
      "processing: pileup\n",
      "processing: Rest\n",
      "processing: pileup\n",
      "processing: ggF\n",
      "processing: ISRPartonShower\n",
      "processing: VBF\n",
      "processing: ISRPartonShower\n",
      "processing: ZH\n",
      "processing: ISRPartonShower\n",
      "processing: WH\n",
      "processing: ISRPartonShower\n",
      "processing: ttH\n",
      "processing: ISRPartonShower\n",
      "processing: TT\n",
      "processing: ISRPartonShower\n",
      "processing: WJets\n",
      "processing: ISRPartonShower\n",
      "processing: ST\n",
      "processing: ISRPartonShower\n",
      "processing: Rest\n",
      "processing: ISRPartonShower\n",
      "processing: ggF\n",
      "processing: FSRPartonShower\n",
      "processing: VBF\n",
      "processing: FSRPartonShower\n",
      "processing: ZH\n",
      "processing: FSRPartonShower\n",
      "processing: WH\n",
      "processing: FSRPartonShower\n",
      "processing: ttH\n",
      "processing: FSRPartonShower\n",
      "processing: TT\n",
      "processing: FSRPartonShower\n",
      "processing: WJets\n",
      "processing: FSRPartonShower\n",
      "processing: ST\n",
      "processing: FSRPartonShower\n",
      "processing: Rest\n",
      "processing: FSRPartonShower\n",
      "processing: TT\n",
      "processing: QCDscale\n",
      "processing: WJets\n",
      "processing: QCDscale\n",
      "processing: ST\n",
      "processing: QCDscale\n",
      "processing: Rest\n",
      "processing: QCDscale\n",
      "processing: ggF\n",
      "processing: trigger\n",
      "processing: VBF\n",
      "processing: trigger\n",
      "processing: ZH\n",
      "processing: trigger\n",
      "processing: WH\n",
      "processing: trigger\n",
      "processing: ttH\n",
      "processing: trigger\n",
      "processing: TT\n",
      "processing: trigger\n",
      "processing: WJets\n",
      "processing: trigger\n",
      "processing: ST\n",
      "processing: trigger\n",
      "processing: Rest\n",
      "processing: trigger\n"
     ]
    }
   ],
   "source": [
    "samples = list(['data','QCD','TT','ST','WJets','Rest','ggF','WH','ZH','ttH','VBF']) #all samples we considered\n",
    "years = [\"2016APV\", \"2016\", \"2017\", \"2018\"]\n",
    "year_to_run = YEAR\n",
    "for shift in [\"down\", \"up\"]:\n",
    "    print(\"processing:\",shift)\n",
    "    for wshift, wsyst in weight_shifts.items():\n",
    "        for wsample in wsyst.samples:\n",
    "            print(\"processing:\",wsample)\n",
    "            if wsample in samples:\n",
    "                if wshift == \"pileup\" :\n",
    "                    print(\"processing:\",wshift)\n",
    "                    if shift == \"up\":\n",
    "                        for year in years: \n",
    "                            if year == year_to_run : \n",
    "                                files[wsample][f\"{wshift}_{shift}_{year}\"] = files[wsample][\"weight\"] * (files[wsample][\"puWeightUp\"] / files[wsample][\"puWeight\"])\n",
    "                            else : \n",
    "                                files[wsample][f\"{wshift}_{shift}_{year}\"] = files[wsample][\"weight\"]\n",
    "                    if shift == \"down\":\n",
    "                        for year in years: \n",
    "                            if year == year_to_run : \n",
    "                                files[wsample][f\"{wshift}_{shift}_{year}\"] = files[wsample][\"weight\"] * (files[wsample][\"puWeightDown\"] / files[wsample][\"puWeight\"])\n",
    "                            else : \n",
    "                                files[wsample][f\"{wshift}_{shift}_{year}\"] = files[wsample][\"weight\"]\n",
    "                if wshift == \"ISRPartonShower\" :\n",
    "                    print(\"processing:\",wshift)\n",
    "                    if shift == \"up\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"PSWeight_0\"]\n",
    "                    if shift == \"down\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"PSWeight_2\"] \n",
    "                if wshift == \"FSRPartonShower\" :\n",
    "                    print(\"processing:\",wshift)\n",
    "                    if shift == \"up\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"PSWeight_1\"]\n",
    "                    if shift == \"down\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"PSWeight_3\"] \n",
    "                if wshift == \"QCDscale\" :\n",
    "                    print(\"processing:\",wshift)\n",
    "                    if shift == \"up\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"LHEScaleWeight_8\"]\n",
    "                    if shift == \"down\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"LHEScaleWeight_0\"] \n",
    "                if wshift == \"trigger\" :\n",
    "                    print(\"processing:\",wshift)\n",
    "                    if shift == \"up\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * (1 + files[wsample][\"SF_unc\"])\n",
    "                    if shift == \"down\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * (1 - files[wsample][\"SF_unc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [0.0737, 0.078, ... 0.00021, 0.000176] type='4689995 * float64'>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[\"WJets\"][\"pileup_down_2016APV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [95.8, 80.2, 143, ... 167, 106, 524] type='4689995 * float32'>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[\"WJets\"][\"MH_Reco\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variation shift list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "jecs = {\n",
    "    \"JES\": \"JES_jes\",\n",
    "    \"JER\": \"JER\",\n",
    "}\n",
    "\n",
    "uncluste = {\n",
    "    \"UE\": \"unclusteredEnergy\",\n",
    "}\n",
    "\n",
    "jec_shifts = {}\n",
    "for key in jecs:\n",
    "    for shift in [\"up\", \"down\"]:\n",
    "        if key == \"JES\": \n",
    "            if shift == \"up\"   : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jesTotalUp_a\"\n",
    "            if shift == \"down\" : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jesTotalDown_a\"\n",
    "        if key == \"JER\": \n",
    "            if shift == \"up\"   : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jerUp_a\"\n",
    "            if shift == \"down\" : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jerDown_a\"\n",
    "\n",
    "ue_shifts = {}\n",
    "for key in uncluste:\n",
    "    for shift in [\"up\", \"down\"]:\n",
    "        if shift == \"up\"   : ue_shifts[f\"{key}_{shift}\"] = \"MH_Reco_UE_up\"\n",
    "        if shift == \"down\" : ue_shifts[f\"{key}_{shift}\"] = \"MH_Reco_UE_down\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JES_up': 'Mj_jesTotalUp_a',\n",
       " 'JES_down': 'Mj_jesTotalDown_a',\n",
       " 'JER_up': 'Mj_jerUp_a',\n",
       " 'JER_down': 'Mj_jerDown_a'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jec_shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UE_up': 'MH_Reco_UE_up', 'UE_down': 'MH_Reco_UE_down'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ue_shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CUT(aka. regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT = {        \n",
    "    \"SR1a\" : {k: (files[k][\"MH_Reco\"] >= 50) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] <= 0.25) & (files[k][\"a_HWW_V2\"] >= 0.99) for k in files}, \n",
    "    \"SR1b\" : {k: (files[k][\"MH_Reco\"] >= 50) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] <= 0.25) & (files[k][\"a_HWW_V2\"] >= 0.92) & (files[k][\"a_HWW_V2\"] < 0.99) for k in files},   \n",
    "    \"SR2a\" : {k: (files[k][\"MH_Reco\"] >= 50) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] > 0.25) & (files[k][\"a_HWW_V2\"] >= 0.99) & (files[k][\"DPhi\"] < 0.8)for k in files}, \n",
    "    \"SR2b\" : {k: (files[k][\"MH_Reco\"] >= 50) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] > 0.25) & (files[k][\"a_HWW_V2\"] >= 0.92) & (files[k][\"a_HWW_V2\"] < 0.99) & (files[k][\"DPhi\"] < 0.8)for k in files},   \n",
    "    \"CR1\"  : {k: (files[k][\"MH_Reco\"] >= 50) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] <= 0.25) & (files[k][\"a_HWW_V2\"] < 0.92) for k in files},\n",
    "    \"CR2\"  : {k: (files[k][\"MH_Reco\"] >= 50) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] > 0.25)  & (files[k][\"a_HWW_V2\"] < 0.92) & (files[k][\"DPhi\"] < 0.8) for k in files},   \n",
    "    }\n",
    "\n",
    "# CR: tagger < 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used actually, since we can directly blind the mass window in save_pkl function\n",
    "\n",
    "# CUT_BLINDED = {\n",
    "#         \"SR1a_blinded\" : {k: ( (files[k][\"MH_Reco\"] >= 50) & ((files[k][\"MH_Reco\"] <= 90) | (files[k][\"MH_Reco\"] >= 150)) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] <= 0.25) & (files[k][\"a_HWW_V2\"] >= 0.99) for k in files}, \n",
    "#         \"SR1b_blinded\" : {k: ( (files[k][\"MH_Reco\"] >= 50) & ((files[k][\"MH_Reco\"] <= 90) | (files[k][\"MH_Reco\"] >= 150)) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] <= 0.25) & (files[k][\"a_HWW_V2\"] >= 0.92) & (files[k][\"a_HWW_V2\"] < 0.99) for k in files},   \n",
    "#         \"SR2a_blinded\" : {k: ( (files[k][\"MH_Reco\"] >= 50) & ((files[k][\"MH_Reco\"] <= 90) | (files[k][\"MH_Reco\"] >= 150)) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] > 0.25) & (files[k][\"a_HWW_V2\"] >= 0.99) & (files[k][\"DPhi\"] < 0.8)for k in files}, \n",
    "#         \"SR2b_blinded\" : {k: ( (files[k][\"MH_Reco\"] >= 50) & ((files[k][\"MH_Reco\"] <= 90) | (files[k][\"MH_Reco\"] >= 150)) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] > 0.25) & (files[k][\"a_HWW_V2\"] >= 0.92) & (files[k][\"a_HWW_V2\"] < 0.99) & (files[k][\"DPhi\"] < 0.8)for k in files},   \n",
    "#         \"CR1_blinded\"  : {k: ( (files[k][\"MH_Reco\"] >= 50) & ((files[k][\"MH_Reco\"] <= 90) | (files[k][\"MH_Reco\"] >= 150)) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] <= 0.25) & (files[k][\"a_HWW_V2\"] < 0.92) for k in files},\n",
    "#         \"CR2_blinded\"  : {k: ( (files[k][\"MH_Reco\"] >= 50) & ((files[k][\"MH_Reco\"] <= 90) | (files[k][\"MH_Reco\"] >= 150)) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] > 0.25)  & (files[k][\"a_HWW_V2\"] < 0.92) & (files[k][\"DPhi\"] < 0.8)for k in files},   \n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save hist templates to pkl files\n",
    "\n",
    "We note here that no particular operation is needed for QCD, since we only need raw QCD MC ratio as initial tranfer factor in the actual QCD prediction, and rhalphabet method will use (data - other bkg) in fail(control) region and perform simultaneous fit with pass(signal) region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_pkl(files, path_str = plot_dir, template_file = \"templates\",year_to_run = \"2018\"):\n",
    "    \n",
    "    templates = {} #empty dict to store the templates file\n",
    "    regions = [\"SR1a\",\"SR1b\",\"CR1\",\"SR2a\",\"SR2b\",\"CR2\"] #signal regions or control regions\n",
    "    signal_region_as = [\"SR1a\",\"SR2a\"]\n",
    "    signal_region_bs = [\"SR1b\",\"SR2b\"]\n",
    "    years = [\"2016APV\", \"2016\", \"2017\", \"2018\"]\n",
    "    samples = list(['data','QCD','TT','ST','WJets','Rest','ggF','WH','ZH','ttH','VBF']) #all samples we considered\n",
    "    print(\"now running year:\",year_to_run)\n",
    "    #initialize weight based variation samples\n",
    "    hist_samples = []\n",
    "    for shift in [\"down\", \"up\"]:\n",
    "        for wshift, wsyst in weight_shifts.items():\n",
    "            for wsample in wsyst.samples:\n",
    "                if (wshift == \"pileup\") : continue\n",
    "                hist_samples.append(f\"{wsample}_{wshift}_{shift}\")\n",
    "    \n",
    "    #additionally, we have to add different year info to pileup\n",
    "    for shift in [\"down\", \"up\"]:\n",
    "        for wshift, wsyst in weight_shifts.items():\n",
    "            if not (wshift == \"pileup\") : continue\n",
    "            for wsample in wsyst.samples:\n",
    "                for year in years:\n",
    "                    hist_samples.append(f\"{wsample}_{wshift}_{shift}_{year}\")\n",
    "    \n",
    "    hist_samples += samples\n",
    "    #fill templates for different regions\n",
    "    \n",
    "    for region in regions:\n",
    "        \n",
    "        templates[region] = hist2.Hist(\n",
    "            hist2.axis.StrCategory(hist_samples, name=\"Sample\"),\n",
    "            *[shape_var.axis for shape_var in shape_vars],\n",
    "            storage=\"weight\",\n",
    "            ) #initialize a hist object\n",
    "        \n",
    "        #add center value templates first\n",
    "        for sample in samples:\n",
    "            \n",
    "            #if region is signal region and sample is signal samples, we multiply by lund plane sf\n",
    "            if (region in signal_region_as) and (sample in sig_keys): weight_to_add = weight_lp[\"a\"]\n",
    "            elif (region in signal_region_bs) and (sample in sig_keys): weight_to_add = weight_lp[\"b\"]\n",
    "            else : weight_to_add = 1.0\n",
    "            \n",
    "            data = files[sample][CUT[region][sample]]\n",
    "            templates[region].fill(\n",
    "                                Sample=sample,\n",
    "                                MH_Reco=data[\"MH_Reco\"],\n",
    "                                weight=data[\"weight\"] * weight_to_add,\n",
    "                            )\n",
    "            if sample == \"data\": \n",
    "                if (region.endswith(\"a\") or region.endswith(\"b\")):\n",
    "                    # blind signal mass windows in pass region in data even for not \"Blinded\" region\n",
    "                    # print(\"blind data of \",region)\n",
    "                    for i, shape_var in enumerate(shape_vars):\n",
    "                        if shape_var.blind_window is not None:\n",
    "                            blindBins(templates[region], shape_var.blind_window, \"data\", axis=i)\n",
    "                            \n",
    "\n",
    "\n",
    "        #add weight based variation for each sample            \n",
    "        for shift in [\"down\", \"up\"]:\n",
    "            for wshift, wsyst in weight_shifts.items():\n",
    "                if wshift == \"pileup\":\n",
    "                    for year in years: \n",
    "                        # pileup need year specific information\n",
    "                        for wsample in wsyst.samples:\n",
    "                            if wsample in samples:\n",
    "                                data = files[wsample][CUT[region][wsample]] \n",
    "                                # print(region, wsample, wshift, shift)\n",
    "                                # print(\"mass info:\",data[\"MH_Reco\"])\n",
    "                                # print(\"weight infor:\",data[f\"{wshift}_{shift}_{year}\"])\n",
    "                                            \n",
    "                                #if region is signal region and sample is signal samples, we multiply by lund plane sf\n",
    "                                if (region in signal_region_as) and (wsample in sig_keys): weight_to_add = weight_lp[\"a\"]\n",
    "                                elif (region in signal_region_bs) and (wsample in sig_keys): weight_to_add = weight_lp[\"b\"]\n",
    "                                else : weight_to_add = 1.0\n",
    "                                \n",
    "                                templates[region].fill(\n",
    "                                    Sample=wsample + f\"_{wshift}_{shift}_{year}\",\n",
    "                                    MH_Reco=data[\"MH_Reco\"],\n",
    "                                    weight=data[f\"{wshift}_{shift}_{year}\"] * weight_to_add,\n",
    "                                )                    \n",
    "                else:\n",
    "                    for wsample in wsyst.samples:\n",
    "                        if wsample in samples:\n",
    "                            data = files[wsample][CUT[region][wsample]] \n",
    "                            \n",
    "                            #if region is signal region and sample is signal samples, we multiply by lund plane sf\n",
    "                            if (region in signal_region_as) and (wsample in sig_keys): weight_to_add = weight_lp[\"a\"]\n",
    "                            elif (region in signal_region_bs) and (wsample in sig_keys): weight_to_add = weight_lp[\"b\"]\n",
    "                            else : weight_to_add = 1.0\n",
    "                            \n",
    "                            templates[region].fill(\n",
    "                                Sample=wsample + f\"_{wshift}_{shift}\",\n",
    "                                MH_Reco=data[\"MH_Reco\"],\n",
    "                                weight=data[f\"{wshift}_{shift}\"] * weight_to_add,\n",
    "                            )\n",
    "                        \n",
    "        #add shift variation for each sample\n",
    "        #1.initialize hist info\n",
    "        for wshift, wsyst in jec_shifts.items():\n",
    "            for year in years: \n",
    "                # split the JES/JER uncertainties according to year, i.e., one variation for each era\n",
    "                templates[f\"{region}_{wshift}_{year}\"] = hist2.Hist(\n",
    "                hist2.axis.StrCategory(samples, name=\"Sample\"),\n",
    "                *[shape_var.axis for shape_var in shape_vars],\n",
    "                storage=\"weight\",\n",
    "                ) #initialize a hist object\n",
    "        for wshift, wsyst in ue_shifts.items():\n",
    "                templates[f\"{region}_{wshift}\"] = hist2.Hist(\n",
    "                hist2.axis.StrCategory(samples, name=\"Sample\"),\n",
    "                *[shape_var.axis for shape_var in shape_vars],\n",
    "                storage=\"weight\",\n",
    "                ) #initialize a hist object                \n",
    "        \n",
    "        #2.fill the hist\n",
    "        for sample in mc_keys:\n",
    "            #JECS\n",
    "            for wshift, wsyst in jec_shifts.items():\n",
    "                for year in years: \n",
    "                    # split the JES/JER uncertainties according to year, i.e., one variation for each era\n",
    "                    if year == year_to_run:\n",
    "                        data = files[sample][CUT[region][sample]]\n",
    "\n",
    "                        #if region is signal region and sample is signal samples, we multiply by lund plane sf\n",
    "                        if (region in signal_region_as) and (sample in sig_keys): weight_to_add = weight_lp[\"a\"]\n",
    "                        elif (region in signal_region_bs) and (sample in sig_keys): weight_to_add = weight_lp[\"b\"]\n",
    "                        else : weight_to_add = 1.0\n",
    "                            \n",
    "                        #assign variation only to year_to_run\n",
    "                        templates[f\"{region}_{wshift}_{year}\"].fill(\n",
    "                                Sample=sample,\n",
    "                                MH_Reco=data[wsyst],\n",
    "                                weight=data[\"weight\"] * weight_to_add,\n",
    "                            )\n",
    "                        # print(f\"{region}_{wshift}_{year}\", sample, wsyst)\n",
    "                        # print(\"mass info:\",data[wsyst])\n",
    "                    else:\n",
    "                        data = files[sample][CUT[region][sample]] \n",
    "\n",
    "                        #if region is signal region and sample is signal samples, we multiply by lund plane sf\n",
    "                        if (region in signal_region_as) and (sample in sig_keys): weight_to_add = weight_lp[\"a\"]\n",
    "                        elif (region in signal_region_bs) and (sample in sig_keys): weight_to_add = weight_lp[\"b\"]\n",
    "                        else : weight_to_add = 1.0\n",
    "                        \n",
    "                        #assign same variation as center value for other years\n",
    "                        templates[f\"{region}_{wshift}_{year}\"].fill(\n",
    "                                Sample=sample,\n",
    "                                MH_Reco=data[\"MH_Reco\"],\n",
    "                                weight=data[\"weight\"] * weight_to_add,\n",
    "                            )                                    \n",
    "            \n",
    "            #un-clustered energy\n",
    "            for wshift, wsyst in ue_shifts.items():\n",
    "                data = files[sample][CUT[region][sample]] \n",
    "                        \n",
    "                #if region is signal region and sample is signal samples, we multiply by lund plane sf\n",
    "                if (region in signal_region_as) and (sample in sig_keys): weight_to_add = weight_lp[\"a\"]\n",
    "                elif (region in signal_region_bs) and (sample in sig_keys): weight_to_add = weight_lp[\"b\"]\n",
    "                else : weight_to_add = 1.0\n",
    "                        \n",
    "                templates[f\"{region}_{wshift}\"].fill(\n",
    "                                Sample=sample,\n",
    "                                MH_Reco=data[wsyst],\n",
    "                                weight=data[\"weight\"] * weight_to_add,\n",
    "                )\n",
    "\n",
    "        #extra process for QCD and data\n",
    "        for sample in [\"QCD\",\"data\"]:\n",
    "            #QCD and data doesn't have any j-shift nor weight based variation\n",
    "            \n",
    "            #JEC variation\n",
    "            for wshift, wsyst in jec_shifts.items():\n",
    "                for year in years: \n",
    "                    # split the JES/JER uncertainties according to year, i.e., one variation for each era\n",
    "                        data = files[sample][CUT[region][sample]] \n",
    "                        #always assign value with `MH_Reco` variable\n",
    "                        templates[f\"{region}_{wshift}_{year}\"].fill(\n",
    "                                Sample=sample,\n",
    "                                MH_Reco=data[\"MH_Reco\"],\n",
    "                                weight=data[\"weight\"],\n",
    "                            )\n",
    "                        \n",
    "                        #do blind procedure\n",
    "                        if sample == \"data\" and (region.endswith(\"a\") or region.endswith(\"b\")):\n",
    "                            for i, shape_var in enumerate(shape_vars):\n",
    "                                if shape_var.blind_window is not None:\n",
    "                                    blindBins(templates[f\"{region}_{wshift}_{year}\"], shape_var.blind_window, \"data\", axis=i)\n",
    "\n",
    "            #un-clustered energy variation\n",
    "            for wshift, wsyst in ue_shifts.items():\n",
    "                data = files[sample][CUT[region][sample]] \n",
    "                #always assign value with `MH_Reco` variable\n",
    "                templates[f\"{region}_{wshift}\"].fill(\n",
    "                        Sample=sample,\n",
    "                        MH_Reco=data[\"MH_Reco\"],\n",
    "                        weight=data[\"weight\"],\n",
    "                    )\n",
    "                                        \n",
    "                #do blind procedure\n",
    "                if sample == \"data\" and (region.endswith(\"a\") or region.endswith(\"b\")):\n",
    "                    for i, shape_var in enumerate(shape_vars):\n",
    "                        if shape_var.blind_window is not None:\n",
    "                            blindBins(templates[f\"{region}_{wshift}\"], shape_var.blind_window, \"data\", axis=i)\n",
    "\n",
    "                        \n",
    "        print(\"done fill template \",region)        \n",
    "    \n",
    "    #Creates blinded copies of each region's templates and saves a pickle of the templates\n",
    "    blind_window = shape_vars[0].blind_window\n",
    "    for label, template in list(templates.items()):\n",
    "        blinded_template = deepcopy(template)\n",
    "        blindBins(blinded_template, blind_window)\n",
    "        templates[f\"{label}Blinded\"] = blinded_template\n",
    "    \n",
    "    #save files\n",
    "    with open(f\"{path_str}/{template_file}_{year_to_run}.pkl\", \"wb\") as fp:\n",
    "        pkl.dump(templates, fp) # dump the templates of each region in a pkl file\n",
    "        print(\"Saved templates to\", f\"{template_file}_{year_to_run}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now running year: 2016APV\n",
      "done fill template  SR1a\n",
      "done fill template  SR1b\n",
      "done fill template  CR1\n",
      "done fill template  SR2a\n",
      "done fill template  SR2b\n",
      "done fill template  CR2\n",
      "Saved templates to templates_2016APV.pkl\n"
     ]
    }
   ],
   "source": [
    "save_pkl(files = files, year_to_run = YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PLACE_HOLDER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/ospool/cms-user/yuzhe/NanoNtupleChain/boostedHWW/postprocessing/templates.ipynb Cell 41\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22436d73636f6e6e656374227d/ospool/cms-user/yuzhe/NanoNtupleChain/boostedHWW/postprocessing/templates.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m PLACE_HOLDER\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PLACE_HOLDER' is not defined"
     ]
    }
   ],
   "source": [
    "PLACE_HOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some test about the output templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<div style=\"display:flex; align-items:center;\">\n",
       "<div style=\"width:290px;\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"-10 -105 270 120\">\n",
       "<line x1=\"-5\" y1=\"0\" x2=\"255\" y2=\"0\" style=\"fill:none;stroke-width:2;stroke:currentColor\"/>\n",
       "<text text-anchor=\"middle\" x=\"0\" y=\"15\" style=\"fill:currentColor;\">\n",
       "50\n",
       "</text>\n",
       "<text text-anchor=\"middle\" x=\"250\" y=\"15\" style=\"fill:currentColor;\">\n",
       "250\n",
       "</text>\n",
       "<text text-anchor=\"middle\" x=\"125.0\" y=\"15\" style=\"fill:currentColor;\">\n",
       "Higgs candidate MET recovery mass [GeV]\n",
       "</text>\n",
       "<polyline points=\"  0,0   0,-5.86 12.5,-5.86 12.5,-12  25,-12  25,-21.6 37.5,-21.6 37.5,-30.7  50,-30.7  50,-47.6 62.5,-47.6 62.5,-47  75,-47  75,-76.7 87.5,-76.7 87.5,-80.9 100,-80.9 100,-80.7 112.5,-80.7 112.5,-100 125,-100 125,-70.1 137.5,-70.1 137.5,-32.3 150,-32.3 150,-14.6 162.5,-14.6 162.5,-6.87 175,-6.87 175,-9.58 187.5,-9.58 187.5,-0.871 200,-0.871 200,-1.61 212.5,-1.61 212.5,-2.75 225,-2.75 225,-0.814 237.5,-0.814 237.5,-2.68 250,-2.68 250,0\" style=\"fill:none; stroke:currentColor;\"/>\n",
       "</svg>\n",
       "</div>\n",
       "<div style=\"flex=grow:1;\">\n",
       "Regular(20, 50, 250, name='MH_Reco', label='Higgs candidate MET recovery mass [GeV]')<br/>\n",
       "<hr style=\"margin-top:.2em; margin-bottom:.2em;\"/>\n",
       "Weight() =WeightedSum(value=7.28138, variance=0.0711846) <em>(WeightedSum(value=7.30128, variance=0.0713826) with flow)</em>\n",
       "\n",
       "</div>\n",
       "</div>\n",
       "</html>"
      ],
      "text/plain": [
       "Hist(Regular(20, 50, 250, name='MH_Reco', label='Higgs candidate MET recovery mass [GeV]'), storage=Weight()) # Sum: WeightedSum(value=7.28138, variance=0.0711846) (WeightedSum(value=7.30128, variance=0.0713826) with flow)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"{plot_dir}/templates_2018.pkl\",\"rb\") as f:\n",
    "    hists_template1 = pkl.load(f)\n",
    "# hists_template[\"pass\"][\"QCD\",:]  \n",
    "# hists_template[\"pass\"][\"QCD\",:].sum().value\n",
    "# hists_template[\"CR2\"][\"QCD\",:]\n",
    "hists_template1[\"SR1a_JES_up_2018\"][\"ggF\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SR1a', 'SR1a_JES_up_2016APV', 'SR1a_JES_up_2016', 'SR1a_JES_up_2017', 'SR1a_JES_up_2018', 'SR1a_JES_down_2016APV', 'SR1a_JES_down_2016', 'SR1a_JES_down_2017', 'SR1a_JES_down_2018', 'SR1a_JER_up_2016APV', 'SR1a_JER_up_2016', 'SR1a_JER_up_2017', 'SR1a_JER_up_2018', 'SR1a_JER_down_2016APV', 'SR1a_JER_down_2016', 'SR1a_JER_down_2017', 'SR1a_JER_down_2018', 'SR1a_UE_up', 'SR1a_UE_down', 'SR1b', 'SR1b_JES_up_2016APV', 'SR1b_JES_up_2016', 'SR1b_JES_up_2017', 'SR1b_JES_up_2018', 'SR1b_JES_down_2016APV', 'SR1b_JES_down_2016', 'SR1b_JES_down_2017', 'SR1b_JES_down_2018', 'SR1b_JER_up_2016APV', 'SR1b_JER_up_2016', 'SR1b_JER_up_2017', 'SR1b_JER_up_2018', 'SR1b_JER_down_2016APV', 'SR1b_JER_down_2016', 'SR1b_JER_down_2017', 'SR1b_JER_down_2018', 'SR1b_UE_up', 'SR1b_UE_down', 'CR1', 'CR1_JES_up_2016APV', 'CR1_JES_up_2016', 'CR1_JES_up_2017', 'CR1_JES_up_2018', 'CR1_JES_down_2016APV', 'CR1_JES_down_2016', 'CR1_JES_down_2017', 'CR1_JES_down_2018', 'CR1_JER_up_2016APV', 'CR1_JER_up_2016', 'CR1_JER_up_2017', 'CR1_JER_up_2018', 'CR1_JER_down_2016APV', 'CR1_JER_down_2016', 'CR1_JER_down_2017', 'CR1_JER_down_2018', 'CR1_UE_up', 'CR1_UE_down', 'SR2a', 'SR2a_JES_up_2016APV', 'SR2a_JES_up_2016', 'SR2a_JES_up_2017', 'SR2a_JES_up_2018', 'SR2a_JES_down_2016APV', 'SR2a_JES_down_2016', 'SR2a_JES_down_2017', 'SR2a_JES_down_2018', 'SR2a_JER_up_2016APV', 'SR2a_JER_up_2016', 'SR2a_JER_up_2017', 'SR2a_JER_up_2018', 'SR2a_JER_down_2016APV', 'SR2a_JER_down_2016', 'SR2a_JER_down_2017', 'SR2a_JER_down_2018', 'SR2a_UE_up', 'SR2a_UE_down', 'SR2b', 'SR2b_JES_up_2016APV', 'SR2b_JES_up_2016', 'SR2b_JES_up_2017', 'SR2b_JES_up_2018', 'SR2b_JES_down_2016APV', 'SR2b_JES_down_2016', 'SR2b_JES_down_2017', 'SR2b_JES_down_2018', 'SR2b_JER_up_2016APV', 'SR2b_JER_up_2016', 'SR2b_JER_up_2017', 'SR2b_JER_up_2018', 'SR2b_JER_down_2016APV', 'SR2b_JER_down_2016', 'SR2b_JER_down_2017', 'SR2b_JER_down_2018', 'SR2b_UE_up', 'SR2b_UE_down', 'CR2', 'CR2_JES_up_2016APV', 'CR2_JES_up_2016', 'CR2_JES_up_2017', 'CR2_JES_up_2018', 'CR2_JES_down_2016APV', 'CR2_JES_down_2016', 'CR2_JES_down_2017', 'CR2_JES_down_2018', 'CR2_JER_up_2016APV', 'CR2_JER_up_2016', 'CR2_JER_up_2017', 'CR2_JER_up_2018', 'CR2_JER_down_2016APV', 'CR2_JER_down_2016', 'CR2_JER_down_2017', 'CR2_JER_down_2018', 'CR2_UE_up', 'CR2_UE_down', 'SR1aBlinded', 'SR1a_JES_up_2016APVBlinded', 'SR1a_JES_up_2016Blinded', 'SR1a_JES_up_2017Blinded', 'SR1a_JES_up_2018Blinded', 'SR1a_JES_down_2016APVBlinded', 'SR1a_JES_down_2016Blinded', 'SR1a_JES_down_2017Blinded', 'SR1a_JES_down_2018Blinded', 'SR1a_JER_up_2016APVBlinded', 'SR1a_JER_up_2016Blinded', 'SR1a_JER_up_2017Blinded', 'SR1a_JER_up_2018Blinded', 'SR1a_JER_down_2016APVBlinded', 'SR1a_JER_down_2016Blinded', 'SR1a_JER_down_2017Blinded', 'SR1a_JER_down_2018Blinded', 'SR1a_UE_upBlinded', 'SR1a_UE_downBlinded', 'SR1bBlinded', 'SR1b_JES_up_2016APVBlinded', 'SR1b_JES_up_2016Blinded', 'SR1b_JES_up_2017Blinded', 'SR1b_JES_up_2018Blinded', 'SR1b_JES_down_2016APVBlinded', 'SR1b_JES_down_2016Blinded', 'SR1b_JES_down_2017Blinded', 'SR1b_JES_down_2018Blinded', 'SR1b_JER_up_2016APVBlinded', 'SR1b_JER_up_2016Blinded', 'SR1b_JER_up_2017Blinded', 'SR1b_JER_up_2018Blinded', 'SR1b_JER_down_2016APVBlinded', 'SR1b_JER_down_2016Blinded', 'SR1b_JER_down_2017Blinded', 'SR1b_JER_down_2018Blinded', 'SR1b_UE_upBlinded', 'SR1b_UE_downBlinded', 'CR1Blinded', 'CR1_JES_up_2016APVBlinded', 'CR1_JES_up_2016Blinded', 'CR1_JES_up_2017Blinded', 'CR1_JES_up_2018Blinded', 'CR1_JES_down_2016APVBlinded', 'CR1_JES_down_2016Blinded', 'CR1_JES_down_2017Blinded', 'CR1_JES_down_2018Blinded', 'CR1_JER_up_2016APVBlinded', 'CR1_JER_up_2016Blinded', 'CR1_JER_up_2017Blinded', 'CR1_JER_up_2018Blinded', 'CR1_JER_down_2016APVBlinded', 'CR1_JER_down_2016Blinded', 'CR1_JER_down_2017Blinded', 'CR1_JER_down_2018Blinded', 'CR1_UE_upBlinded', 'CR1_UE_downBlinded', 'SR2aBlinded', 'SR2a_JES_up_2016APVBlinded', 'SR2a_JES_up_2016Blinded', 'SR2a_JES_up_2017Blinded', 'SR2a_JES_up_2018Blinded', 'SR2a_JES_down_2016APVBlinded', 'SR2a_JES_down_2016Blinded', 'SR2a_JES_down_2017Blinded', 'SR2a_JES_down_2018Blinded', 'SR2a_JER_up_2016APVBlinded', 'SR2a_JER_up_2016Blinded', 'SR2a_JER_up_2017Blinded', 'SR2a_JER_up_2018Blinded', 'SR2a_JER_down_2016APVBlinded', 'SR2a_JER_down_2016Blinded', 'SR2a_JER_down_2017Blinded', 'SR2a_JER_down_2018Blinded', 'SR2a_UE_upBlinded', 'SR2a_UE_downBlinded', 'SR2bBlinded', 'SR2b_JES_up_2016APVBlinded', 'SR2b_JES_up_2016Blinded', 'SR2b_JES_up_2017Blinded', 'SR2b_JES_up_2018Blinded', 'SR2b_JES_down_2016APVBlinded', 'SR2b_JES_down_2016Blinded', 'SR2b_JES_down_2017Blinded', 'SR2b_JES_down_2018Blinded', 'SR2b_JER_up_2016APVBlinded', 'SR2b_JER_up_2016Blinded', 'SR2b_JER_up_2017Blinded', 'SR2b_JER_up_2018Blinded', 'SR2b_JER_down_2016APVBlinded', 'SR2b_JER_down_2016Blinded', 'SR2b_JER_down_2017Blinded', 'SR2b_JER_down_2018Blinded', 'SR2b_UE_upBlinded', 'SR2b_UE_downBlinded', 'CR2Blinded', 'CR2_JES_up_2016APVBlinded', 'CR2_JES_up_2016Blinded', 'CR2_JES_up_2017Blinded', 'CR2_JES_up_2018Blinded', 'CR2_JES_down_2016APVBlinded', 'CR2_JES_down_2016Blinded', 'CR2_JES_down_2017Blinded', 'CR2_JES_down_2018Blinded', 'CR2_JER_up_2016APVBlinded', 'CR2_JER_up_2016Blinded', 'CR2_JER_up_2017Blinded', 'CR2_JER_up_2018Blinded', 'CR2_JER_down_2016APVBlinded', 'CR2_JER_down_2016Blinded', 'CR2_JER_down_2017Blinded', 'CR2_JER_down_2018Blinded', 'CR2_UE_upBlinded', 'CR2_UE_downBlinded'])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hists_template1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a_JES_up_2018\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a\"][\"TT_FSRPartonShower_up\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a\"][\"TT\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a\"][\"TT_FSRPartonShower_down\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_template1 = hists_template1[\"SR1a\"][\"QCD\", :]\n",
    "err = sample_template1.variances()\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , axis in enumerate(hists_template1[\"SR1a\"].axes[1:]):\n",
    "    print(i, axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test of HHbbVV analysis for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/home/pku/zhaoyz/Higgs/HHbbVV/src/HHbbVV/postprocessing/templates/23Jun14/2018_templates.pkl\",\"rb\") as f:\n",
    "with open(\"/ospool/cms-user/yuzhe/BoostedHWW/prediction/HHbbVV/src/HHbbVV/postprocessing/templates/24Mar15UpdateData/2018_templates.pkl\",\"rb\") as f:\n",
    "    hists_template2 = pkl.load(f)\n",
    "# hists_template[\"pass\"][\"QCD\",:]  \n",
    "# hists_template[\"pass\"][\"QCD\",:].sum().value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass_JES_up\"][\"ST\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hist(\n",
    "  StrCategory(['HHbbVV', 'ggHH_kl_2p45_kt_1_HHbbVV', 'ggHH_kl_5_kt_1_HHbbVV', 'ggHH_kl_0_kt_1_HHbbVV', 'VBFHHbbVV', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV', 'QCD', 'TT', 'ST', 'W+Jets', 'Z+Jets', 'Diboson', 'ggFHbb', 'VBFHbb', 'ZHbb', 'WHbb', 'ggZHbb', 'ttHbb', 'HWW', 'Data', 'HHbbVV_txbb_down', 'ggHH_kl_2p45_kt_1_HHbbVV_txbb_down', 'ggHH_kl_5_kt_1_HHbbVV_txbb_down', 'ggHH_kl_0_kt_1_HHbbVV_txbb_down', 'VBFHHbbVV_txbb_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_txbb_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_txbb_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_txbb_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_txbb_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_txbb_down', 'HHbbVV_pileup_down', 'ggHH_kl_2p45_kt_1_HHbbVV_pileup_down', 'ggHH_kl_5_kt_1_HHbbVV_pileup_down', 'ggHH_kl_0_kt_1_HHbbVV_pileup_down', 'VBFHHbbVV_pileup_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pileup_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pileup_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pileup_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pileup_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pileup_down', 'TT_pileup_down', 'ST_pileup_down', 'W+Jets_pileup_down', 'Z+Jets_pileup_down', 'HHbbVV_pileupID_down', 'ggHH_kl_2p45_kt_1_HHbbVV_pileupID_down', 'ggHH_kl_5_kt_1_HHbbVV_pileupID_down', 'ggHH_kl_0_kt_1_HHbbVV_pileupID_down', 'VBFHHbbVV_pileupID_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pileupID_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pileupID_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pileupID_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pileupID_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pileupID_down', 'TT_pileupID_down', 'ST_pileupID_down', 'W+Jets_pileupID_down', 'Z+Jets_pileupID_down', 'HHbbVV_ISRPartonShower_down', 'ggHH_kl_2p45_kt_1_HHbbVV_ISRPartonShower_down', 'ggHH_kl_5_kt_1_HHbbVV_ISRPartonShower_down', 'ggHH_kl_0_kt_1_HHbbVV_ISRPartonShower_down', 'VBFHHbbVV_ISRPartonShower_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_ISRPartonShower_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_ISRPartonShower_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_ISRPartonShower_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_ISRPartonShower_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_ISRPartonShower_down', 'TT_ISRPartonShower_down', 'ST_ISRPartonShower_down', 'W+Jets_ISRPartonShower_down', 'Z+Jets_ISRPartonShower_down', 'HHbbVV_FSRPartonShower_down', 'ggHH_kl_2p45_kt_1_HHbbVV_FSRPartonShower_down', 'ggHH_kl_5_kt_1_HHbbVV_FSRPartonShower_down', 'ggHH_kl_0_kt_1_HHbbVV_FSRPartonShower_down', 'VBFHHbbVV_FSRPartonShower_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_FSRPartonShower_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_FSRPartonShower_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_FSRPartonShower_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_FSRPartonShower_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_FSRPartonShower_down', 'TT_FSRPartonShower_down', 'ST_FSRPartonShower_down', 'W+Jets_FSRPartonShower_down', 'Z+Jets_FSRPartonShower_down', 'HHbbVV_L1EcalPrefiring_down', 'ggHH_kl_2p45_kt_1_HHbbVV_L1EcalPrefiring_down', 'ggHH_kl_5_kt_1_HHbbVV_L1EcalPrefiring_down', 'ggHH_kl_0_kt_1_HHbbVV_L1EcalPrefiring_down', 'VBFHHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_L1EcalPrefiring_down', 'TT_L1EcalPrefiring_down', 'ST_L1EcalPrefiring_down', 'W+Jets_L1EcalPrefiring_down', 'Z+Jets_L1EcalPrefiring_down', 'HHbbVV_electron_id_down', 'ggHH_kl_2p45_kt_1_HHbbVV_electron_id_down', 'ggHH_kl_5_kt_1_HHbbVV_electron_id_down', 'ggHH_kl_0_kt_1_HHbbVV_electron_id_down', 'VBFHHbbVV_electron_id_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_electron_id_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_electron_id_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_electron_id_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_electron_id_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_electron_id_down', 'TT_electron_id_down', 'ST_electron_id_down', 'W+Jets_electron_id_down', 'Z+Jets_electron_id_down', 'HHbbVV_muon_id_down', 'ggHH_kl_2p45_kt_1_HHbbVV_muon_id_down', 'ggHH_kl_5_kt_1_HHbbVV_muon_id_down', 'ggHH_kl_0_kt_1_HHbbVV_muon_id_down', 'VBFHHbbVV_muon_id_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_muon_id_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_muon_id_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_muon_id_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_muon_id_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_muon_id_down', 'TT_muon_id_down', 'ST_muon_id_down', 'W+Jets_muon_id_down', 'Z+Jets_muon_id_down', 'HHbbVV_scale_down', 'ggHH_kl_2p45_kt_1_HHbbVV_scale_down', 'ggHH_kl_5_kt_1_HHbbVV_scale_down', 'ggHH_kl_0_kt_1_HHbbVV_scale_down', 'VBFHHbbVV_scale_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_scale_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_scale_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_scale_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_scale_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_scale_down', 'TT_scale_down', 'HHbbVV_pdf_down', 'ggHH_kl_2p45_kt_1_HHbbVV_pdf_down', 'ggHH_kl_5_kt_1_HHbbVV_pdf_down', 'ggHH_kl_0_kt_1_HHbbVV_pdf_down', 'VBFHHbbVV_pdf_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pdf_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pdf_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pdf_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pdf_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pdf_down', 'HHbbVV_txbb_up', 'ggHH_kl_2p45_kt_1_HHbbVV_txbb_up', 'ggHH_kl_5_kt_1_HHbbVV_txbb_up', 'ggHH_kl_0_kt_1_HHbbVV_txbb_up', 'VBFHHbbVV_txbb_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_txbb_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_txbb_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_txbb_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_txbb_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_txbb_up', 'HHbbVV_pileup_up', 'ggHH_kl_2p45_kt_1_HHbbVV_pileup_up', 'ggHH_kl_5_kt_1_HHbbVV_pileup_up', 'ggHH_kl_0_kt_1_HHbbVV_pileup_up', 'VBFHHbbVV_pileup_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pileup_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pileup_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pileup_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pileup_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pileup_up', 'TT_pileup_up', 'ST_pileup_up', 'W+Jets_pileup_up', 'Z+Jets_pileup_up', 'HHbbVV_pileupID_up', 'ggHH_kl_2p45_kt_1_HHbbVV_pileupID_up', 'ggHH_kl_5_kt_1_HHbbVV_pileupID_up', 'ggHH_kl_0_kt_1_HHbbVV_pileupID_up', 'VBFHHbbVV_pileupID_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pileupID_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pileupID_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pileupID_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pileupID_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pileupID_up', 'TT_pileupID_up', 'ST_pileupID_up', 'W+Jets_pileupID_up', 'Z+Jets_pileupID_up', 'HHbbVV_ISRPartonShower_up', 'ggHH_kl_2p45_kt_1_HHbbVV_ISRPartonShower_up', 'ggHH_kl_5_kt_1_HHbbVV_ISRPartonShower_up', 'ggHH_kl_0_kt_1_HHbbVV_ISRPartonShower_up', 'VBFHHbbVV_ISRPartonShower_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_ISRPartonShower_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_ISRPartonShower_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_ISRPartonShower_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_ISRPartonShower_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_ISRPartonShower_up', 'TT_ISRPartonShower_up', 'ST_ISRPartonShower_up', 'W+Jets_ISRPartonShower_up', 'Z+Jets_ISRPartonShower_up', 'HHbbVV_FSRPartonShower_up', 'ggHH_kl_2p45_kt_1_HHbbVV_FSRPartonShower_up', 'ggHH_kl_5_kt_1_HHbbVV_FSRPartonShower_up', 'ggHH_kl_0_kt_1_HHbbVV_FSRPartonShower_up', 'VBFHHbbVV_FSRPartonShower_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_FSRPartonShower_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_FSRPartonShower_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_FSRPartonShower_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_FSRPartonShower_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_FSRPartonShower_up', 'TT_FSRPartonShower_up', 'ST_FSRPartonShower_up', 'W+Jets_FSRPartonShower_up', 'Z+Jets_FSRPartonShower_up', 'HHbbVV_L1EcalPrefiring_up', 'ggHH_kl_2p45_kt_1_HHbbVV_L1EcalPrefiring_up', 'ggHH_kl_5_kt_1_HHbbVV_L1EcalPrefiring_up', 'ggHH_kl_0_kt_1_HHbbVV_L1EcalPrefiring_up', 'VBFHHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_L1EcalPrefiring_up', 'TT_L1EcalPrefiring_up', 'ST_L1EcalPrefiring_up', 'W+Jets_L1EcalPrefiring_up', 'Z+Jets_L1EcalPrefiring_up', 'HHbbVV_electron_id_up', 'ggHH_kl_2p45_kt_1_HHbbVV_electron_id_up', 'ggHH_kl_5_kt_1_HHbbVV_electron_id_up', 'ggHH_kl_0_kt_1_HHbbVV_electron_id_up', 'VBFHHbbVV_electron_id_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_electron_id_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_electron_id_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_electron_id_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_electron_id_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_electron_id_up', 'TT_electron_id_up', 'ST_electron_id_up', 'W+Jets_electron_id_up', 'Z+Jets_electron_id_up', 'HHbbVV_muon_id_up', 'ggHH_kl_2p45_kt_1_HHbbVV_muon_id_up', 'ggHH_kl_5_kt_1_HHbbVV_muon_id_up', 'ggHH_kl_0_kt_1_HHbbVV_muon_id_up', 'VBFHHbbVV_muon_id_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_muon_id_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_muon_id_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_muon_id_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_muon_id_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_muon_id_up', 'TT_muon_id_up', 'ST_muon_id_up', 'W+Jets_muon_id_up', 'Z+Jets_muon_id_up', 'HHbbVV_scale_up', 'ggHH_kl_2p45_kt_1_HHbbVV_scale_up', 'ggHH_kl_5_kt_1_HHbbVV_scale_up', 'ggHH_kl_0_kt_1_HHbbVV_scale_up', 'VBFHHbbVV_scale_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_scale_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_scale_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_scale_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_scale_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_scale_up', 'TT_scale_up', 'HHbbVV_pdf_up', 'ggHH_kl_2p45_kt_1_HHbbVV_pdf_up', 'ggHH_kl_5_kt_1_HHbbVV_pdf_up', 'ggHH_kl_0_kt_1_HHbbVV_pdf_up', 'VBFHHbbVV_pdf_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pdf_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pdf_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pdf_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pdf_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pdf_up'], name='Sample'),\n",
    "  Regular(20, 50, 250, name='bbFatJetParticleNetMass', label='$m^{bb}_\\\\mathrm{Reg}$ (GeV)'),\n",
    "  storage=Weight()) # Sum: WeightedSum(value=-nan, variance=88.7794) (WeightedSum(value=-nan, variance=88.7794) with flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"][\"ST\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass_JES_down\"][\"ST\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"][\"HHbbVV_pileup_up\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"][\"HHbbVV_pileup_up\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"][\"HHbbVV\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"passBlinded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test how to load and use the *.pkl template file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template(h, sample):\n",
    "    ''' \n",
    "    histogram h Hist, with axes:[\"samples\",\"systematic\",\"MH_Reco\"]\n",
    "    sample is sample name in [\"QCD\",...,\"data\"]\n",
    "    '''\n",
    "    mass_axis = 1 #axis index\n",
    "    massbins = h.axes[mass_axis].edges\n",
    "    return (h[sample, :].values(), massbins, \"MH_Reco\")\n",
    "\n",
    "a = get_template(hists_template1[\"SR1a\"],\"QCD\")\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make some plots to test the variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins, x_min, x_max = 20, 50, 250\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=[\"tab:blue\",\t\"tab:orange\",\t\"tab:green\",\t\"tab:red\",\t\"tab:purple\", \"tab:brown\", \"tab:pink\", \"k\",\"tab:olive\" ,\t\"tab:cyan\"])\n",
    "\n",
    "f = plt.figure(figsize=(14, 15))\n",
    "ax = f.add_subplot(1, 1, 1)  \n",
    "ax.grid()\n",
    "\n",
    "year = \"2018\"\n",
    "LUMI = {\"2016\": 36.33, \"2017\": 41.48, \"2018\": 59.83,\"Full-Run2\":138}\n",
    "hep.cms.label(loc = 1, data=True, year=year, ax=ax, lumi=LUMI[year], fontsize=18, llabel='Preliminary')\n",
    "\n",
    "hist_region = bh.Histogram(bh.axis.Regular(nbins, x_min, x_max), storage=bh.storage.Weight())\n",
    "hep.histplot(get_template(hists_template1[\"SR1a\"],\"WJets_QCDscale_up\")[0], bins=get_template(hists_template1[\"SR1a_JES_up_2018\"],\"TT\")[1], label=\"Test \", histtype='step', stack=False, linewidth=2, ax=ax,color = \"red\")\n",
    "\n",
    "hist_region = bh.Histogram(bh.axis.Regular(nbins, x_min, x_max), storage=bh.storage.Weight())\n",
    "hep.histplot(get_template(hists_template1[\"SR1a\"],\"WJets\")[0], bins=get_template(hists_template1[\"SR1a_JES_up_2018\"],\"TT\")[1], label=\"Test \", histtype='step', stack=False, linewidth=2, ax=ax,color = \"blue\")\n",
    "\n",
    "hist_region = bh.Histogram(bh.axis.Regular(nbins, x_min, x_max), storage=bh.storage.Weight())\n",
    "hep.histplot(get_template(hists_template1[\"SR1a\"],\"WJets_QCDscale_down\")[0], bins=get_template(hists_template1[\"SR1a_JES_up_2018\"],\"TT\")[1], label=\"Test \", histtype='step', stack=False, linewidth=2, ax=ax,color = \"orange\")\n",
    "\n",
    "\n",
    "ax.set_ylabel(\"Events\")\n",
    "ax.legend(loc=\"upper right\", ncol=1, frameon=False, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = {\n",
    "        \"CR1\" :{\"SRa\": \"SR1a\",\"SRb\":\"SR1b\"},\n",
    "        \"CR2\" :{\"SRa\": \"SR2a\",\"SRb\":\"SR2b\"},\n",
    "        \"CR3\" :{\"SRa\": \"SR3a\",\"SRb\":\"SR3b\"},\n",
    "        }\n",
    "\n",
    "regions_blinded = { key_fail + \"_blinded\": {key_pass + \"_blinded\" : key_pass_ab + \"_blinded\" for key_pass , key_pass_ab in key_pass_dict.items()}  for key_fail , key_pass_dict in regions.items()}\n",
    "regions_blinded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"SR1a_blinded\"\n",
    "pass_region = (\"a_\" in region)\n",
    "pass_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"SR1aBlinded\"\n",
    "region_noblinded = region.split(\"Blinded\")[0]\n",
    "region_noblinded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc8653c37afde981a02f518cc5ed66e36d68f5e1c41895fdf66da08341e86c45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
