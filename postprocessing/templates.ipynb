{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction: This is the Jupyter notebook to do the flowing things:\n",
    "\n",
    "1. Read slimmed PKU Tree files\n",
    "2. Store the raw MC histograms to pickle files\n",
    "\n",
    "kernel:HWW\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# import pandas as pd\n",
    "import random\n",
    "import awkward as ak\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import boost_histogram as bh\n",
    "# from scipy import interpolate\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "from cycler import cycler\n",
    "import uproot\n",
    "# means uproot4\n",
    "# import ROOT\n",
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle as pkl\n",
    "import warnings\n",
    "\n",
    "import hist as hist2\n",
    "import pyarrow\n",
    "# import utils #local file: utils.py\n",
    "import yaml\n",
    "from typing import Dict, List, Union\n",
    "from dataclasses import dataclass, field\n",
    "from copy import deepcopy\n",
    "\n",
    "# from coffea import hist\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "from coffea.nanoevents.methods import vector\n",
    "# from coffea.nanoevents.methods.vector import PtEtaPhiMLorentzVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = \"2017\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read SlimmedTree files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the slimmedtree files using uproot\n",
    "\n",
    "#different year available here.\n",
    "# year = \"2016APV\"\n",
    "# year = \"2016\"\n",
    "# year = \"2017\"\n",
    "# year = \"2018\"\n",
    "# year = \"Full-Run2\"\n",
    "year = YEAR\n",
    "\n",
    "#if run on PKU cluster, use this:\n",
    "# CustNanoData = {\n",
    "#     'data'        : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/Data/SlimmedTree_Data.root\"%(year),\n",
    "#     'QCD'         : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/MC/SlimmedTree_QCD.root\"%(year),\n",
    "#     'Top'         : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/MC/SlimmedTree_Top.root\"%(year),\n",
    "#     'WJets'       : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/MC/SlimmedTree_WJets.root\"%(year),\n",
    "#     'Rest'        : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/MC/SlimmedTree_Rest.root\"%(year),\n",
    "#     'TotalSignal' : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/Signal/SlimmedTree_Total.root\"%(year),\n",
    "#     'ggF'         : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/Signal/SlimmedTree_GluGlu.root\"%(year),\n",
    "#     'VH'          : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/Signal/SlimmedTree_VH.root\"%(year),\n",
    "#     'ttH'         : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/Signal/SlimmedTree_ttH.root\"%(year),\n",
    "#     'VBF'         : \"/data/bond/zhaoyz/SlimmedTree/V5/%s/Signal/SlimmedTree_VBF.root\"%(year),\n",
    "# } \n",
    "\n",
    "#if run on lxplus, use this:\n",
    "# CustNanoData = {\n",
    "#     'data'        : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/Data/SlimmedTree_Data.root\"%(year),\n",
    "#     'QCD'         : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/MC/SlimmedTree_QCD.root\"%(year),\n",
    "#     'Top'         : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/MC/SlimmedTree_Top.root\"%(year),\n",
    "#     'WJets'       : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/MC/SlimmedTree_WJets.root\"%(year),\n",
    "#     'Rest'        : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/MC/SlimmedTree_Rest.root\"%(year),\n",
    "#     'TotalSignal' : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/Signal/SlimmedTree_Total.root\"%(year),\n",
    "#     'ggF'         : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/Signal/SlimmedTree_GluGlu.root\"%(year),\n",
    "#     'VH'          : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/Signal/SlimmedTree_VH.root\"%(year),\n",
    "#     'ttH'         : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/Signal/SlimmedTree_ttH.root\"%(year),\n",
    "#     'VBF'         : \"/eos/user/y/yuzhe/HWW/SlimmedTree/V5/%s/Signal/SlimmedTree_VBF.root\"%(year),\n",
    "# }  \n",
    "\n",
    "#if run on CMSconnect, use this:\n",
    "CustNanoData = {\n",
    "    'data'        : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Data/SlimmedTree_Data.root\"%(year),\n",
    "    'QCD'         : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/MC/SlimmedTree_QCD.root\"%(year),\n",
    "    'TT'          : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/MC/SlimmedTree_TT.root\"%(year),\n",
    "    'ST'          : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/MC/SlimmedTree_ST.root\"%(year),\n",
    "    'WJets'       : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/MC/SlimmedTree_WJets.root\"%(year),\n",
    "    'Rest'        : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/MC/SlimmedTree_Rest.root\"%(year),\n",
    "    'TotalSignal' : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Signal/SlimmedTree_Total.root\"%(year),\n",
    "    'ggF'         : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Signal/SlimmedTree_GluGlu.root\"%(year),\n",
    "    'ZH'          : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Signal/SlimmedTree_WH.root\"%(year),\n",
    "    'WH'          : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Signal/SlimmedTree_HZJ.root\"%(year),\n",
    "    'ttH'         : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Signal/SlimmedTree_ttH.root\"%(year),\n",
    "    'VBF'         : \"/ospool/cms-user/yuzhe/SlimmedTree/V6/%s/Signal/SlimmedTree_VBF.root\"%(year),\n",
    "}        \n",
    "\n",
    "files = {typefile : {} for typefile in CustNanoData}\n",
    "for typefile in CustNanoData:\n",
    "    files[typefile] = uproot.lazy({CustNanoData[typefile]: \"PKUTree\" })\n",
    "    \n",
    "#for signal decomposition plots, we store these information in different objects\n",
    "files[r\"$H^{4q}$\"] = files[\"TotalSignal\"][files[\"TotalSignal\"][\"R4q_a\"] == 1]\n",
    "files[r\"$H^{3q}$\"] = files[\"TotalSignal\"][(files[\"TotalSignal\"][\"R3q_a\"] == 1) & (files[\"TotalSignal\"][\"R3q_taudecay_a\"] != 1)]\n",
    "files[\"W\"] = files[\"TotalSignal\"][files[\"TotalSignal\"][\"w_a\"] == 1]\n",
    "files[\"top\"] = files[\"TotalSignal\"][files[\"TotalSignal\"][\"t_a\"] == 1]\n",
    "files[\"Z\"] = files[\"TotalSignal\"][files[\"TotalSignal\"][\"z_a\"] == 1]\n",
    "files[r\"$H^{lqq}$\"] = files[\"TotalSignal\"][(files[\"TotalSignal\"][\"Rlqq_a\"] == 1) | (files[\"TotalSignal\"][\"R3q_taudecay_a\"] == 1)]\n",
    "files[\"g/q\"] = files[\"TotalSignal\"][files[\"TotalSignal\"][\"gKK_g_a\"] == 1]\n",
    "files[\"rest\"] = files[\"TotalSignal\"][(files[\"TotalSignal\"][\"u_a\"] == 1) | (files[\"TotalSignal\"][\"Rlq_a\"] == 1) | (files[\"TotalSignal\"][\"R2q_a\"] == 1)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get DPhi in the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dphi(events):\n",
    "    pT_higgs   = events[\"PTj_V2_a\"]\n",
    "    eta_higgs  = events[\"Etaj_V2_a\"]\n",
    "    phi_higgs  = events[\"Phij_V2_a\"]\n",
    "    mass_higgs = events[\"Mj_V2_a\"]\n",
    "    pT_MET = events[\"MET_et\"]\n",
    "    eta_MET = events[\"Etaj_V2_a\"]\n",
    "    phi_MET = events[\"MET_phi\"]\n",
    "    mass_MET = ak.zeros_like(events[\"MET_phi\"])\n",
    "    vec_higgs = ak.zip({\n",
    "        \"pt\"   : pT_higgs   ,\n",
    "        \"eta\"  : eta_higgs  ,\n",
    "        \"phi\"  : phi_higgs  ,\n",
    "        \"mass\" : mass_higgs ,\n",
    "    },\n",
    "    with_name=\"PtEtaPhiMLorentzVector\",\n",
    "    behavior=vector.behavior,\n",
    "    )\n",
    "    vec_MET = ak.zip({\n",
    "        \"pt\"   :   pT_MET   ,\n",
    "        \"eta\"  :  eta_MET  ,\n",
    "        \"phi\"  :  phi_MET  ,\n",
    "        \"mass\" : mass_MET ,\n",
    "    },\n",
    "    with_name=\"PtEtaPhiMLorentzVector\",\n",
    "    behavior=vector.behavior,\n",
    "    )\n",
    "    delta_phi = np.subtract(vec_MET.phi, vec_higgs.phi)\n",
    "    delta_phi = np.where(delta_phi > np.pi, delta_phi - 2*np.pi, delta_phi)\n",
    "    delta_phi = np.where(delta_phi < -np.pi, delta_phi + 2*np.pi, delta_phi)\n",
    "    delta_phi = np.abs(delta_phi)\n",
    "    print(delta_phi)\n",
    "    events[\"DPhi\"] = delta_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in files:\n",
    "    print(\"Add dphi of:\",k)\n",
    "    get_dphi(events=files[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MET recovery mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_reco(events,mass = \"Mj_corr_V2_a\", MET_UE = None):\n",
    "    pT_higgs   = events[\"PTj_V2_a\"]\n",
    "    eta_higgs  = events[\"Etaj_V2_a\"]\n",
    "    phi_higgs  = events[\"Phij_V2_a\"]\n",
    "    mass_higgs = events[mass]\n",
    "    if not MET_UE:\n",
    "        pT_MET = events[\"MET_et\"]\n",
    "        phi_MET = events[\"MET_phi\"]\n",
    "    elif MET_UE == \"up\":\n",
    "        pT_MET = events[\"MET_et_UEup\"]\n",
    "        phi_MET = events[\"MET_phi_UEup\"]\n",
    "    elif MET_UE == \"down\":\n",
    "        pT_MET = events[\"MET_et_UEdown\"]\n",
    "        phi_MET = events[\"MET_phi_UEdown\"]\n",
    "    eta_MET = events[\"Etaj_V2_a\"]\n",
    "    mass_MET = ak.zeros_like(events[\"MET_phi\"])\n",
    "    vec_higgs = ak.zip({\n",
    "        \"pt\"   : pT_higgs   ,\n",
    "        \"eta\"  : eta_higgs  ,\n",
    "        \"phi\"  : phi_higgs  ,\n",
    "        \"mass\" : mass_higgs ,\n",
    "    },\n",
    "    with_name=\"PtEtaPhiMLorentzVector\",\n",
    "    behavior=vector.behavior,\n",
    "    )\n",
    "\n",
    "    vec_MET = ak.zip({\n",
    "        \"pt\"   :   pT_MET   ,\n",
    "        \"eta\"  :  eta_MET  ,\n",
    "        \"phi\"  :  phi_MET  ,\n",
    "        \"mass\" : mass_MET ,\n",
    "    },\n",
    "    with_name=\"PtEtaPhiMLorentzVector\",\n",
    "    behavior=vector.behavior,\n",
    "    )\n",
    "\n",
    "    vec_sum = vec_MET + vec_higgs\n",
    "    mH_reco = vec_sum.mass\n",
    "    if not MET_UE: mass_str = \"MH_Reco\" if mass == \"Mj_corr_V2_a\" or mass == \"Mj_V2_a\" else mass\n",
    "    else : mass_str = \"MH_Reco\" + \"_UE_\" + MET_UE\n",
    "    \n",
    "    if not MET_UE: \n",
    "        events[mass_str] = ak.where(( (events[\"DPhi\"] < 0.8) & (events[\"MET_et\"]/events[\"PTj_V2_a\"] > 0.1)), mH_reco, events[mass])\n",
    "    elif MET_UE == \"up\" : \n",
    "        events[mass_str] = ak.where(( (events[\"DPhi\"] < 0.8) & (events[\"MET_et_UEup\"]/events[\"PTj_V2_a\"] > 0.1)), mH_reco, events[mass])\n",
    "    elif MET_UE == \"down\" : \n",
    "        events[mass_str] = ak.where(( (events[\"DPhi\"] < 0.8) & (events[\"MET_et_UEdown\"]/events[\"PTj_V2_a\"] > 0.1)), mH_reco, events[mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in files:\n",
    "    # note that QCD and Data don't have such variation\n",
    "    if k == \"data\" or k == \"QCD\": continue\n",
    "    print(\"Add reco of:\",k)\n",
    "    get_reco(events=files[k])\n",
    "    get_reco(events=files[k],MET_UE = \"up\")\n",
    "    get_reco(events=files[k],MET_UE = \"down\")\n",
    "    \n",
    "    get_reco(events=files[k],mass = \"Mj_jesTotalUp_a\")\n",
    "    get_reco(events=files[k],mass = \"Mj_jesTotalDown_a\")\n",
    "    get_reco(events=files[k],mass = \"Mj_jerUp_a\")\n",
    "    get_reco(events=files[k],mass = \"Mj_jerDown_a\")\n",
    "    \n",
    "    get_reco(events=files[k],mass = \"Mj_jmsUp_a\")\n",
    "    get_reco(events=files[k],mass = \"Mj_jmsDown_a\")\n",
    "    get_reco(events=files[k],mass = \"Mj_jmrUp_a\")\n",
    "    get_reco(events=files[k],mass = \"Mj_jmrDown_a\")\n",
    "for k in [\"QCD\",\"data\"]:\n",
    "    get_reco(events=files[k],mass = \"Mj_V2_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some test about variables / output all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[\"TotalSignal\"].fields"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "import boost_histogram as bh\n",
    "from cycler import cycler\n",
    "\n",
    "use_helvet = False ## true: use helvetica for plots, make sure the system have the font installed\n",
    "if use_helvet:\n",
    "    CMShelvet = hep.style.CMS\n",
    "    CMShelvet['font.sans-serif'] = ['Helvetica', 'Arial']\n",
    "    plt.style.use(CMShelvet)\n",
    "else:\n",
    "    plt.style.use(hep.style.CMS)\n",
    "\n",
    "def flow(hist: bh.Histogram, overflow: bool=True, underflow: bool=True):\n",
    "    h, var = hist.view(flow=(overflow | underflow)).value, hist.view(flow=(overflow | underflow)).variance\n",
    "    if overflow: \n",
    "        # h, var also include underflow bins but in plots usually no underflow data\n",
    "        # And we've filled None with -999, so we shouldn't show underflow data (mostly from filled None)\n",
    "        # You have to access the overflow and underflow bins data like below:\n",
    "        h[-2] += h[-1]; var[-2] += var[-1]\n",
    "    if underflow:\n",
    "        h[1] += h[0]; var[1] += var[0]\n",
    "    if overflow or underflow:\n",
    "        h, var = h[1:-1], var[1:-1]\n",
    "    return h, var\n",
    "    # Return the updated histogram and variance\n",
    "\n",
    "def error_bar(h, var, type='data'):\n",
    "    from scipy.interpolate import CubicSpline\n",
    "    if type == 'data':\n",
    "        number = h\n",
    "    elif type == 'mc':  # h = k*N, var = k^2*N, std = k*sqrt(N)\n",
    "        number = h**2 / var\n",
    "    else:\n",
    "        raise ValueError(\"type should be 'data' or 'mc'! \")\n",
    "    center = range(11) # Number: 0-10\n",
    "    up = np.array([1.84, 3.30, 4.64, 5.92, 7.16, 8.38, 9.58, 10.77, 11.95, 13.11, 14.27]) - center\n",
    "    down = center - np.array([0, 0.17, 0.71, 1.37, 2.09, 2.84, 3.62, 4.42, 5.23, 6.06, 6.89])\n",
    "    #cs means to create a CubicSpline object\n",
    "    cs_up = CubicSpline(x=center, y=up)\n",
    "    cs_down = CubicSpline(x=center, y=down)\n",
    "    \n",
    "    Garwood = (number>0)&(number<10)\n",
    "    poison_error_bar = np.sqrt(number)\n",
    "    up_error_bar = np.copy(poison_error_bar)\n",
    "    down_error_bar = np.copy(poison_error_bar)\n",
    "    up_error_bar[Garwood] = cs_up(number[Garwood])\n",
    "    down_error_bar[Garwood] = cs_down(number[Garwood])\n",
    "    if type == 'mc':\n",
    "        up_error_bar *= var/h\n",
    "        down_error_bar *= var/h\n",
    "    up_error_bar [up_error_bar < 0 ] = 0\n",
    "    down_error_bar [down_error_bar < 0 ] = 0\n",
    "    return np.array([down_error_bar, up_error_bar])\n",
    "\n",
    "\n",
    "# function to find the optimal region with S/sqrt(B)\n",
    "# not used so far\n",
    "def optimalcut(shist, bhist):\n",
    "    n_bins = len(shist)\n",
    "    best_lower = None\n",
    "    best_upper = None\n",
    "    best_s_sqrt_b = 0\n",
    "\n",
    "    for lower in range(n_bins):\n",
    "        for upper in range(lower+1, n_bins+1):\n",
    "            s = np.sum(shist[lower:upper])\n",
    "            b = np.sum(bhist[lower:upper])\n",
    "            s_sqrt_b = s / np.sqrt(b + 1)\n",
    "\n",
    "            if s_sqrt_b > best_s_sqrt_b:\n",
    "                best_lower = lower\n",
    "                best_upper = upper\n",
    "                best_s_sqrt_b = s_sqrt_b\n",
    "\n",
    "    return best_lower, best_upper, best_s_sqrt_b\n",
    "\n",
    "def optimalcut_oneside(shist, bhist, epsilon = 0.01):\n",
    "    '''\n",
    "    Given the signal histogram and background histogram, \n",
    "    show the one-side cut for the variable to get best s/sqrt(b).\n",
    "    Args:\n",
    "        shist:signal histogram\n",
    "        bhist:background histogram\n",
    "        epsilon(float): epsilon to avoid numerical errs \n",
    "    '''\n",
    "    n_bins = len(shist)\n",
    "    best_cut = 0\n",
    "    best_s_sqrt_b = 0\n",
    "\n",
    "    for cut in range(n_bins):\n",
    "        s = np.sum(shist[cut:])\n",
    "        b = np.sum(bhist[cut:])\n",
    "        s_sqrt_b = s / np.sqrt(b + epsilon)\n",
    "        if s_sqrt_b > best_s_sqrt_b:\n",
    "            best_cut = cut\n",
    "            best_s_sqrt_b = s_sqrt_b\n",
    "        \n",
    "    return best_cut, best_s_sqrt_b\n",
    "\n",
    "def optimalcut_mid_combine(shist1, shist2, bhist, epsilon = 1):\n",
    "    '''\n",
    "    Given the signal histogram and background histogram, \n",
    "    show the one-side cut for the variable to get best s/sqrt(b).\n",
    "    Args:\n",
    "        shist:signal histogram\n",
    "        bhist:background histogram\n",
    "        epsilon(float): epsilon to avoid numerical errs \n",
    "    '''\n",
    "    n_bins = len(shist1)\n",
    "    best_cut = 0\n",
    "    best_combined_sig_two_side = 0\n",
    "\n",
    "    for cut in range(n_bins):\n",
    "        s_right_side = np.sum(shist2[cut:])\n",
    "        b_right_side = np.sum(bhist[cut:])\n",
    "        s_left_side = np.sum(shist1[:cut])\n",
    "        b_left_side = np.sum(bhist[:cut])\n",
    "        s_sqrt_b_right_side = s_right_side / np.sqrt(b_right_side + epsilon)\n",
    "        s_sqrt_b_left_side = s_left_side / np.sqrt(b_left_side + epsilon)\n",
    "        combined_sig_two_side = np.sqrt((s_sqrt_b_right_side)**2 + (s_sqrt_b_left_side)**2)\n",
    "        if combined_sig_two_side > best_combined_sig_two_side:\n",
    "            best_cut = cut\n",
    "            best_combined_sig_two_side = combined_sig_two_side\n",
    "        \n",
    "    return best_cut, best_combined_sig_two_side\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define templates dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \".\"\n",
    "\n",
    "plot_dir = f\"{MAIN_DIR}/templates/21Mar24\"\n",
    "_ = os.system(f\"mkdir -p {plot_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define observable object variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ShapeVar:\n",
    "    \"\"\"Class to store attributes of a variable to make a histogram of.\n",
    "\n",
    "    Args:\n",
    "        var (str): variable name\n",
    "        label (str): variable label\n",
    "        bins (List[int]): bins\n",
    "        reg (bool, optional): Use a regular axis or variable binning. Defaults to True.\n",
    "        blind_window (List[int], optional): if blinding, set min and max values to set 0. Defaults to None.\n",
    "        significance_dir (str, optional): if plotting significance, which direction to plot it in.\n",
    "          See more in plotting.py:ratioHistPlot(). Options are [\"left\", \"right\", \"bin\"]. Defaults to \"right\".\n",
    "    \"\"\"\n",
    "\n",
    "    var: str = None\n",
    "    label: str = None\n",
    "    bins: List[int] = None\n",
    "    reg: bool = True #regular axis\n",
    "    blind_window: List[int] = None\n",
    "    significance_dir: str = \"right\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # create axis used for histogramming\n",
    "        if self.reg:\n",
    "            self.axis = hist2.axis.Regular(*self.bins, name=self.var, label=self.label)\n",
    "        else:\n",
    "            self.axis = hist2.axis.Variable(self.bins, name=self.var, label=self.label)\n",
    "\n",
    "@dataclass\n",
    "class Syst:\n",
    "    samples: list[str] = None\n",
    "    years: list[str] = field(default_factory=lambda: years)\n",
    "    label: str = None\n",
    "    \n",
    "def blindBins(h: hist2.Hist, blind_region: List, blind_sample: str = None, axis=0):\n",
    "    \"\"\"\n",
    "    Blind (i.e. zero) bins in histogram ``h``.\n",
    "    If ``blind_sample`` specified, only blind that sample, else blinds all.\n",
    "    \"\"\"\n",
    "    if axis > 0:\n",
    "        raise Exception(\"not implemented > 1D blinding yet\")\n",
    "\n",
    "    bins = h.axes[axis + 1].edges\n",
    "    lv = int(np.searchsorted(bins, blind_region[0], \"right\"))\n",
    "    rv = int(np.searchsorted(bins, blind_region[1], \"left\") + 1)\n",
    "\n",
    "    if blind_sample is not None:\n",
    "        data_key_index = np.where(np.array(list(h.axes[0])) == blind_sample)[0][0]\n",
    "        h.view(flow=True)[data_key_index][lv:rv].value = 0\n",
    "        h.view(flow=True)[data_key_index][lv:rv].variance = 0\n",
    "    else:\n",
    "        h.view(flow=True)[:, lv:rv].value = 0\n",
    "        h.view(flow=True)[:, lv:rv].variance = 0       \n",
    "shape_vars = [\n",
    "    ShapeVar(\n",
    "        \"MH_Reco\",\n",
    "        r\"Higgs candidate MET recovery mass [GeV]\",\n",
    "        [20, 50, 250],\n",
    "        reg=True,\n",
    "        blind_window=[90, 150],\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define samples we consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_keys = [\n",
    "    \"ggF\",\n",
    "    \"VBF\",\n",
    "    \"ZH\",\n",
    "    \"WH\",\n",
    "    \"ttH\",\n",
    "]\n",
    "\n",
    "bkg_keys = [\n",
    "    \"TT\",\n",
    "    \"WJets\",\n",
    "    \"ST\",\n",
    "    \"Rest\"\n",
    "]\n",
    "\n",
    "mc_keys = sig_keys + bkg_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define weight shift list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [\"2016APV\", \"2016\", \"2017\", \"2018\"]\n",
    "\n",
    "weight_shifts = {\n",
    "    \"pileup\": Syst(samples=mc_keys, label=\"Pileup\"),\n",
    "    \"ISRPartonShower\": Syst(samples=mc_keys, label=\"ISR Parton Shower\"),\n",
    "    \"FSRPartonShower\": Syst(samples=mc_keys, label=\"FSR Parton Shower\"),\n",
    "    \"QCDscale\": Syst(samples=bkg_keys, label=\"QCDScale\"),\n",
    "    \"trigger\" : Syst(samples=mc_keys, label=\"Trigger SF\"),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Lund Plane SF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_lp = {\n",
    "    \"a\" : 0.898,\n",
    "    \"b\" : 0.957,\n",
    "}\n",
    "\n",
    "#unc is not used here, since we will add the unc in the datacards directly\n",
    "weight_lp_unc = {\n",
    "    \"a\" : 0.334,\n",
    "    \"b\" : 0.349,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-organize weight information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(['data','QCD','TT','ST','WJets','Rest','ggF','WH','ZH','ttH','VBF']) #all samples we considered\n",
    "years = [\"2016APV\", \"2016\", \"2017\", \"2018\"]\n",
    "year_to_run = YEAR\n",
    "for shift in [\"down\", \"up\"]:\n",
    "    print(\"processing:\",shift)\n",
    "    for wshift, wsyst in weight_shifts.items():\n",
    "        for wsample in wsyst.samples:\n",
    "            print(\"processing:\",wsample)\n",
    "            if wsample in samples:\n",
    "                if wshift == \"pileup\" :\n",
    "                    print(\"processing:\",wshift)\n",
    "                    if shift == \"up\":\n",
    "                        for year in years: \n",
    "                            if year == year_to_run : \n",
    "                                files[wsample][f\"{wshift}_{shift}_{year}\"] = files[wsample][\"weight\"] * (files[wsample][\"puWeightUp\"] / files[wsample][\"puWeight\"])\n",
    "                            else : \n",
    "                                files[wsample][f\"{wshift}_{shift}_{year}\"] = files[wsample][\"weight\"]\n",
    "                    if shift == \"down\":\n",
    "                        for year in years: \n",
    "                            if year == year_to_run : \n",
    "                                files[wsample][f\"{wshift}_{shift}_{year}\"] = files[wsample][\"weight\"] * (files[wsample][\"puWeightDown\"] / files[wsample][\"puWeight\"])\n",
    "                            else : \n",
    "                                files[wsample][f\"{wshift}_{shift}_{year}\"] = files[wsample][\"weight\"]\n",
    "                if wshift == \"ISRPartonShower\" :\n",
    "                    print(\"processing:\",wshift)\n",
    "                    if shift == \"up\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"PSWeight_0\"]\n",
    "                    if shift == \"down\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"PSWeight_2\"] \n",
    "                if wshift == \"FSRPartonShower\" :\n",
    "                    print(\"processing:\",wshift)\n",
    "                    if shift == \"up\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"PSWeight_1\"]\n",
    "                    if shift == \"down\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"PSWeight_3\"] \n",
    "                if wshift == \"QCDscale\" :\n",
    "                    print(\"processing:\",wshift)\n",
    "                    if shift == \"up\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"LHEScaleWeight_8\"]\n",
    "                    if shift == \"down\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * files[wsample][\"LHEScaleWeight_0\"] \n",
    "                if wshift == \"trigger\" :\n",
    "                    print(\"processing:\",wshift)\n",
    "                    if shift == \"up\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * (1 + files[wsample][\"SF_unc\"])\n",
    "                    if shift == \"down\":\n",
    "                        files[wsample][f\"{wshift}_{shift}\"] = files[wsample][\"weight\"] * (1 - files[wsample][\"SF_unc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[\"WJets\"][\"pileup_down_2016APV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[\"WJets\"][\"MH_Reco\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variation shift list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jecs = {\n",
    "    \"JES\": \"JES_jes\",\n",
    "    \"JER\": \"JER\",\n",
    "}\n",
    "\n",
    "uncluste = {\n",
    "    \"UE\": \"unclusteredEnergy\",\n",
    "}\n",
    "\n",
    "jec_shifts = {}\n",
    "for key in jecs:\n",
    "    for shift in [\"up\", \"down\"]:\n",
    "        if key == \"JES\": \n",
    "            if shift == \"up\"   : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jesTotalUp_a\"\n",
    "            if shift == \"down\" : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jesTotalDown_a\"\n",
    "        if key == \"JER\": \n",
    "            if shift == \"up\"   : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jerUp_a\"\n",
    "            if shift == \"down\" : jec_shifts[f\"{key}_{shift}\"] = \"Mj_jerDown_a\"\n",
    "\n",
    "ue_shifts = {}\n",
    "for key in uncluste:\n",
    "    for shift in [\"up\", \"down\"]:\n",
    "        if shift == \"up\"   : ue_shifts[f\"{key}_{shift}\"] = \"MH_Reco_UE_up\"\n",
    "        if shift == \"down\" : ue_shifts[f\"{key}_{shift}\"] = \"MH_Reco_UE_down\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jec_shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ue_shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CUT(aka. regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT = {        \n",
    "    \"SR1a\" : {k: (files[k][\"MH_Reco\"] >= 50) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] <= 0.25) & (files[k][\"a_HWW_V2\"] >= 0.99) for k in files}, \n",
    "    \"SR1b\" : {k: (files[k][\"MH_Reco\"] >= 50) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] <= 0.25) & (files[k][\"a_HWW_V2\"] >= 0.92) & (files[k][\"a_HWW_V2\"] < 0.99) for k in files},   \n",
    "    \"SR2a\" : {k: (files[k][\"MH_Reco\"] >= 50) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] > 0.25) & (files[k][\"a_HWW_V2\"] >= 0.99) & (files[k][\"DPhi\"] < 0.8)for k in files}, \n",
    "    \"SR2b\" : {k: (files[k][\"MH_Reco\"] >= 50) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] > 0.25) & (files[k][\"a_HWW_V2\"] >= 0.92) & (files[k][\"a_HWW_V2\"] < 0.99) & (files[k][\"DPhi\"] < 0.8)for k in files},   \n",
    "    \"CR1\"  : {k: (files[k][\"MH_Reco\"] >= 50) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] <= 0.25) & (files[k][\"a_HWW_V2\"] < 0.92) for k in files},\n",
    "    \"CR2\"  : {k: (files[k][\"MH_Reco\"] >= 50) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] > 0.25)  & (files[k][\"a_HWW_V2\"] < 0.92) & (files[k][\"DPhi\"] < 0.8) for k in files},   \n",
    "    }\n",
    "\n",
    "# CR: tagger < 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not used actually, since we can directly blind the mass window in save_pkl function\n",
    "\n",
    "# CUT_BLINDED = {\n",
    "#         \"SR1a_blinded\" : {k: ( (files[k][\"MH_Reco\"] >= 50) & ((files[k][\"MH_Reco\"] <= 90) | (files[k][\"MH_Reco\"] >= 150)) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] <= 0.25) & (files[k][\"a_HWW_V2\"] >= 0.99) for k in files}, \n",
    "#         \"SR1b_blinded\" : {k: ( (files[k][\"MH_Reco\"] >= 50) & ((files[k][\"MH_Reco\"] <= 90) | (files[k][\"MH_Reco\"] >= 150)) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] <= 0.25) & (files[k][\"a_HWW_V2\"] >= 0.92) & (files[k][\"a_HWW_V2\"] < 0.99) for k in files},   \n",
    "#         \"SR2a_blinded\" : {k: ( (files[k][\"MH_Reco\"] >= 50) & ((files[k][\"MH_Reco\"] <= 90) | (files[k][\"MH_Reco\"] >= 150)) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] > 0.25) & (files[k][\"a_HWW_V2\"] >= 0.99) & (files[k][\"DPhi\"] < 0.8)for k in files}, \n",
    "#         \"SR2b_blinded\" : {k: ( (files[k][\"MH_Reco\"] >= 50) & ((files[k][\"MH_Reco\"] <= 90) | (files[k][\"MH_Reco\"] >= 150)) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] > 0.25) & (files[k][\"a_HWW_V2\"] >= 0.92) & (files[k][\"a_HWW_V2\"] < 0.99) & (files[k][\"DPhi\"] < 0.8)for k in files},   \n",
    "#         \"CR1_blinded\"  : {k: ( (files[k][\"MH_Reco\"] >= 50) & ((files[k][\"MH_Reco\"] <= 90) | (files[k][\"MH_Reco\"] >= 150)) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] <= 0.25) & (files[k][\"a_HWW_V2\"] < 0.92) for k in files},\n",
    "#         \"CR2_blinded\"  : {k: ( (files[k][\"MH_Reco\"] >= 50) & ((files[k][\"MH_Reco\"] <= 90) | (files[k][\"MH_Reco\"] >= 150)) & (files[k][\"MET_et\"]/files[k][\"PTj_V2_a\"] > 0.25)  & (files[k][\"a_HWW_V2\"] < 0.92) & (files[k][\"DPhi\"] < 0.8)for k in files},   \n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save hist templates to pkl files\n",
    "\n",
    "We note here that no particular operation is needed for QCD, since we only need raw QCD MC ratio as initial tranfer factor in the actual QCD prediction, and rhalphabet method will use (data - other bkg) in fail(control) region and perform simultaneous fit with pass(signal) region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_pkl(files, path_str = plot_dir, template_file = \"templates\",year_to_run = \"2018\"):\n",
    "    \n",
    "    templates = {} #empty dict to store the templates file\n",
    "    regions = [\"SR1a\",\"SR1b\",\"CR1\",\"SR2a\",\"SR2b\",\"CR2\"] #signal regions or control regions\n",
    "    signal_region_as = [\"SR1a\",\"SR2a\"]\n",
    "    signal_region_bs = [\"SR1b\",\"SR2b\"]\n",
    "    years = [\"2016APV\", \"2016\", \"2017\", \"2018\"]\n",
    "    samples = list(['data','QCD','TT','ST','WJets','Rest','ggF','WH','ZH','ttH','VBF']) #all samples we considered\n",
    "    print(\"now running year:\",year_to_run)\n",
    "    #initialize weight based variation samples\n",
    "    hist_samples = []\n",
    "    for shift in [\"down\", \"up\"]:\n",
    "        for wshift, wsyst in weight_shifts.items():\n",
    "            for wsample in wsyst.samples:\n",
    "                if (wshift == \"pileup\") : continue\n",
    "                hist_samples.append(f\"{wsample}_{wshift}_{shift}\")\n",
    "    \n",
    "    #additionally, we have to add different year info to pileup\n",
    "    for shift in [\"down\", \"up\"]:\n",
    "        for wshift, wsyst in weight_shifts.items():\n",
    "            if not (wshift == \"pileup\") : continue\n",
    "            for wsample in wsyst.samples:\n",
    "                for year in years:\n",
    "                    hist_samples.append(f\"{wsample}_{wshift}_{shift}_{year}\")\n",
    "    \n",
    "    hist_samples += samples\n",
    "    #fill templates for different regions\n",
    "    \n",
    "    for region in regions:\n",
    "        \n",
    "        templates[region] = hist2.Hist(\n",
    "            hist2.axis.StrCategory(hist_samples, name=\"Sample\"),\n",
    "            *[shape_var.axis for shape_var in shape_vars],\n",
    "            storage=\"weight\",\n",
    "            ) #initialize a hist object\n",
    "        \n",
    "        #add center value templates first\n",
    "        for sample in samples:\n",
    "            \n",
    "            #if region is signal region and sample is signal samples, we multiply by lund plane sf\n",
    "            if (region in signal_region_as) and (sample in sig_keys): weight_to_add = weight_lp[\"a\"]\n",
    "            elif (region in signal_region_bs) and (sample in sig_keys): weight_to_add = weight_lp[\"b\"]\n",
    "            else : weight_to_add = 1.0\n",
    "            \n",
    "            data = files[sample][CUT[region][sample]]\n",
    "            templates[region].fill(\n",
    "                                Sample=sample,\n",
    "                                MH_Reco=data[\"MH_Reco\"],\n",
    "                                weight=data[\"weight\"] * weight_to_add,\n",
    "                            )\n",
    "            if sample == \"data\": \n",
    "                if (region.endswith(\"a\") or region.endswith(\"b\")):\n",
    "                    # blind signal mass windows in pass region in data even for not \"Blinded\" region\n",
    "                    # print(\"blind data of \",region)\n",
    "                    for i, shape_var in enumerate(shape_vars):\n",
    "                        if shape_var.blind_window is not None:\n",
    "                            blindBins(templates[region], shape_var.blind_window, \"data\", axis=i)\n",
    "                            \n",
    "\n",
    "\n",
    "        #add weight based variation for each sample            \n",
    "        for shift in [\"down\", \"up\"]:\n",
    "            for wshift, wsyst in weight_shifts.items():\n",
    "                if wshift == \"pileup\":\n",
    "                    for year in years: \n",
    "                        # pileup need year specific information\n",
    "                        for wsample in wsyst.samples:\n",
    "                            if wsample in samples:\n",
    "                                data = files[wsample][CUT[region][wsample]] \n",
    "                                # print(region, wsample, wshift, shift)\n",
    "                                # print(\"mass info:\",data[\"MH_Reco\"])\n",
    "                                # print(\"weight infor:\",data[f\"{wshift}_{shift}_{year}\"])\n",
    "                                            \n",
    "                                #if region is signal region and sample is signal samples, we multiply by lund plane sf\n",
    "                                if (region in signal_region_as) and (wsample in sig_keys): weight_to_add = weight_lp[\"a\"]\n",
    "                                elif (region in signal_region_bs) and (wsample in sig_keys): weight_to_add = weight_lp[\"b\"]\n",
    "                                else : weight_to_add = 1.0\n",
    "                                \n",
    "                                templates[region].fill(\n",
    "                                    Sample=wsample + f\"_{wshift}_{shift}_{year}\",\n",
    "                                    MH_Reco=data[\"MH_Reco\"],\n",
    "                                    weight=data[f\"{wshift}_{shift}_{year}\"] * weight_to_add,\n",
    "                                )                    \n",
    "                else:\n",
    "                    for wsample in wsyst.samples:\n",
    "                        if wsample in samples:\n",
    "                            data = files[wsample][CUT[region][wsample]] \n",
    "                            \n",
    "                            #if region is signal region and sample is signal samples, we multiply by lund plane sf\n",
    "                            if (region in signal_region_as) and (wsample in sig_keys): weight_to_add = weight_lp[\"a\"]\n",
    "                            elif (region in signal_region_bs) and (wsample in sig_keys): weight_to_add = weight_lp[\"b\"]\n",
    "                            else : weight_to_add = 1.0\n",
    "                            \n",
    "                            templates[region].fill(\n",
    "                                Sample=wsample + f\"_{wshift}_{shift}\",\n",
    "                                MH_Reco=data[\"MH_Reco\"],\n",
    "                                weight=data[f\"{wshift}_{shift}\"] * weight_to_add,\n",
    "                            )\n",
    "                        \n",
    "        #add shift variation for each sample\n",
    "        #1.initialize hist info\n",
    "        for wshift, wsyst in jec_shifts.items():\n",
    "            for year in years: \n",
    "                # split the JES/JER uncertainties according to year, i.e., one variation for each era\n",
    "                templates[f\"{region}_{wshift}_{year}\"] = hist2.Hist(\n",
    "                hist2.axis.StrCategory(samples, name=\"Sample\"),\n",
    "                *[shape_var.axis for shape_var in shape_vars],\n",
    "                storage=\"weight\",\n",
    "                ) #initialize a hist object\n",
    "        for wshift, wsyst in ue_shifts.items():\n",
    "                templates[f\"{region}_{wshift}\"] = hist2.Hist(\n",
    "                hist2.axis.StrCategory(samples, name=\"Sample\"),\n",
    "                *[shape_var.axis for shape_var in shape_vars],\n",
    "                storage=\"weight\",\n",
    "                ) #initialize a hist object                \n",
    "        \n",
    "        #2.fill the hist\n",
    "        for sample in mc_keys:\n",
    "            #JECS\n",
    "            for wshift, wsyst in jec_shifts.items():\n",
    "                for year in years: \n",
    "                    # split the JES/JER uncertainties according to year, i.e., one variation for each era\n",
    "                    if year == year_to_run:\n",
    "                        data = files[sample][CUT[region][sample]]\n",
    "\n",
    "                        #if region is signal region and sample is signal samples, we multiply by lund plane sf\n",
    "                        if (region in signal_region_as) and (sample in sig_keys): weight_to_add = weight_lp[\"a\"]\n",
    "                        elif (region in signal_region_bs) and (sample in sig_keys): weight_to_add = weight_lp[\"b\"]\n",
    "                        else : weight_to_add = 1.0\n",
    "                            \n",
    "                        #assign variation only to year_to_run\n",
    "                        templates[f\"{region}_{wshift}_{year}\"].fill(\n",
    "                                Sample=sample,\n",
    "                                MH_Reco=data[wsyst],\n",
    "                                weight=data[\"weight\"] * weight_to_add,\n",
    "                            )\n",
    "                        # print(f\"{region}_{wshift}_{year}\", sample, wsyst)\n",
    "                        # print(\"mass info:\",data[wsyst])\n",
    "                    else:\n",
    "                        data = files[sample][CUT[region][sample]] \n",
    "\n",
    "                        #if region is signal region and sample is signal samples, we multiply by lund plane sf\n",
    "                        if (region in signal_region_as) and (sample in sig_keys): weight_to_add = weight_lp[\"a\"]\n",
    "                        elif (region in signal_region_bs) and (sample in sig_keys): weight_to_add = weight_lp[\"b\"]\n",
    "                        else : weight_to_add = 1.0\n",
    "                        \n",
    "                        #assign same variation as center value for other years\n",
    "                        templates[f\"{region}_{wshift}_{year}\"].fill(\n",
    "                                Sample=sample,\n",
    "                                MH_Reco=data[\"MH_Reco\"],\n",
    "                                weight=data[\"weight\"] * weight_to_add,\n",
    "                            )                                    \n",
    "            \n",
    "            #un-clustered energy\n",
    "            for wshift, wsyst in ue_shifts.items():\n",
    "                data = files[sample][CUT[region][sample]] \n",
    "                        \n",
    "                #if region is signal region and sample is signal samples, we multiply by lund plane sf\n",
    "                if (region in signal_region_as) and (sample in sig_keys): weight_to_add = weight_lp[\"a\"]\n",
    "                elif (region in signal_region_bs) and (sample in sig_keys): weight_to_add = weight_lp[\"b\"]\n",
    "                else : weight_to_add = 1.0\n",
    "                        \n",
    "                templates[f\"{region}_{wshift}\"].fill(\n",
    "                                Sample=sample,\n",
    "                                MH_Reco=data[wsyst],\n",
    "                                weight=data[\"weight\"] * weight_to_add,\n",
    "                )\n",
    "\n",
    "        #extra process for QCD and data\n",
    "        for sample in [\"QCD\",\"data\"]:\n",
    "            #QCD and data doesn't have any j-shift nor weight based variation\n",
    "            \n",
    "            #JEC variation\n",
    "            for wshift, wsyst in jec_shifts.items():\n",
    "                for year in years: \n",
    "                    # split the JES/JER uncertainties according to year, i.e., one variation for each era\n",
    "                        data = files[sample][CUT[region][sample]] \n",
    "                        #always assign value with `MH_Reco` variable\n",
    "                        templates[f\"{region}_{wshift}_{year}\"].fill(\n",
    "                                Sample=sample,\n",
    "                                MH_Reco=data[\"MH_Reco\"],\n",
    "                                weight=data[\"weight\"],\n",
    "                            )\n",
    "                        \n",
    "                        #do blind procedure\n",
    "                        if sample == \"data\" and (region.endswith(\"a\") or region.endswith(\"b\")):\n",
    "                            for i, shape_var in enumerate(shape_vars):\n",
    "                                if shape_var.blind_window is not None:\n",
    "                                    blindBins(templates[f\"{region}_{wshift}_{year}\"], shape_var.blind_window, \"data\", axis=i)\n",
    "\n",
    "            #un-clustered energy variation\n",
    "            for wshift, wsyst in ue_shifts.items():\n",
    "                data = files[sample][CUT[region][sample]] \n",
    "                #always assign value with `MH_Reco` variable\n",
    "                templates[f\"{region}_{wshift}\"].fill(\n",
    "                        Sample=sample,\n",
    "                        MH_Reco=data[\"MH_Reco\"],\n",
    "                        weight=data[\"weight\"],\n",
    "                    )\n",
    "                                        \n",
    "                #do blind procedure\n",
    "                if sample == \"data\" and (region.endswith(\"a\") or region.endswith(\"b\")):\n",
    "                    for i, shape_var in enumerate(shape_vars):\n",
    "                        if shape_var.blind_window is not None:\n",
    "                            blindBins(templates[f\"{region}_{wshift}\"], shape_var.blind_window, \"data\", axis=i)\n",
    "\n",
    "                        \n",
    "        print(\"done fill template \",region)        \n",
    "    \n",
    "    #Creates blinded copies of each region's templates and saves a pickle of the templates\n",
    "    blind_window = shape_vars[0].blind_window\n",
    "    for label, template in list(templates.items()):\n",
    "        blinded_template = deepcopy(template)\n",
    "        blindBins(blinded_template, blind_window)\n",
    "        templates[f\"{label}Blinded\"] = blinded_template\n",
    "    \n",
    "    #save files\n",
    "    with open(f\"{path_str}/{template_file}_{year_to_run}.pkl\", \"wb\") as fp:\n",
    "        pkl.dump(templates, fp) # dump the templates of each region in a pkl file\n",
    "        print(\"Saved templates to\", f\"{template_file}_{year_to_run}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(files = files, year_to_run = YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLACE_HOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some test about the output templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{plot_dir}/templates_2018.pkl\",\"rb\") as f:\n",
    "    hists_template1 = pkl.load(f)\n",
    "# hists_template[\"pass\"][\"QCD\",:]  \n",
    "# hists_template[\"pass\"][\"QCD\",:].sum().value\n",
    "# hists_template[\"CR2\"][\"QCD\",:]\n",
    "hists_template1[\"SR1a_JES_up_2018\"][\"ggF\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a_JES_up_2018\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a\"][\"TT_FSRPartonShower_up\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a\"][\"TT\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template1[\"SR1a\"][\"TT_FSRPartonShower_down\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_template1 = hists_template1[\"SR1a\"][\"QCD\", :]\n",
    "err = sample_template1.variances()\n",
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , axis in enumerate(hists_template1[\"SR1a\"].axes[1:]):\n",
    "    print(i, axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test of HHbbVV analysis for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/home/pku/zhaoyz/Higgs/HHbbVV/src/HHbbVV/postprocessing/templates/23Jun14/2018_templates.pkl\",\"rb\") as f:\n",
    "with open(\"/ospool/cms-user/yuzhe/BoostedHWW/prediction/HHbbVV/src/HHbbVV/postprocessing/templates/24Mar15UpdateData/2018_templates.pkl\",\"rb\") as f:\n",
    "    hists_template2 = pkl.load(f)\n",
    "# hists_template[\"pass\"][\"QCD\",:]  \n",
    "# hists_template[\"pass\"][\"QCD\",:].sum().value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass_JES_up\"][\"ST\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hist(\n",
    "  StrCategory(['HHbbVV', 'ggHH_kl_2p45_kt_1_HHbbVV', 'ggHH_kl_5_kt_1_HHbbVV', 'ggHH_kl_0_kt_1_HHbbVV', 'VBFHHbbVV', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV', 'QCD', 'TT', 'ST', 'W+Jets', 'Z+Jets', 'Diboson', 'ggFHbb', 'VBFHbb', 'ZHbb', 'WHbb', 'ggZHbb', 'ttHbb', 'HWW', 'Data', 'HHbbVV_txbb_down', 'ggHH_kl_2p45_kt_1_HHbbVV_txbb_down', 'ggHH_kl_5_kt_1_HHbbVV_txbb_down', 'ggHH_kl_0_kt_1_HHbbVV_txbb_down', 'VBFHHbbVV_txbb_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_txbb_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_txbb_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_txbb_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_txbb_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_txbb_down', 'HHbbVV_pileup_down', 'ggHH_kl_2p45_kt_1_HHbbVV_pileup_down', 'ggHH_kl_5_kt_1_HHbbVV_pileup_down', 'ggHH_kl_0_kt_1_HHbbVV_pileup_down', 'VBFHHbbVV_pileup_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pileup_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pileup_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pileup_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pileup_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pileup_down', 'TT_pileup_down', 'ST_pileup_down', 'W+Jets_pileup_down', 'Z+Jets_pileup_down', 'HHbbVV_pileupID_down', 'ggHH_kl_2p45_kt_1_HHbbVV_pileupID_down', 'ggHH_kl_5_kt_1_HHbbVV_pileupID_down', 'ggHH_kl_0_kt_1_HHbbVV_pileupID_down', 'VBFHHbbVV_pileupID_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pileupID_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pileupID_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pileupID_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pileupID_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pileupID_down', 'TT_pileupID_down', 'ST_pileupID_down', 'W+Jets_pileupID_down', 'Z+Jets_pileupID_down', 'HHbbVV_ISRPartonShower_down', 'ggHH_kl_2p45_kt_1_HHbbVV_ISRPartonShower_down', 'ggHH_kl_5_kt_1_HHbbVV_ISRPartonShower_down', 'ggHH_kl_0_kt_1_HHbbVV_ISRPartonShower_down', 'VBFHHbbVV_ISRPartonShower_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_ISRPartonShower_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_ISRPartonShower_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_ISRPartonShower_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_ISRPartonShower_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_ISRPartonShower_down', 'TT_ISRPartonShower_down', 'ST_ISRPartonShower_down', 'W+Jets_ISRPartonShower_down', 'Z+Jets_ISRPartonShower_down', 'HHbbVV_FSRPartonShower_down', 'ggHH_kl_2p45_kt_1_HHbbVV_FSRPartonShower_down', 'ggHH_kl_5_kt_1_HHbbVV_FSRPartonShower_down', 'ggHH_kl_0_kt_1_HHbbVV_FSRPartonShower_down', 'VBFHHbbVV_FSRPartonShower_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_FSRPartonShower_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_FSRPartonShower_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_FSRPartonShower_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_FSRPartonShower_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_FSRPartonShower_down', 'TT_FSRPartonShower_down', 'ST_FSRPartonShower_down', 'W+Jets_FSRPartonShower_down', 'Z+Jets_FSRPartonShower_down', 'HHbbVV_L1EcalPrefiring_down', 'ggHH_kl_2p45_kt_1_HHbbVV_L1EcalPrefiring_down', 'ggHH_kl_5_kt_1_HHbbVV_L1EcalPrefiring_down', 'ggHH_kl_0_kt_1_HHbbVV_L1EcalPrefiring_down', 'VBFHHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_L1EcalPrefiring_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_L1EcalPrefiring_down', 'TT_L1EcalPrefiring_down', 'ST_L1EcalPrefiring_down', 'W+Jets_L1EcalPrefiring_down', 'Z+Jets_L1EcalPrefiring_down', 'HHbbVV_electron_id_down', 'ggHH_kl_2p45_kt_1_HHbbVV_electron_id_down', 'ggHH_kl_5_kt_1_HHbbVV_electron_id_down', 'ggHH_kl_0_kt_1_HHbbVV_electron_id_down', 'VBFHHbbVV_electron_id_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_electron_id_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_electron_id_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_electron_id_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_electron_id_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_electron_id_down', 'TT_electron_id_down', 'ST_electron_id_down', 'W+Jets_electron_id_down', 'Z+Jets_electron_id_down', 'HHbbVV_muon_id_down', 'ggHH_kl_2p45_kt_1_HHbbVV_muon_id_down', 'ggHH_kl_5_kt_1_HHbbVV_muon_id_down', 'ggHH_kl_0_kt_1_HHbbVV_muon_id_down', 'VBFHHbbVV_muon_id_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_muon_id_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_muon_id_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_muon_id_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_muon_id_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_muon_id_down', 'TT_muon_id_down', 'ST_muon_id_down', 'W+Jets_muon_id_down', 'Z+Jets_muon_id_down', 'HHbbVV_scale_down', 'ggHH_kl_2p45_kt_1_HHbbVV_scale_down', 'ggHH_kl_5_kt_1_HHbbVV_scale_down', 'ggHH_kl_0_kt_1_HHbbVV_scale_down', 'VBFHHbbVV_scale_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_scale_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_scale_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_scale_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_scale_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_scale_down', 'TT_scale_down', 'HHbbVV_pdf_down', 'ggHH_kl_2p45_kt_1_HHbbVV_pdf_down', 'ggHH_kl_5_kt_1_HHbbVV_pdf_down', 'ggHH_kl_0_kt_1_HHbbVV_pdf_down', 'VBFHHbbVV_pdf_down', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pdf_down', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pdf_down', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pdf_down', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pdf_down', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pdf_down', 'HHbbVV_txbb_up', 'ggHH_kl_2p45_kt_1_HHbbVV_txbb_up', 'ggHH_kl_5_kt_1_HHbbVV_txbb_up', 'ggHH_kl_0_kt_1_HHbbVV_txbb_up', 'VBFHHbbVV_txbb_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_txbb_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_txbb_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_txbb_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_txbb_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_txbb_up', 'HHbbVV_pileup_up', 'ggHH_kl_2p45_kt_1_HHbbVV_pileup_up', 'ggHH_kl_5_kt_1_HHbbVV_pileup_up', 'ggHH_kl_0_kt_1_HHbbVV_pileup_up', 'VBFHHbbVV_pileup_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pileup_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pileup_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pileup_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pileup_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pileup_up', 'TT_pileup_up', 'ST_pileup_up', 'W+Jets_pileup_up', 'Z+Jets_pileup_up', 'HHbbVV_pileupID_up', 'ggHH_kl_2p45_kt_1_HHbbVV_pileupID_up', 'ggHH_kl_5_kt_1_HHbbVV_pileupID_up', 'ggHH_kl_0_kt_1_HHbbVV_pileupID_up', 'VBFHHbbVV_pileupID_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pileupID_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pileupID_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pileupID_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pileupID_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pileupID_up', 'TT_pileupID_up', 'ST_pileupID_up', 'W+Jets_pileupID_up', 'Z+Jets_pileupID_up', 'HHbbVV_ISRPartonShower_up', 'ggHH_kl_2p45_kt_1_HHbbVV_ISRPartonShower_up', 'ggHH_kl_5_kt_1_HHbbVV_ISRPartonShower_up', 'ggHH_kl_0_kt_1_HHbbVV_ISRPartonShower_up', 'VBFHHbbVV_ISRPartonShower_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_ISRPartonShower_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_ISRPartonShower_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_ISRPartonShower_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_ISRPartonShower_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_ISRPartonShower_up', 'TT_ISRPartonShower_up', 'ST_ISRPartonShower_up', 'W+Jets_ISRPartonShower_up', 'Z+Jets_ISRPartonShower_up', 'HHbbVV_FSRPartonShower_up', 'ggHH_kl_2p45_kt_1_HHbbVV_FSRPartonShower_up', 'ggHH_kl_5_kt_1_HHbbVV_FSRPartonShower_up', 'ggHH_kl_0_kt_1_HHbbVV_FSRPartonShower_up', 'VBFHHbbVV_FSRPartonShower_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_FSRPartonShower_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_FSRPartonShower_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_FSRPartonShower_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_FSRPartonShower_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_FSRPartonShower_up', 'TT_FSRPartonShower_up', 'ST_FSRPartonShower_up', 'W+Jets_FSRPartonShower_up', 'Z+Jets_FSRPartonShower_up', 'HHbbVV_L1EcalPrefiring_up', 'ggHH_kl_2p45_kt_1_HHbbVV_L1EcalPrefiring_up', 'ggHH_kl_5_kt_1_HHbbVV_L1EcalPrefiring_up', 'ggHH_kl_0_kt_1_HHbbVV_L1EcalPrefiring_up', 'VBFHHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_L1EcalPrefiring_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_L1EcalPrefiring_up', 'TT_L1EcalPrefiring_up', 'ST_L1EcalPrefiring_up', 'W+Jets_L1EcalPrefiring_up', 'Z+Jets_L1EcalPrefiring_up', 'HHbbVV_electron_id_up', 'ggHH_kl_2p45_kt_1_HHbbVV_electron_id_up', 'ggHH_kl_5_kt_1_HHbbVV_electron_id_up', 'ggHH_kl_0_kt_1_HHbbVV_electron_id_up', 'VBFHHbbVV_electron_id_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_electron_id_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_electron_id_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_electron_id_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_electron_id_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_electron_id_up', 'TT_electron_id_up', 'ST_electron_id_up', 'W+Jets_electron_id_up', 'Z+Jets_electron_id_up', 'HHbbVV_muon_id_up', 'ggHH_kl_2p45_kt_1_HHbbVV_muon_id_up', 'ggHH_kl_5_kt_1_HHbbVV_muon_id_up', 'ggHH_kl_0_kt_1_HHbbVV_muon_id_up', 'VBFHHbbVV_muon_id_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_muon_id_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_muon_id_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_muon_id_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_muon_id_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_muon_id_up', 'TT_muon_id_up', 'ST_muon_id_up', 'W+Jets_muon_id_up', 'Z+Jets_muon_id_up', 'HHbbVV_scale_up', 'ggHH_kl_2p45_kt_1_HHbbVV_scale_up', 'ggHH_kl_5_kt_1_HHbbVV_scale_up', 'ggHH_kl_0_kt_1_HHbbVV_scale_up', 'VBFHHbbVV_scale_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_scale_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_scale_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_scale_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_scale_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_scale_up', 'TT_scale_up', 'HHbbVV_pdf_up', 'ggHH_kl_2p45_kt_1_HHbbVV_pdf_up', 'ggHH_kl_5_kt_1_HHbbVV_pdf_up', 'ggHH_kl_0_kt_1_HHbbVV_pdf_up', 'VBFHHbbVV_pdf_up', 'qqHH_CV_1_C2V_0_kl_1_HHbbVV_pdf_up', 'qqHH_CV_1p5_C2V_1_kl_1_HHbbVV_pdf_up', 'qqHH_CV_1_C2V_1_kl_2_HHbbVV_pdf_up', 'qqHH_CV_1_C2V_2_kl_1_HHbbVV_pdf_up', 'qqHH_CV_1_C2V_1_kl_0_HHbbVV_pdf_up'], name='Sample'),\n",
    "  Regular(20, 50, 250, name='bbFatJetParticleNetMass', label='$m^{bb}_\\\\mathrm{Reg}$ (GeV)'),\n",
    "  storage=Weight()) # Sum: WeightedSum(value=-nan, variance=88.7794) (WeightedSum(value=-nan, variance=88.7794) with flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"][\"ST\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass_JES_down\"][\"ST\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"][\"HHbbVV_pileup_up\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"][\"HHbbVV_pileup_up\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"pass\"][\"HHbbVV\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists_template2[\"passBlinded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test how to load and use the *.pkl template file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_template(h, sample):\n",
    "    ''' \n",
    "    histogram h Hist, with axes:[\"samples\",\"systematic\",\"MH_Reco\"]\n",
    "    sample is sample name in [\"QCD\",...,\"data\"]\n",
    "    '''\n",
    "    mass_axis = 1 #axis index\n",
    "    massbins = h.axes[mass_axis].edges\n",
    "    return (h[sample, :].values(), massbins, \"MH_Reco\")\n",
    "\n",
    "a = get_template(hists_template1[\"SR1a\"],\"QCD\")\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make some plots to test the variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins, x_min, x_max = 20, 50, 250\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=[\"tab:blue\",\t\"tab:orange\",\t\"tab:green\",\t\"tab:red\",\t\"tab:purple\", \"tab:brown\", \"tab:pink\", \"k\",\"tab:olive\" ,\t\"tab:cyan\"])\n",
    "\n",
    "f = plt.figure(figsize=(14, 15))\n",
    "ax = f.add_subplot(1, 1, 1)  \n",
    "ax.grid()\n",
    "\n",
    "year = \"2018\"\n",
    "LUMI = {\"2016\": 36.33, \"2017\": 41.48, \"2018\": 59.83,\"Full-Run2\":138}\n",
    "hep.cms.label(loc = 1, data=True, year=year, ax=ax, lumi=LUMI[year], fontsize=18, llabel='Preliminary')\n",
    "\n",
    "hist_region = bh.Histogram(bh.axis.Regular(nbins, x_min, x_max), storage=bh.storage.Weight())\n",
    "hep.histplot(get_template(hists_template1[\"SR1a\"],\"WJets_QCDscale_up\")[0], bins=get_template(hists_template1[\"SR1a_JES_up_2018\"],\"TT\")[1], label=\"Test \", histtype='step', stack=False, linewidth=2, ax=ax,color = \"red\")\n",
    "\n",
    "hist_region = bh.Histogram(bh.axis.Regular(nbins, x_min, x_max), storage=bh.storage.Weight())\n",
    "hep.histplot(get_template(hists_template1[\"SR1a\"],\"WJets\")[0], bins=get_template(hists_template1[\"SR1a_JES_up_2018\"],\"TT\")[1], label=\"Test \", histtype='step', stack=False, linewidth=2, ax=ax,color = \"blue\")\n",
    "\n",
    "hist_region = bh.Histogram(bh.axis.Regular(nbins, x_min, x_max), storage=bh.storage.Weight())\n",
    "hep.histplot(get_template(hists_template1[\"SR1a\"],\"WJets_QCDscale_down\")[0], bins=get_template(hists_template1[\"SR1a_JES_up_2018\"],\"TT\")[1], label=\"Test \", histtype='step', stack=False, linewidth=2, ax=ax,color = \"orange\")\n",
    "\n",
    "\n",
    "ax.set_ylabel(\"Events\")\n",
    "ax.legend(loc=\"upper right\", ncol=1, frameon=False, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some other test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = {\n",
    "        \"CR1\" :{\"SRa\": \"SR1a\",\"SRb\":\"SR1b\"},\n",
    "        \"CR2\" :{\"SRa\": \"SR2a\",\"SRb\":\"SR2b\"},\n",
    "        \"CR3\" :{\"SRa\": \"SR3a\",\"SRb\":\"SR3b\"},\n",
    "        }\n",
    "\n",
    "regions_blinded = { key_fail + \"_blinded\": {key_pass + \"_blinded\" : key_pass_ab + \"_blinded\" for key_pass , key_pass_ab in key_pass_dict.items()}  for key_fail , key_pass_dict in regions.items()}\n",
    "regions_blinded.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"SR1a_blinded\"\n",
    "pass_region = (\"a_\" in region)\n",
    "pass_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"SR1aBlinded\"\n",
    "region_noblinded = region.split(\"Blinded\")[0]\n",
    "region_noblinded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc8653c37afde981a02f518cc5ed66e36d68f5e1c41895fdf66da08341e86c45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
